<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>First order methods</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h1>Thoughts on first order descent methods</h1> <p>First order methods broadly designate iterative methods for continuous and differentiable optimisation that mainly use information coming from the gradient of the function at the current point. The success of first order methods is often related to the fact that iterations are cheap to compute compared to, say, second order methods that use information coming from the hessian of the function at the current point.</p> <p>In these notes we consider again the constrained minimisation problem \(\min_C f\) and, to simplify the presentation, we&#39;ll assume that \(C\) is closed and convex and that \(f\) is strictly convex and smooth on \(C\). What we&#39;re interested in is discussing, at a high level, how to generate a <em>minimising sequence</em> for \(f\) i.e., a sequence \(\{x_k\}\) with \(x_k\in C\) such that \(f(x_{k+1}) < f(x_k)\) and \(f(x_k) \to f(x^\dagger)\) as \(k\) grows.</p> <p>All norms \(\|\cdot\|\) are 2-norms unless otherwise mentioned.</p> <h2>Local linearisation</h2> <p>Local linearisation is basically what first order methods are about: form a linear approximation of the function around a current point. Still, let&#39;s try to look at this from scratch: consider the local linearisation of \(f\) around a point \(a\in C^\circ\):</p> $$ f(x) \quad\!\! =\quad\!\! f(a) + \left\langle x-a, \nabla f(a)\right\rangle + r(x, a) $$ <p>where \(r(x, a)\) is the <em>remainder function</em>.</p> <p><strong>Note</strong>: here you may be thinking: <em>Taylor expansion</em>, but actually let&#39;s put that on the side for now and just assume that you&#39;re building this \(f(a)+\left\langle x-a,\nabla f(a)\right\rangle\) and that you want to investigate the properties of the remainder function.</p> <p>That function enjoys the following properties:</p> <ol> <li><p>\(r(a, a)=0\), and \(r(x, a)>0\) for all \(x\neq a\) by strict convexity of \(f\),</p> <li><p>\(r(\cdot, a)\) is also strictly convex and smooth for all \(a\in C^\circ\).</p> </ol> <p>Let us now introduce the following notation: \(r_a(\delta) := r(a+\delta, a)\) which will be quite useful. It is easy to note that \(r_a\) is smooth and strictly convex with \(r_a(0)=0\) and, in fact, is globally minimised at \(0\). By definition of strict convexity, we have</p> $$ r_a(\delta') \quad\!\! >\quad\!\! r_a(\delta) + \left\langle \delta'-\delta, \nabla r_a(\delta)\right\rangle, \quad \forall\delta' \neq \delta. $$ <p>Taking \(\delta'=0\) and rearranging terms yields</p> $$ r_a(\delta) \quad\!\! <\quad\!\! \left\langle \delta, \nabla r_a(\delta)\right\rangle \quad\!\! \le\quad\!\! \|\delta\|\|\nabla r_a(\delta)\|, $$ <p>using Cauchy-Schwartz for the second inequality. Rearranging again gives \(r_a(\delta)/\|\delta\| < \|\nabla r_a(\delta)\|\) for all \(\delta\neq 0\). Since \(r_a\) is smooth away from \(0\) and since \(\nabla r_a(0)=0\) &#40;minimiser&#41;, we can take the limit on both sides of the inequality as \(\|\delta\|\to 0\) and get the following result.</p> <div class=colbox-blue >The function \(r_a\) is \(o(\|\delta\|)\) meaning $$ \lim_{\|\delta\|\to 0} {r_a(\delta)\over \|\delta\|} \quad\!\! =\quad\!\! 0. $$</div> <p>This could have been obtained directly from consider the Taylor expansion of \(f\) but I think it&#39;s nice to obtain it using only the notion of convexity. Another note is that the reasoning still holds if \(f\) is only convex and sub-differentiable &#40;though you&#39;d need to be a bit careful &#41;</p> <h2>Admissible descent steps</h2> <p>Let&#39;s now consider a point \(x\) in \(C\) and a step from \(x\) to \(x+\delta\) for some \(\delta\neq 0\). Using the notations from the previous point, we have</p> $$ f(x+\delta) \quad\!\! =\quad\!\! f(x)+\left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta). $$ <p>Such a step will be called an <em>admissible descent step</em> if \(x+\delta\in C\) and if it decreases the function, i.e. if \(f(x+\delta) < f(x)\) or:</p> $$ \left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta) \quad\!\! <\quad\!\! 0. $$ <div class=colbox-red >ongoing</div> <h2>References</h2> <ol> <li><p><a name=eNFy ></a> <strong>El Ghaoui</strong>, <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture19.pdf">Interior-Point Methods</a>, 2012. Lecture notes at Berkeley, covering another topic &#40;IPMs&#41; but also summarising descent methods in general.</p> </ol> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 28, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>