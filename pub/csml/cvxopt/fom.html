<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>First order methods</title> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=jd-content > <h1>Thoughts on first order descent methods</h1> <p>First order methods &#40;FOM&#41; broadly designate iterative methods for continuous and differentiable optimisation that mainly use information from the &#40;sub&#41;gradient of the function.</p> <p>In these notes we consider again the constrained minimisation problem \(\min_{x\in C} f(x)\) and, to simplify the presentation, we&#39;ll assume that \(C\) is closed and convex and that \(f\) is strictly convex and smooth on \(C\).</p> <p>What we&#39;re interested in, at a high level, is how to generate a <em>minimising sequence</em> for \(f\) i.e., a sequence \(\{x_k\}\) with \(x_k\in C\) such that \(f(x_{k+1}) < f(x_k)\) and \(f(x_k) \to f(x^\dagger)\) as \(k\) grows.</p> <p><em>Remark</em>: all norms \(\|\cdot\|\) are 2-norms.</p> <h2>Local linearisation</h2> <p>Local linearisation is basically what first order methods are about: form a linear approximation of the function around a current point and use that to move in a promising direction. Still, let&#39;s try to look at this from scratch: consider the local linearisation of \(f\) around a point \(a\in C^\circ\):</p> $$ f(x) \quad\!\! =\quad\!\! f(a) + \left\langle x-a, \nabla f(a)\right\rangle + r(x, a) $$ <p>where \(r(x, a)\) is the <em>remainder function</em>.</p> <p><strong>Note</strong>: here you may be thinking: <em>Taylor expansion</em>, but actually let&#39;s put that on the side for now and just assume that you&#39;re building this \(f(a)+\left\langle x-a,\nabla f(a)\right\rangle\) and that you want to investigate the properties of the remainder function.</p> <p>That function enjoys the following properties:</p> <ol> <li><p>\(r(a, a)=0\), and \(r(x, a)>0\) for all \(x\neq a\) by strict convexity of \(f\),</p> <li><p>\(r(\cdot, a)\) is also strictly convex and smooth for all \(a\in C^\circ\).</p> </ol> <p>Let us introduce a more compact notation: \(r_a(\delta) := r(a+\delta, a)\) which will be quite useful. It is easy to note that \(r_a\) is smooth and strictly convex with \(r_a(0)=0\) and, in fact, is globally minimised at \(0\). By definition of strict convexity, we have</p> $$ r_a(\delta') \quad\!\! >\quad\!\! r_a(\delta) + \left\langle \delta'-\delta, \nabla r_a(\delta)\right\rangle, \quad \forall\delta' \neq \delta. $$ <p>Taking \(\delta'=0\) and rearranging terms yields</p> $$ r_a(\delta) \quad\!\! <\quad\!\! \left\langle \delta, \nabla r_a(\delta)\right\rangle \quad\!\! \le\quad\!\! \|\delta\|\|\nabla r_a(\delta)\|, $$ <p>using Cauchy-Schwartz for the second inequality. Rearranging again gives \(r_a(\delta)/\|\delta\| < \|\nabla r_a(\delta)\|\) for all \(\delta\neq 0\). Since \(r_a\) is smooth away from \(0\) and since \(\nabla r_a(0)=0\) &#40;minimiser&#41;, we can take the limit on both sides of the inequality as \(\|\delta\|\to 0\) and get the following result.</p> <div class=colbox-blue >The function \(r_a\) is \(o(\|\delta\|)\) meaning <a id=R3Ni ></a>$$ \lim_{\|\delta\|\to 0} {r_a(\delta)\over \|\delta\|} \quad\!\! =\quad\!\! 0. $$</div> <p>This could have been obtained directly from consider the Taylor expansion of \(f\) but I think it&#39;s nice to obtain it using only the notion of convexity. Another note is that the reasoning essentially still holds if \(f\) is only convex and sub-differentiable.</p> <h2>Admissible descent steps</h2> <p>Let&#39;s consider a point \(x\) in \(C\) and a step from \(x\) to \(x+\delta\) for some \(\delta\neq 0\). We&#39;re interested in determining what are &quot;good&quot; steps \(\delta\) to take. Using the notations from the previous point, we have</p> $$ f(x+\delta) \quad\!\! =\quad\!\! f(x)+\left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta). $$ <p>Such a step will be called an <em>admissible descent step</em> if \(x+\delta\in C\) and if it decreases the function, i.e. if \(f(x+\delta) < f(x)\) or:</p> <a id=A0Qz ></a>$$ \left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta) \quad\!\! <\quad\!\! 0. $$ <p>Let \(\mathcal D_x\) be the set of admissible descent steps from \(x\). Observe that it is always non-empty provided that \(0<\|\nabla f(x)\|<\infty\).</p> <p>To show this, let \(\delta_\epsilon := -\epsilon(g+v)\) with \(\epsilon>0\), \(g=\nabla f(x)/\|\nabla f(x)\|\) &#40;the unit vector in direction of the gradient&#41; and \(v\) such that \(\left\langle v, \nabla f(x)\right\rangle=0\) and \(0 < \|v\|\le 1\). Then just by plugging things in we have $$\left\langle \delta_\epsilon, \nabla f(x)\right\rangle + r_x(\delta_\epsilon) \quad\!\! =\quad\!\! -\epsilon + r_x(\epsilon(g+v)) $$ but recall that \(r_x(\epsilon(g+v)) = o(\epsilon\|g+v\|)\) by <span class=eqref >(<a href="#R3Ni">4</a>)</span>; by definition of \(v\) and the triangular inequality we also have that \(\|g+v\|\le 2\epsilon\) so that \(r_x(\delta_\epsilon)=o(\epsilon)\) so that, for sufficiently small \(\epsilon\), the condition <span class=eqref >(<a href="#A0Qz">6</a>)</span> holds for \(\delta_\epsilon\).</p> <div class=colbox-blue >For sufficiently small \(\epsilon\), \(\delta_\epsilon=-\epsilon(g+v)\) is an admissible descent step.</div> <p>Note that, by construction, these \(\delta_\epsilon\) span a half-ball of radius \(\epsilon\) so that \(\mathcal D_x\) is non-empty and also non-degenerate.</p> <div class=colbox-blue >Let \(w\) be such that \(0<\|w\|=:\eta\) and \(\left\langle w, \nabla f(x)\right\rangle<0\), then, provided \(\eta\) is small enough, \(w\in\mathcal D_x\).</div> <p>Obviously, what we would like is to get the <em>best possible step</em>:</p> <a id=57xf ></a>$$ \delta^\dagger \quad\!\! \in\quad\!\! \arg\min_{\delta \mid x+\delta \in C} \,\, \left[\left\langle \delta, \nabla f(x)\right\rangle+r_x(\delta)\right] $$ <p>which leads directly to the minimiser \(x^\dagger = x+\delta^\dagger\). Of course that&#39;s a bit silly since solving <span class=eqref >(<a href="#57xf">8</a>)</span> is as hard as solving the original problem. However the expression <span class=eqref >(<a href="#57xf">8</a>)</span> will help us generate descent algorithms.</p> <h2>Local update schemes</h2> <p>What we would like is thus to consider a problem that is simpler than <span class=eqref >(<a href="#57xf">8</a>)</span> and yet still generates an admissible descent direction &#40;and iterate&#41;. A natural way to try to do just that is to replace \(r(z,x)\) and \(r_x(\delta)\) by <em>a proxy function</em> \(d(z,x)\) and corresponding \(d_x(\delta)\) enjoying the same properties of positive definiteness and strict convexity. The corresponding approximate problem is then</p> <a id=rS3O ></a>$$ \tilde\delta_\beta \quad\!\! \in\quad\!\! \arg\min_{\delta \mid x+\delta \in C} \left[\left\langle \delta, \nabla f(x)\right\rangle + \beta d_x(\delta)\right]. $$ <p>Let&#39;s now show that these problems can lead to admissible descent steps for the original problem. We can follow a similar reasoning to that which led us to show the non-degeneracy of \(\mathcal D_x\). In particular, observe that as \(\beta\to\infty\), \(\|\tilde\delta_{\beta}\|\to 0\). Hence, there exists a \(\beta^\bullet\) large enough such that for any \(\nu \ge \beta^\bullet\), \(\|\tilde\delta_\nu\|\) is small enough for \(\tilde\delta_\nu\) to be in \(\mathcal D_x\).</p> <h3>GPGD is back</h3> <p>Now that we know that <span class=eqref >(<a href="#rS3O">9</a>)</span> can lead to an admissible step, we can suggest iterating over the problem with a sequence of \(\{\beta_k\}\):</p> $$ \tilde\delta_{\beta_k} \quad\!\! \in\quad\!\! \arg\min_{\delta\mid x_k+\delta \in C} \,\, \left[\left\langle \delta, \nabla f(x_k)\right\rangle + \beta_kd_x(\delta_k)\right].$$ <p>However, basic manipulation of that expression show that this is in fact the generalised projected gradient descent &#40;GPGD&#41; that we <a href="/pub/csml/cvxopt/mda.html">saw before</a>.</p> <div class=colbox-blue >The generalised projected gradient descent &#40;GPGD&#41; corresponds to the following iteration:</p> $$ x_{k+1} \quad\!\! \in\quad\!\! \arg\min_{x\in C} \left\{\left\langle x, \nabla f(x_k)\right\rangle + {1\over \alpha_k}d(x, x_k)\right\} $$ <p>for some \(\alpha_k>0\) and for any positive-definite function \(d\) that is strictly convex in its first argument. It generates a minimising sequence for \(f\) provided the \(\alpha_k\) are small enough.</div> <p>This may seem like it&#39;s not telling us that much, in particular it should be clear that you could pick the \(\alpha_k\) infinitesimally small, that it would indeed give you a minimising sequence but also that it wouldn&#39;t bring you very far. So at this point there&#39;s two comments we can make:</p> <ol> <li><p>ideal \(\alpha_k\) encapsulate a tradeoff between leading to steps that are too big and may not be admissible an steps that are too small to provide useful improvement,</p> <li><p>a key element that should hopefully be obvious by now corresponds to how we can interpret \(d\): if we know nothing about the function at hand, we can just use a default \(\|\cdot\|_2^2\) but if we <em>do</em> know something useful about the function &#40;and, in fact, about \(C\)&#41;, then that could be encoded in \(d\).</p> </ol> <h3>Choice of distance and problem structure</h3> <p>The second point is <em>very important</em>: it should be clear to you that you&#39;d want the local problems to be as informed as possible while at the same time you&#39;d want the iterations to not be overly expensive to compute, two extreme cases being:</p> <ul> <li><p>\(d(x, x_k)/\alpha_k = \|x-x_k\|_2^2/{2\alpha_k}\) and \(\alpha_k\) small, the iterations are cheap to compute but potentially quite poor at decreasing the function, many steps are needed, minimal use of problem structure,</p> <li><p>\(d(x, x_k)/\alpha_k = r(x, x_k)\), the iteration is maximally expensive but only a single step is needed; maximal use of problem structure.</p> </ul> <p>This key tradeoff can be exposed in most iterative methods using gradient information that you&#39;ll find in the literature.</p> <h2>References</h2> <ol> <li><p><a id=eNFy ></a> <strong>El Ghaoui</strong>, <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture19.pdf">Interior-Point Methods</a>, 2012. â€“ Lecture notes at Berkeley, covering another topic &#40;IPMs&#41; but also summarising descent methods in general.</p> </ol> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: December 16, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>