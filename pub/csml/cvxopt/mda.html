<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport content="width=device-width, initial-scale=1"> <link rel=stylesheet href="/css/main.css"> <link rel=icon href="/assets/infra/favicon.png"> <title>Mirror descent</title> <link rel=stylesheet href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h1>Mirror descent</h1> <h2>From Euclidean to arbitrary projections</h2> <p>In the notes on the <a href=" /pub/csml/cvxopt/pgd.html">projected gradient descent</a> &#40;PGD&#41; we showed that a way to solve the constrained minimisation problem with a differentiable \(f\) is to follow the iteration</p> $$ x_{k+1} \quad\!\! =\quad\!\! \pi_C(x_k - \alpha_k \nabla f(x_k)), $$ <p>where \(\pi_C\) denotes the Euclidean projection onto \(C\). By definition of the Euclidean projection, this can also be written as</p> $$\begin{aligned} x_{k+1} \quad\!\!&=\quad\!\! \arg\min_{x\in C}\,\, \|(x_k-\alpha_k\nabla f(x_k))-x\|_2^2 \\ &=\quad\!\! \arg\min_{x\in C} \,\,\| (x-x_k) + \alpha_k\nabla f(x_k)\|_2^2. \end{aligned}$$ </p> <p>Now, since the \(\ell^2\)-norm is induced by the inner product, we can use the decomposition</p> $$ \|(x-x_k) + \alpha_k\nabla f(x_k)\|_2^2 \quad\!\! =\quad\!\! \|x-x_k\|_2^2 + 2\alpha_k\left\langle x, \nabla f(x_k)\right\rangle + M_k, $$ </p> <p>where \(M_k\) does not depend on \(x\) &#40;and can thus be ignored&#41;. Rearranging terms, we&#39;re left with the equivalent iteration:</p> $$ x_{k+1} \quad\!\! =\quad\!\! \arg\min_{x\in C}\,\, \left\{\left\langle x, \nabla f(x_k)\right\rangle + {1\over \alpha_k} {\|x-x_k\|_2^2\over 2}\right\}. $$ </p> <p>This new way of writing the PGD allows two important comments:</p> <ol> <li><p>the objective shows how the PGD corresponds to a tradeoff between following the direction of the negative gradient &#40;first term&#41; and not moving too much from the current point &#40;second term&#41;,</p> <li><p>the second term is an isotropic measure of distance from the current point \(x_k\), what if we used another measure of distance?</p> </ol> <p>The first point might be obvious to you if you&#39;re already seen the gradient descent and related methods but it&#39;s actually deeper than what it may seems, I&#39;ll discuss this more in <a href=" /pub/csml/cvxopt/fom.html">thoughts on first order methods</a>.</p> <p>The second point gives the <em>generalised projected gradient descent</em>.</p> <div class=colbox-blue >The <em>generalised projected gradient descent</em> &#40;GPGD&#41; is defined for any positive-definite function \(d:\mathbb R^n\times\mathbb R^n\mapsto\mathbb R^+\) by the following iteration: <a name=iZ5U ></a>$$ x_{k+1} \quad\!\! =\quad\!\! \arg\min_{x\in C}\,\, \left\{\left\langle x, \nabla f(x_k)\right\rangle + {1\over \alpha_k} d(x, x_k)\right\}. $$</div> <p>We&#39;ll call such positive-definite functions <em>divergences</em>. Recall that such a function verifies \(d(u, v)>0\) for \(u\neq v\) and \(d(u, u)=0\).</p> <p>One reason why one might want to consider another distance-like function to penalise how much we move in a particular direction is that doing so can better reflect what we may know about the geometry of \(C\) which can make steps easier to compute or convergence to a minimiser faster.</p> <p>For a given divergence, there now remains to compute the corresponding iteration steps which may be intractable depending on \(d\). But there happens to be a popular class of divergences which <a href=" /pub/csml/cvxopt/ca3.html">we&#39;ve already discussed</a>: Bregman divergences.</p> <h2>Bregman divergences and the mirror descent algorithm</h2> <p>Recall that for a <em>strictly convex</em> and differentiable function \(\psi\), we can define the <em>Bregman divergence</em> associated with \(\psi\), a positive-definite function \(B_\psi\) with</p> $$ B_\psi(x, y) \quad\!\! :=\quad\!\! \psi(x)-\psi(y)-\left\langle x - y, \nabla \psi(y)\right\rangle. $$ <p>Let us now consider a \(\mu\)-<em>strongly convex</em> and differentiable function \(\varphi\) and the associated Bregman divergence \(B_\varphi\). If we use this as divergence in <span class=eqref >(<a href="#iZ5U">5</a>)</span>, the GPGD iteration is</p> $$ x_{k+1} \quad\!\! \in\quad\!\! \arg \min_{x\in C} \,\,\left\{ \left\langle x, \nabla f(x_k)\right\rangle + {1\over \alpha_k}B_\varphi(x, x_k)\right\} $$ <p>for \(\alpha_k>0\). It&#39;s straightforward to take the gradient of the objective and we can thus write the FOC for one step:</p> $$ 0 \quad\!\! \in\quad\!\! \alpha_k\nabla f(x_k) + \nabla \varphi(x_{k+1}) - \nabla \varphi (x_k) + N_C(x_{k+1}), $$ <p>which, after rearranging terms, reads</p> $$ x_{k+1} \quad\!\! \in\quad\!\! (\nabla\varphi + N_C)^{-1} (\nabla \varphi(x_k) - \alpha_k \nabla f(x_k)).$$ <p>To complete this, observe that \((\nabla \varphi + N_C) \equiv \partial (\varphi + i_C)\) so that letting \(\phi \equiv \varphi + i_C\), and using that for \(h\in \Gamma_0\), \((\partial h)^{-1}\equiv\partial h^\star\) &#40;cf. <a href=" /pub/csml/cvxopt/ca2.html">convex analysis part 2</a>&#41;, we end up with the <em>mirror descent algorithm</em>.</p> <div class=colbox-blue >The <em>mirror descent algorithm</em> &#40;MDA&#41; is the iteration $$ x_{k+1} \quad\!\! =\quad\!\! \nabla\phi^\star (\nabla \varphi(x_k) - \alpha_k\nabla f(x_k)) $$ where \(\phi^\star(y) = \sup_{z\in C}[\left\langle z,y\right\rangle-\varphi(z)]\).</div> <p>Note that \(\phi\) is also strongly convex on \(C\) so that it is differentiable which is why we can write \(\nabla \phi^\star\) &#40;its gradient is even Lipschitz as we showed in <a href=" /pub/csml/cvxopt/ca3.html">convex analysis part 3</a>&#41;.</p> <p>A final note is that, as for the PGD, the whole development of the MDA by relaxing the differentiability of \(f\) to sub-differentiability without much change, the iteration is then \(x_{k+1}=\nabla\phi^\star(\nabla \varphi(x_k)-\alpha_k f'(x_k))\) with \(f'(x_k)\in \partial f(x_k)\).</p> <h2>References</h2> <ol> <li><p><a name=fnuL ></a> <strong>Beck</strong> and <strong>Teboulle</strong>, <a href="https://web.iem.technion.ac.il/images/user-files/becka/papers/3.pdf">Mirror descent and nonlinear projected subgradient methods for convex optimization</a>, 2003. The paper behind the MDA, it also presents a convergence analysis and gives an example of application.</p> <li><p><a name=8q3r ></a> <strong>Nemirovski</strong>, <a href="https://www2.isye.gatech.edu/~nemirovs/COLT2012Tut.pdf">Tutorial: mirror descent algorithms for large-scale deterministic and stochastic convex optimization</a>, 2012. A deck of slides from a 2012 presentation, covering the MDA as well as applications.</p> </ol> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 27, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>