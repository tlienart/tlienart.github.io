<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>Variational Bayes</title> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=jd-content > <h1>Variational Bayes</h1> <p>In VB the starting point is the variational problem where the discrepancy measure that we attempt to minimise is \(\mathrm{KL}(q, p)\) with \(p\) the target distribution, \(q\) the proxy and \[\begin{array}{c} \mathrm{KL}(q, p) &=& \mathbb E_q[\log q(X) - \log p(X)]. \end{array}\] The KL is a divergence and so verifies \(\mathrm{KL}(q, p)\ge 0\) and \(\mathrm{KL}(p,p)=0\) for all \(p, q\in\mathcal P(\mathcal X)\) &#40;which is easy to prove&#41;. Of course if we try to minimise it without further constraining the space in which candidates \(q\) belong, we find the trivial answer \(q^\star = p\).</p> <h2>Mean-Field approximation</h2> <p>A way forward is to consider the space of distributions that fully factorise &#40;<em>mean-field</em> approximation&#41;: \[ \mathcal F_{_{\text{MF}}} \quad\!\! :=\quad\!\! \left\{q \mid q(x)\propto \prod_{i=1}^d q_i(x_i)\right\}. \] Note that we could relax this a bit by considering the space of distributions that only factorise according to a specific graphical model but that may generally not lead to as nice simplifications as with &quot;naive&quot; MF and typically requires extra assumptions &#40;but it&#39;s doable, see e.g. <span class=bibref ><a href="#yMFW">Ghahramani and Beal (2000)</a></span> or search <em>generalised mean field</em>&#41;.</p> <p>Under \(q\in\mathcal F_{_{\text{MF}}}\), the KL decomposes as <a id=YTWH ></a>\[ \mathrm{KL}(q, p) \quad\!\! =\quad\!\! \mathbb E_q\left[\sum_i \log q_i(X_i) - \log p(X)\right] + \kappa \] where \(\kappa\) corresponds to the log of the normalisation constant of \(q\) and can be ignored in the minimisation.</p> <h3>Coordinate descent</h3> <p>Let us denote \(q_{\neg i}\) the distribution proportional to \(q/q_i\) so that we can write \(\mathbb E_q[\cdot]=\mathbb E_{q_i}[\mathbb E_{q_{\neg i}}[\cdot]]\). The expectation in <span class=eqref >(<a href="#YTWH">3</a>)</span> can then be expressed as <a id=bDMj ></a>\[ \mathbb E_{q_i}\left[ \log q_i(X_i) - \mathbb E_{q_{\neg i}}[\log p(X)] \right] + \mathbb E_{q_{\neg i}}\left[\sum_{j\neq i} \log q_j(X_j)\right]. \] This is amenable to a coordinate descent scheme &#40;see e.g. <span class=bibref ><a href="#w9TA">Wright (2015)</a></span>&#41; where we cyclically optimise one of the \(q_i\) while keeping all the others fixed. Keeping all \(q_j\) fixed with \(j\neq i\), the second term in <span class=eqref >(<a href="#bDMj">4</a>)</span> can be ignored and we&#39;re left with <a id=8dtf ></a>\[ \min_{q_i \in \mathcal P(\mathcal X_i)} \,\, \mathbb E_{q_i}[\log q_i(X_i) - \mathbb E_{q_{\neg i}}[\log p(X)]]. \] The second expectation can be written as \(\log g_u(X_i)\) for some positive function \(g_u\). Let&#39;s further assume that \(g_u\) is integrable so that there exists a constant \(Z\) with \(g:=g_u/Z\) a probability distribution function over \(\mathcal X_i\). Then, since \(Z\) does not depend on \(q_i\), we can ignore it and the problem <span class=eqref >(<a href="#8dtf">5</a>)</span> effectively amounts to \[ \min_{q_i \in\mathcal P(\mathcal X_i)} \,\, \mathrm{KL}(q_i, g), \] for which a solution is simply \(q_i^\star=g\) or \[ q_i^\star \quad\!\! \propto\quad\!\! \exp(\mathbb E_{q_{\neg i}}[\log p(X)]). \] <p>Putting the bits together, MFVB at its core corresponds to the following iteration: \[\begin{aligned} q_i^{t+1} \quad\!\!&\propto\quad\!\! \exp(\mathbb E_{q_{\neg i}}[\log p(X)])\\ q \quad\!\!&\propto\quad\!\! q(q_i^{t+1}/ q_i^t)\\ q_{\neg (i+1)} \quad\!\!&\propto\quad\!\! q/q_(i+1)^t \end{aligned}\] where the second and third line indicate how \(q\) and \(q_{\neg j}\) are updated. There is thus an outer loop over the number of descent steps \(t=1,2,\dots\) and an inner loop over each of the dimension \(i=1,\dots,d\).</p> <h3>Computing the update</h3> <h3>Convergence and quality</h3> <p>A nice thing about MFVB is that the scheme provably <strong>converges</strong> in general &#40;thanks to the convergence properties of coordinate descent&#41;. This is generally a very appreciated property of any schemes though your next question should be <em>what does it converge to and how good is it?</em>. To which the answer is, it converges to \(q^\star \in \mathcal F_{_{\text{MF}}}\) and it can be... quite bad in general.</p> <p>To see how bad, let&#39;s consider a simple 2D Gaussian \(p\) with zero mean and a dense covariance matrix. MFVB will give you something like \(q(x_1)q(x_2) \approx p(x_1, x_2)\). So without even doing any of the calculations, you can already see that cross terms in \(x_1x_2\) will be completely ignored. If the off-diagonal terms of the covariance matrix are large, this can be very bad.</p> <p>To summarise, MFVB will give a very poor proxy for any target distribution which has strong dependence structure between &#40;some of&#41; its variables. As a result it may severely underestimate the uncertainty of model variables.</p> <h2>References</h2> <ol> <li><p><a id=eNFy ></a> <strong>Fox</strong> and <strong>Roberts</strong>, <a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf">A tutorial on variational Bayesian inference</a>, 2011. – Introductory notes covering VB and MFVB.</p> <li><p><a id=yMFW ></a> <strong>Ghahramani</strong> and <strong>Beal</strong>, <a href="http://papers.nips.cc/paper/1907-propagation-algorithms-for-variational-bayesian-learning.pdf">Propagation algorithms for variational Bayesian learning</a>, 2000. – NIPS paper discussing VB with different restrictions for the space of proxy distributions \(\mathcal F\).</p> <li><p><a id=w9TA ></a> <strong>Wright</strong>, <a href="https://arxiv.org/pdf/1502.04759.pdf">Coordinate descent algorithms</a>, 2015. – More than you wanted to know about coordinate descent algorithms with proofs of convergence.</p> </ol> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: March 26, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script>