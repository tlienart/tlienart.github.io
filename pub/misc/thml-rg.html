<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>ThML learning reading group</title> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=jd-content > <h1><a id=theoretical-ml-reading-group  href="#theoretical-ml-reading-group">Theoretical ML reading group</a></h1> <p>The aim of the reading group is to discuss key results from interesting recent theory papers from venues such as JMLR, NIPS, ICML or similar without spending too much time on proofs unless warranted by the discussion. Applied papers and deep learning papers would generally be avoided.</p> <h2><a id=spirit  href="#spirit">Spirit</a></h2> <p>The format is a ~30-40 min presentation <em>leaving ample room for discussion</em>; white-board presentations are preferred. The purpose of these reading groups is heavily focused on the discussion part, in particular the strengths and weaknesses of the paper, links with existing literature and possible applications or further work.</p> <p>In order for this reading group to be a success, all participants are expected to present at some point &#40;especially PhDs and postdocs&#41;. A bit of friendly chasing will hopefully ensure this happens ðŸš‚.</p> <h2><a id=logistics  href="#logistics">Logistics</a></h2> <ul> <li><p>ðŸ“† <strong>schedule</strong>: the reading groups take place on Fridays from 10am to 11am,</p> <li><p>ðŸŽª <strong>room</strong>: McDonnell, usually 7.02 unless otherwise mentioned,</p> <li><p>ðŸ“§ <strong>mailing list</strong>: you can subscribe by <a href="https://lists.unimelb.edu.au/subscribe/ml-theory">clicking this link</a>.</p> </ul> <h2><a id=presentations  href="#presentations">Presentations</a></h2> <h3><a id=upcoming  href="#upcoming">Upcoming</a></h3> <p><strong>Note</strong>: change of time from Thursday afternoon to Friday morning&#33;</p> <ul> <li><p>&#40;<strong>Jun 7, &#39;19</strong>&#41; <a href="https://arxiv.org/abs/1904.01681">Augmented Neural ODEs</a> by Dupont, Doucet and Teh. <em>Presenter: Thibaut Lienart</em>.</p> <li><p>&#40;<strong>Jun 14, &#39;19</strong>&#41; <a href="http://mcts.ai/pubs/mcts-survey-master.pdf">A survey of Monte Carlo Tree Search Methods</a> by Browne et al. <em>Presenter: Dongge Liu</em>.</p> </ul> <h3><a id=past  href="#past">Past</a></h3> <ul> <li><p>&#40;<strong>May 31, &#39;19</strong>&#41; <a href="https://epubs.siam.org/doi/pdf/10.1137/18M1183480">Tutorial on Thomson Sampling for Bandits algorithms</a>. <em>Presenter: Bastian Oetomo</em>.</p> <li><p>&#40;<strong>May 23, &#39;19</strong>&#41; <a href="https://epubs.siam.org/doi/pdf/10.1137/18M1183480">Why are big data matrices approximately low rank</a> by Udell and Townsend. <em>Presenter: Victor Fedyashov</em>.</p> <li><p>&#40;<strong>Apr 18, &#39;19</strong>&#41; <a href="https://arxiv.org/pdf/1806.07366.pdf">Neural Ordinary Differential Equations</a> by Chen, Rubanova, Bettencourt and Duvenaud. <em>Presenter: Thibaut Lienart</em>.</p> <li><p>&#40;<strong>Mar 7, &#39;19</strong>&#41; <a href="https://protect-au.mimecast.com/s/t9StCE8knvs5D9qVuNz-y6?domain&#61;simons.berkeley.edu ">Adaptive Data Analysis</a> by Roth. <em>Presenter: Ben Rubinstein</em>.</p> <li><p>&#40;<strong>Feb 21, &#39;19</strong>&#41; <a href="https://papers.nips.cc/paper/6980-conservative-contextual-linear-bandits">Conservative contextual linear bandits</a> by Kazerouni et al. <em>Presenter: Masoud Khorasani</em>.</p> <li><p>&#40;<strong>Feb 14, &#39;19</strong>&#41; <a href="https://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf">Is Q-Learning provably efficient</a> by Jin et al. <em>Presenter: Neil Merchant</em>.</p> <li><p>&#40;<strong>Feb 7, &#39;19</strong>&#41; <a href="http://proceedings.mlr.press/v48/shalev-shwartzb16-supp.pdf">Minimising the maximal loss: how and why</a> by Shalev-Shwartz and Wexler. <em>Presenter: Yi Han</em>.</p> <li><p>&#40;<strong>Jan 31, &#39;19</strong>&#41; <a href="https://arxiv.org/abs/1811.08413">Sampling can be faster than optimization</a> by Ma, Chen, Jin, Flammarion and Jordan. <em>Presenter: Miquel RamÃ­rez</em>.</p> <li><p>&#40;<strong>Jan 24, &#39;19</strong>&#41; <a href="http://jmlr.org/papers/volume18/16-491/16-491.pdf">Second-order stochastic optimisation for machine learning in linear time</a> by Agarwal, Bullins and Hazan, JMLR 2017. <em>Presenter: Bastian Oetomo</em>.</p> <li><p>&#40;<strong>Dec 20, &#39;18</strong>&#41; <a href="http://www.argmin.net/2018/06/25/outsider-rl/">An Outsider&#39;s Tour of RL</a> by Recht. <em>Presenter: Thibaut Lienart</em>.</p> <li><p>&#40;<strong>Dec 13, &#39;18</strong>&#41; <a href="https://arxiv.org/abs/1805.11505">Classification with imperfect training labels</a> by Cannings, Fan and Samworth, arXiv &#39;18. <em>Presenter: Yi Han</em>.</p> <li><p>&#40;<strong>Nov 29, &#39;18</strong>&#41; <a href="http://jmlr.org/papers/volume18/17-269/17-269.pdf">To tune or not to tune the number of trees in random forest</a> by Probst and Boulesteix, JMLR 2018. <em>Presenter: Bastian Oetomo</em>.</p> <li><p>&#40;<strong>Nov 22, &#39;18</strong>&#41; <a href="http://jmlr.org/papers/volume18/16-632/16-632.pdf">An optimal algorithm for bandit and zero-order convex optimisation with two-point feedback</a> by Shamir, JMLR 2017. <em>Presenter: Dongge Liu</em>.</p> <li><p>&#40;<strong>Nov 15, &#39;18</strong>&#41; <a href="http://jmlr.org/papers/volume18/15-240/15-240.pdf">Explaining the success of adaboost and random forests as interpolating classifiers</a> by Wyner, Olson, Bleich and Mease, JMLR 2017. <em>Presenter: Neil Merchant</em>.</p> <li><p>&#40;<strong>Nov 1, &#39;18</strong>&#41; <a href="http://jmlr.org/papers/volume19/17-006/17-006.pdf">Can we trust the bootstrap in high dimension</a> by El Karoui and Purdom, JMLR 2017. <em>Presenter: Thibaut Lienart</em>.</p> </ul> <h3><a id=suggested-papers-to-review  href="#suggested-papers-to-review">Suggested papers to review</a></h3> <p>To add papers to this list, please send me an email or let me know, it is just a draft at this point. The papers are &#40;a bit arbitrarily&#41; sorted in two blocks with the first one judged to have priority as potentially more likely to be of interest to everyone. At some point in the future, I&#39;ll organise the list with indicators of the topics.</p> <h4><a id=priority-block  href="#priority-block">priority block</a></h4> <h4><a id=secondary-block  href="#secondary-block">secondary block</a></h4> <ul> <li><p><a href="http://jmlr.org/papers/volume18/15-205/15-205.pdf">On Markov chain Monte Carlo methods for tall data</a> by Bardenet, Doucet and Holmes, JMLR 2017.</p> <li><p><a href="http://proceedings.mlr.press/v37/romera-paredes15.pdf">An embarrassingly simple approach to zero-shot learning</a> by Romera-Paredes and Torr, ICML 2015.</p> <li><p><a href="http://papers.nips.cc/paper/5925-on-the-global-linear-convergence-of-frank-wolfe-optimization-variants.pdf">On the global linear convergence of Frank-Wolfe optimization variants</a> by Lacoste-Julien and Jaggi, NIPS 2015.</p> <li><p><a href="https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf">Fast and provably good seedings for k-means</a> by Bachem, Lucic, Hassani and Krause, NIPS 2016.</p> <li><p><a href="http://www.jmlr.org/papers/volume17/gittens16a/gittens16a.pdf">Revisiting the Nystrom method for improved large-scale machine learning</a>, by Gittens and Mahoney, JMLR 2016.</p> <li><p><a href="http://jmlr.org/papers/volume18/15-205/15-205.pdf">Hamiltonian descent methods</a> by Maddison, Paulin, Teh, Donoghue and Doucet, ArXiv 2018.</p> <li><p><a href="http://jmlr.org/papers/volume18/16-491/16-491.pdf">Robust and scalable bayes via a median of subset posterior measures</a> by Minsker, Srivastava, Lin and Dunson, JMLR 2017.</p> <li><p><a href="http://papers.nips.cc/paper/6445-fast-algorithms-for-robust-pca-via-gradient-descent.pdf">Fast algorithms for robust PCA via gradient descent</a> by Yi, Park, Chen and Caramanis, NIPS 2016.</p> <li><p><a href="http://jmlr.org/papers/volume18/15-143/15-143.pdf">Adaptive randomized dimension reduction on massive data</a> by Darnell, Georgiev, Mukherjee and Engelhardt, JMLR 2017.</p> </ul> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: June 06, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div>