<!doctype html> <html lang=en  class=h-100 > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta name=description  content="Thibaut Lienart's website"> <meta name=author  content="Thibaut Lienart"> <meta name=generator  content=Franklin > <link rel=icon href="/assets/favicon.png"> <link href="/css/bootstrap.min.css" rel=stylesheet > <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <script src="/libs/clipboard.min.js"></script> <title>GMRES & Conjugate Gradient &ndash; pt. I</title> <link rel=stylesheet  href="/css/main.css"> <style> :root { --oxb: #002147; --camb: #A3C1AD; --banana: #FFE135; --lgray: #D3D3D3; } .bd-placeholder-img { font-size: 1.125rem; text-anchor: middle; -webkit-user-select: none; -moz-user-select: none; user-select: none; } @media (min-width: 768px) { .bd-placeholder-img-lg {font-size: 3.5rem;} } .border-oxb {border-color: var(--oxb);} .bg-oxb {background-color: var(--oxb);} .text-oxb {color: var(--oxb);} .border-camb {border-color: var(--camb);} .bg-camb {background-color: var(--camb);} .text-camb {color: var(--camb);} </style> <body class="d-flex flex-column h-100"> <header> <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> <div class=container-fluid > <a class=navbar-brand  id=main-name  href="/">T. Lienart</a> <ul class=navbar-nav  id=main-nav > <li class=nav-item > <a class="nav-link " aria-current=page  href="/posts/">Posts</a> <li class=nav-item > <a class="nav-link " href="/tags/">Tags</a> <li class=nav-item > <a class="nav-link " href="/about/">About</a> </ul> <ul class="navbar-nav me-auto" id=title-nav > <li class=nav-item  id=post-title > <a class="nav-link active" href="#">GMRES & Conjugate Gradient &ndash; pt. I</a> </ul> </div> </nav> </header> <main class=flex-shrink-0 > <div class=container > <div class=pt-5 > </div> <h1 id=pretitle_part_i ><a href="#pretitle_part_i" class=header-anchor >GMRES & Conjugate Gradient &#40;part I&#41;</a></h1> <div class=tags ><a href="/tags/" id=tag-icon ><svg width=20  height=20  viewBox="0 0 512 512"><defs><style>.cls-1{fill:#141f38}</style></defs><path class=cls-1  d="M215.8 512a76.1 76.1 0 0 1-54.17-22.44L22.44 350.37a76.59 76.59 0 0 1 0-108.32L242 22.44A76.11 76.11 0 0 1 296.2 0h139.2A76.69 76.69 0 0 1 512 76.6v139.19A76.08 76.08 0 0 1 489.56 270L270 489.56A76.09 76.09 0 0 1 215.8 512zm80.4-486.4a50.69 50.69 0 0 0-36.06 14.94l-219.6 219.6a51 51 0 0 0 0 72.13l139.19 139.19a51 51 0 0 0 72.13 0l219.6-219.61a50.67 50.67 0 0 0 14.94-36.06V76.6a51.06 51.06 0 0 0-51-51zm126.44 102.08A38.32 38.32 0 1 1 461 89.36a38.37 38.37 0 0 1-38.36 38.32zm0-51a12.72 12.72 0 1 0 12.72 12.72 12.73 12.73 0 0 0-12.72-12.76z"/><path class=cls-1  d="M217.56 422.4a44.61 44.61 0 0 1-31.76-13.16l-83-83a45 45 0 0 1 0-63.52L211.49 154a44.91 44.91 0 0 1 63.51 0l83 83a45 45 0 0 1 0 63.52L249.31 409.24a44.59 44.59 0 0 1-31.75 13.16zm-96.7-141.61a19.34 19.34 0 0 0 0 27.32l83 83a19.77 19.77 0 0 0 27.31 0l108.77-108.7a19.34 19.34 0 0 0 0-27.32l-83-83a19.77 19.77 0 0 0-27.31 0l-108.77 108.7z"/><path class=cls-1  d="M294.4 281.6a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85zM256 320a12.75 12.75 0 0 1-9.05-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2A12.8 12.8 0 0 1 256 320zM217.6 358.4a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 1 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85z"/></svg></a><a href="/tag/code/">code</a>, <a href="/tag/linear_algebra/">linear algebra</a></div> <p>Let \(A \in \mathbb R^{n\times n}\) non singular and \(b\in \mathbb R^n\), we want to find &#40;or approximate&#41; \(x\in \mathbb R^n\) such that \(Ax = b\). In this first part we briefly discuss <em>direct methods</em>, where one recovers \(x\) exactly, and a set of simple <em>iterative methods</em> including the <em>Generalised Minimal Residual Method</em> or GMRES, where one constructs a sequence \(x_k \to x\). Most of what we cover also holds for the case where either \(A\) or \(b\) have complex entries though we stick to the real case for ease of presentation.</p> <p>In the <a href="/posts/07/05-cg/">second part</a> we discuss how GMRES is related to Krylov Subspaces as well as improvements when it is known that \(A\) is positive definite or symmetric or both.</p> <div class=franklin-toc ><ol><li><a href="#direct_methods">Direct methods</a><li><a href="#iterative_methods_to_solve_a_linear_system">Iterative methods to solve a linear system</a><ol><li><a href="#minimum_residual_criterion">Minimum residual criterion</a><li><a href="#generating_directions">Generating directions</a><li><a href="#connection_to_fixed-point_methods">Connection to fixed-point methods</a></ol><li><a href="#implementation">Implementation</a><ol><li><a href="#modified-gs_step">Modified-GS step</a><li><a href="#upper_triangular_solve">Upper triangular solve</a><li><a href="#core_method">Core method</a><li><a href="#comparison">Comparison</a></ol><li><a href="#short_references">Short references</a></ol></div> <h2 id=direct_methods ><a href="#direct_methods" class=header-anchor >Direct methods</a></h2> <p>There are plenty of direct methods to solve a linear system, a common one is the <a href="https://en.wikipedia.org/wiki/LU_decomposition">LU-factorisation</a> where \(A\) is decomposed into \(A=LU\) with \(L\) a lower-triangular matrix and \(U\) an upper-triangular one.<br/> We won&#39;t cover LU here but will briefly cover another approach that leverages the QR-factorisation of \(A\) which we introduced in <a href="/posts/06/25-gram-schmidt/">a previous post</a>. If \(A=QR\) with \(Q^tQ=QQ^t=I\) and \(R\) upper-triangular, then the problem to solve becomes</p> <a id=qrls1  class=anchor ></a>\[ Rx = Q^tb. \] <p>Since \(R\) is an upper triangular matrix, this can be solved directly: denoting \(x_i\) the \(i\)-th component of \(x\) and letting \(\tilde{b} = Q^tb\), equation <span class=eqref >(<a href="#qrls1">1</a>)</span> can be expanded into</p> \[\begin{array}{rcl} r_{nn}x_n &=& \tilde{b}_n\\ r_{n-1,n-1}x_{n-1} + r_{n-1,n}x_n &=& \tilde{b}_{n-1}\\ &\vdots&\end{array}\] <p>which can be iteratively solved with</p> <a id=qrls2  class=anchor ></a>\[ x_{k} \quad\!\! =\quad\!\! r_{kk}^{-1}\left(\tilde{b}_k - \sum_{i=1}^{n-k}r_{k,k+i}x_{k+i}\right)\quad\text{for}\quad k=n,n-1,...,1. \] <p>Here&#39;s a simple implementation in Julia:</p> <pre><code class=language-julia >using StableRNGs, LinearAlgebra
rng &#61; StableRNG&#40;111&#41;

n &#61; 5
A &#61; rand&#40;rng, n, n&#41;
b &#61; rand&#40;rng, n&#41;

Q, R &#61; qr&#40;A&#41;
x &#61; zero&#40;b&#41;
b̃ &#61; Q&#39; * b
for k &#61; n:-1:1
    w &#61; b̃&#91;k&#93;
    for i &#61; 1:n-k
        w -&#61; R&#91;k, k&#43;i&#93; * x&#91;k&#43;i&#93;
    end
    x&#91;k&#93; &#61; w / R&#91;k, k&#93;
end

e &#61; round&#40;norm&#40;A * x - b&#41;, sigdigits&#61;2&#41;
println&#40;&quot;‖Ax-b‖ &#61; &#36;e&quot;&#41;</code></pre><pre><code class="plaintext code-output">‖Ax-b‖ = 4.6e-16
</code></pre> <p>For pairs \((A, b)\) with no specific properties, these direct methods typically have complexity \(\mathcal O(n^3)\). In the QR approach above, the factorisation is the most expensive part, solving the triangular system <span class=eqref >(<a href="#qrls1">1</a>)</span> is only \(\mathcal O(n^2)\). LU-based solvers typically have a better complexity constant &#40;about half that of QR&#41; and so are preferred in practice but either way, for \(n\) &quot;large&quot; &#40;typically, larger than 1000&#41;, these methods all become prohibitively expensive.</p> <p>In what follows, we consider iterative methods which start from a candidate vector \(x_0\) and generate a sequence of \(x_k\) such that \(x_k \to x\) in some sense as \(k\) increases with the hope that for some \(k\) sufficiently large &#40;and yet \(k\ll n\)&#41;, the solution is &quot;good enough&quot;. Of course each step of such methods should be cheap so that the overall cost of the method to reach a suitable approximation is significantly lower than the \(\mathcal O(n^3)\) of a direct method.</p> <h2 id=iterative_methods_to_solve_a_linear_system ><a href="#iterative_methods_to_solve_a_linear_system" class=header-anchor >Iterative methods to solve a linear system</a></h2> <p>We consider iterative methods of the form</p> <a id=iter1  class=anchor ></a>\[ x_{k} \quad\!\! =\quad\!\! x_{k-1} + \alpha_{k} p_{k} \quad\!\! =\quad\!\! x_0 + \sum_{i=1}^{k} \alpha_i p_i \] <p>for some initial candidate \(x_0\in\mathbb R^n\), a set of linearly independent \(p_k \in\mathbb R^n\) and some coefficients \(\alpha_k\in\mathbb R\), all such that \(x_k \to x\) as \(k\) increases and with \(x_n=x\) &#40;since, ultimately, the \(p_k\) form a basis of \(\mathbb R^n\)&#41;.</p> <p>Such a set of vectors \(p_k\) can be orthogonalised along the way using the Gram-Schmidt procedure &#40;which we covered in a <a href="/posts/06/25-gram-schmidt/">previous post</a>&#41; so that \(p_k\in \mathrm{span}(q_1,\dots,q_k)\) and <span class=eqref >(<a href="#iter1">4</a>)</span> can then be expressed as</p> \[ \begin{cases} q_k &\propto& \,\, p_k - \sum_{i=1}^{k-1} \left\langle p_k, q_i\right\rangle q_i\\ x_{k} &=& x_0 + \sum_{i=1}^k \beta_i q_i \end{cases} \] <p>for some coefficients \(\beta_i\in\mathbb R\). This amounts to iteratively building an orthonormal basis \(\{q_1, \dots, q_k\}\) of a subspace of dimension \(k\) and determining an approximate solution \(x_{k}\) in it.</p> <p>There&#39;s two elements that can be adjusted in the above procedure:</p> <ol> <li><p>how to pick the \(\beta_i\),</p> <li><p>how to pick the vectors \(p_{k}\).</p> </ol> <p>Both of these can be guided by how easy/cheap the iterations are, and how good an approximation \(x_{k+1}\) is to the solution \(x\).</p> <h3 id=minimum_residual_criterion ><a href="#minimum_residual_criterion" class=header-anchor >Minimum residual criterion</a></h3> <p>The criterion we&#39;ll use on this page is the <strong>minimum residual criterion</strong>. We will discuss another one in the <a href="/posts/07/05-cg/">follow up post</a>. It&#39;s a fairly natural criterion that requires taking the \(\beta_i\) such that the 2-norm of the residual \(r_{k} = Ax_{k} - b\) is minimised with</p> \[ \|Ax_k - b\|_2 \quad\!\! =\quad\!\! \left\|r_0 + \sum_{i=1}^k\beta_i Aq_i\right\|_2. \] <p>Writing \(Q^{(k)}\) the matrix with columns \(\{q_1,\dots,q_k\}\) and \(\beta^{(k)}\) the vector in \(\mathbb R^k\) with components \(\{\beta_1,\dots,\beta_k\}\), the above can be expressed in matrix form as</p> \[ \beta^{(k)} \quad\!\! =\quad\!\! \arg\min_{\beta\in\mathbb R^k} \quad\left\| r_0 + AQ^{(k)}\beta \right\|_2 \] <p>which is just a least-square regression in \(k\) dimensions. These subsequent regressions can each be solved cheaply as we show below.</p> <p>Assume we simultaneously orthogonalise the vectors \(Aq_i\) to get another orthonormal basis \(\{\tilde{q}_1,\dots,\tilde{q}_k\}\). Let \(\tilde{Q}^{(k)}\) be the matrix with columns \(\tilde{q}_i\). Then, at step \(k\), we can write \(AQ^{(k)} = \tilde{Q}^{(k)}R^{(k)}\), and the least-square problem becomes</p> \[ \gamma^{(k)} \quad\!\! =\quad\!\! \arg\min_{\gamma \in \mathbb R^k}\quad \left\| r_0 + \tilde{Q}^{(k)}\gamma \right\|_2 \quad\text{with}\quad \gamma^{(k)} = R^{(k)}\beta^{(k)}. \] <p>Since \(\tilde{Q}^{(k)}\) has orthogonal columns, the least-square solution to this problem is simply \[\begin{array}{rcl} \gamma^{(k)} &=& -({\tilde{Q}^{(k)}}^t \tilde{Q}^{(k)})^{-1}{\tilde{Q}^{(k)}}^t r_0 \\ &=& -{\tilde{Q}^{(k)}}^t r_0\end{array}\] and finding \(\beta^{(k)}\) such that \(R^{(k)}\beta^{(k)} = \gamma^{(k)}\) can be done directly and cheaply since \(R^{(k)}\) is upper triangular &#40;using the same iteration as in <span class=eqref >(<a href="#qrls2">3</a>)</span>&#41;. Note also that we get the residual \(r_k\) directly from \(\gamma^{(k)}\) since <nobr> \(r_k = r_0 + \tilde{Q}^{(k)}\gamma^{(k)}\)</nobr>.</p> <p>Picking this criterion as well as the &quot;Krylov&quot; directions &#40;see below&#41; leads to the vanilla version of the <strong>GMRES</strong> algorithm &#40;<em>Generalised Minimal Residual Method</em>&#41;.</p> <h3 id=generating_directions ><a href="#generating_directions" class=header-anchor >Generating directions</a></h3> <p>Ideally, we would like the space \(\mathcal P_k\) spanned by the \(\{p_1, \dots, p_k\}\) to be such that the projection of \(x\) on \(\mathcal P_k\) is close to \(x\). This, unfortunately, is not very easy to translate into a cheap procedure for generating a good sequence of \(p_k\) not least because we don&#39;t have access to \(x\). After executing step \(k-1\), the only new information we can compute is the new residual \(r_{k-1}\) and so, naturally, we could try using that in forming the new direction \(p_k\).</p> <p>In this point we will briefly discuss three approaches to generating the \(p_k\):</p> <ul> <li><p><strong>random</strong>, where the \(p_k\) are drawn at random e.g. with \((p_k)_i \sim \mathcal N(0, 1)\),</p> <li><p><strong>Krylov</strong>, where the \(p_k\) are the residuals \(r_{k-1}\),</p> <li><p><strong>gradient</strong>, where the \(p_k\) are \(A^tr_{k-1}\).</p> </ul> <p>The first one is not motivated by anything else than &quot;it should work&quot; but should not be expected to work very well, the second one is a common choice, that links with Krylov subspace methods &#40;as <a href="/posts/07/05-cg/">discussed in the second part</a>&#41;, and finally, the third one is motivated by the gradient of <nobr> \(F(x) = \|Ax - b\|_2^2\)</nobr> which is \(A^t(Ax-b)\) so that</p> \[ \nabla F(x_{k-1}) \quad\!\! =\quad\!\! A^tr_{k-1}. \] <p>Of course there could be other approaches than these three but they have the advantage of being simple and illustrative.</p> <p><strong>Note</strong>: in a number of important cases, \(A\) is not built explicitly. Rather we just have a procedure to compute \(Ax\) for a given \(x\). In such cases it might be impractical to compute \(A^tx\) explicitly.</p> <h3 id=connection_to_fixed-point_methods ><a href="#connection_to_fixed-point_methods" class=header-anchor >Connection to fixed-point methods</a></h3> <p>In a different context, we had discussed <a href="/posts/2018/12/08-split/">splitting methods</a> where one decomposes the matrix \(A\) into a sum: \(A=B+C\) for some matrices \(B\) and \(C\) to transform the problem into a <em>fixed point</em> equation:</p> \[ Bx = b - Cx \] <p>that can be approached with a damped fixed-point iteration:</p> \[ Bx_{k+1} = (1-\theta_k)x_k + \theta_k ( b - Cx_k) \] <p>for some \(\theta_k \in (0, 1]\). Picking \(B= I\) and \(C=(A- I)\), the iteration is</p> \[\begin{array}{rcl} x_{k+1} &=& (1-\theta_k)x_k + \theta_k(b + x_k - Ax_k)\\ &=& x_k - \theta_k r_k\end{array}\] <p>with \(r_k = Ax_k-b\). In that sense, the &quot;Krylov&quot; choice earlier can be connected to a fixed-point iteration with a specific decomposition of \(A\).</p> <h2 id=implementation ><a href="#implementation" class=header-anchor >Implementation</a></h2> <p>We&#39;ve now discussed the different moving parts and can build a simple implementation which, at step \(k\), does the following:</p> <ol> <li><p>gets a new direction \(p_k\) and finds the corresponding orthonormal \(q_k\),</p> <li><p>computes \(Aq_k\) and finds the corresponding orthonormal \(\tilde{q}_k\) as well as the \(r_{i,k}\),</p> <li><p>computes \(\gamma_k = - (\tilde{Q}^{(k)})^t r_0\),</p> <li><p>solves \(R^{(k)}\beta^{(k)}=\gamma_k\) &#40;a triangular system of size \(k\)&#41;.</p> </ol> <p>For step &#40;1&#41; and &#40;2&#41; we can use the modified Gram-Schmidt procedure.</p> <p>In the points below, we show code that solves each of these steps and ultimately put them all together to form a working iterative solver. The code is not optimised but should hopefully be easy to read and to analyze.</p> <h3 id=modified-gs_step ><a href="#modified-gs_step" class=header-anchor >Modified-GS step</a></h3> <p>This code computes the \(k\)-th column of \(Q\) and \(R\) in place by orthogonalising a given vector \(v\) against the first \((k-1)\) columns of \(Q\).</p> <pre><code class=language-julia >using LinearAlgebra, StableRNGs

function mgs&#33;&#40;Q::AbstractMatrix, R::AbstractMatrix, v::Vector, k::Int&#41;
    n &#61; length&#40;v&#41;
    Q&#91;:, k&#93; .&#61; v
    for i &#61; 1:k-1
        R&#91;i, k&#93; &#61; dot&#40;Q&#91;:, k&#93;, Q&#91;:, i&#93;&#41;
        Q&#91;:, k&#93; .-&#61; R&#91;i, k&#93; .* Q&#91;:, i&#93;
    end
    Q&#91;:, k&#93; &#61; normalize&#40;Q&#91;:,k&#93;&#41;
    R&#91;k, k&#93; &#61; dot&#40;v, Q&#91;:, k&#93;&#41;
    return
end

# simple check
begin
    rng &#61; StableRNG&#40;1234&#41;
    n &#61; 5
    A &#61; randn&#40;rng, n, n&#41;
    Q &#61; zeros&#40;n, n&#41;
    R &#61; zeros&#40;n, n&#41;
    for k &#61; 1:n
        mgs&#33;&#40;Q, R, A&#91;:, k&#93;, k&#41;
    end
    e1 &#61; round&#40;maximum&#40;abs.&#40;Q&#39;*Q - I&#41;&#41;, sigdigits&#61;2&#41;
    e2 &#61; round&#40;maximum&#40;abs.&#40;A - Q*R&#41;&#41;, sigdigits&#61;2&#41;
    println&#40;&quot;max entry in |Q&#39;Q-I|: &#36;e1&quot;&#41;
    println&#40;&quot;max entry in |A-QR|: &#36;e2&quot;&#41;
end</code></pre><pre><code class="plaintext code-output">max entry in |Q'Q-I|: 2.8e-15
max entry in |A-QR|: 2.5e-15
</code></pre> <p>The computational complexity &#40;ignoring constants&#41; of this function is:</p> <ul> <li><p>computation of \(r_{ik}\): \((k-1)\) dot products i.e. \(\mathcal O(kn)\),</p> <li><p>computation of the entries of \(q_k\): \(\mathcal O(kn)\),</p> <li><p>normalisation of \(q_k\) and computation of \(r_{kk}\): \(\mathcal O(n)\)</p> </ul> <p>so \(\mathcal O(kn)\) overall.</p> <h3 id=upper_triangular_solve ><a href="#upper_triangular_solve" class=header-anchor >Upper triangular solve</a></h3> <p>The code below is just a simple function implementing <span class=eqref >(<a href="#qrls2">3</a>)</span>:</p> <pre><code class=language-julia >function trisolve&#40;R::AbstractMatrix, v::Vector&#41;
    k &#61; size&#40;R, 1&#41;
    β &#61; zero&#40;v&#41;
    for j &#61; k:-1:1
        β&#91;j&#93; &#61; v&#91;j&#93;
        for i &#61; 1:k-j
            β&#91;j&#93; -&#61; R&#91;j, j&#43;i&#93; * β&#91;j&#43;i&#93;
        end
        β&#91;j&#93; /&#61; R&#91;j, j&#93;
    end
    return β
end

# simple check
begin
    rng &#61; StableRNG&#40;414&#41;
    m &#61; 5
    R &#61; randn&#40;rng, m, m&#41;
    for i&#61;1:m, j&#61;1:i-1
        R&#91;i, j&#93; &#61; 0
    end
    β &#61; randn&#40;rng, m&#41;
    v &#61; R * β
    β̂ &#61; trisolve&#40;R, v&#41;
    e &#61; round&#40;norm&#40;β-β̂&#41;, sigdigits&#61;2&#41;
    println&#40;&quot;‖β-β̂‖: &#36;e&quot;&#41;
end</code></pre><pre><code class="plaintext code-output">‖β-β̂‖: 8.4e-16
</code></pre> <p>The computational complexity &#40;ignoring constants&#41; of this function is:</p> <ul> <li><p>for each index \(j\) compute \((k-j)\) products and subtractions and one division.</p> </ul> <p>so overall \(\mathcal O(k^2)\) complexity where \(k\) is the size of \(v\).</p> <h3 id=core_method ><a href="#core_method" class=header-anchor >Core method</a></h3> <p>We now put all the blocks together to form the <code>itersolve</code> function. In the function you&#39;ll see that we keep track of elapsed time, and the norm of the residuals, this is to facilitate analysis and comparison later on.</p> <pre><code class=language-julia >function itersolve&#40;A::Matrix, b::Vector, x0::Vector;
                   niter::Int&#61;10, dirs&#61;:rand&#41;
    start &#61; time&#40;&#41;
    n &#61; size&#40;A, 1&#41;
    r0 &#61; A * x0 - b

    # objects we&#39;ll use to store the results
    xk &#61; copy&#40;x0&#41;
    rk &#61; copy&#40;r0&#41;
    pk &#61; zeros&#40;n&#41;
    Q &#61; zeros&#40;n, niter&#41;
    Q̃ &#61; zeros&#40;n, niter&#41;
    R &#61; zeros&#40;niter, niter&#41;
    norm_rk &#61; zeros&#40;niter &#43; 1&#41;
    norm_rk&#91;1&#93; &#61; norm&#40;r0&#41;
    times &#61; zeros&#40;niter&#43;1&#41;
    times&#91;1&#93; &#61; time&#40;&#41; - start

    @views for k &#61; 1:niter
        # &lt;O&gt; Obtain a new direction pk
        if dirs &#61;&#61; :rand
            pk &#61; randn&#40;n&#41;
        elseif dirs &#61;&#61; :krylov
            pk &#61; rk
        elseif dirs &#61;&#61; :grad
            pk &#61; A&#39; * rk
        end

        # &lt;A&gt; Orthog. of pk using MGS. We don&#39;t care about R&#91;:, k&#93; here,
        # it will be overwritten at the next step anyway
        mgs&#33;&#40;Q, R, pk, k&#41;

        # &lt;B&gt; Orthog. of A*qk using MGS, we do care about R&#91;:, k&#93; here
        mgs&#33;&#40;Q̃, R, A*Q&#91;:, k&#93;, k&#41;

        # &lt;C&gt; Solving the least-square problem
        γk &#61; -Q̃&#91;:, 1:k&#93;&#39; * r0

        # &lt;D&gt; Solving the triangular problem
        βk &#61; trisolve&#40;R&#91;1:k, 1:k&#93;, γk&#41;

        # &lt;E&gt; Approximation &#43; residuals
        xk .&#61; x0 .&#43; Q&#91;:, 1:k&#93; * βk
        rk .&#61; r0 .&#43; Q̃&#91;:, 1:k&#93; * γk

        norm_rk&#91;k&#43;1&#93; &#61; norm&#40;rk&#41;
        times&#91;k&#43;1&#93; &#61; time&#40;&#41; - start
    end
    return xk, norm_rk, times
end

# quick check &#40;n steps must lead to the correct solution&#41;
begin
    rng &#61; StableRNG&#40;908&#41;
    n &#61; 5
    A &#61; randn&#40;rng, n, n&#41;
    x &#61; randn&#40;rng, n&#41;
    b &#61; A * x

    x0 &#61; randn&#40;rng, n&#41;
    _, nrk_random, _ &#61; itersolve&#40;A, b, x0; niter&#61;n, dirs&#61;:rand&#41;
    norm_rn &#61; round&#40;nrk_random&#91;end&#93;, sigdigits&#61;2&#41;
    println&#40;&quot;‖Axn - b‖: &#36;norm_rn &#40;random&#41;&quot;&#41;

    _, nrk_krylov, _ &#61; itersolve&#40;A, b, x0; niter&#61;n, dirs&#61;:krylov&#41;
    norm_rn &#61; round&#40;nrk_krylov&#91;end&#93;, sigdigits&#61;2&#41;
    println&#40;&quot;‖Axn - b‖: &#36;norm_rn &#40;krylov&#41;&quot;&#41;

    _, nrk_grad, _ &#61; itersolve&#40;A, b, x0; niter&#61;n, dirs&#61;:grad&#41;
    norm_rn &#61; round&#40;nrk_grad&#91;end&#93;, sigdigits&#61;2&#41;
    println&#40;&quot;‖Axn - b‖: &#36;norm_rn &#40;grad&#41;&quot;&#41;
end</code></pre><pre><code class="plaintext code-output">‖Axn - b‖: 1.2e-15 (random)
‖Axn - b‖: 2.4e-15 (krylov)
‖Axn - b‖: 9.7e-16 (grad)
</code></pre> <p>The computational complexity &#40;ignoring constants&#41; at iteration \(k\) of this function is:</p> <ul> <li><p>&#40;O&#41; for <code>grad</code>: one matrix-vector product with \(A^t\) i.e. \(\mathcal O(n^2)\),</p> <li><p>&#40;A, B&#41; two MGS steps i.e. \(\mathcal O(kn)\)</p> <li><p>&#40;B&#41; one matrix-vector product with \(A\) i.e. \(\mathcal O(n^2)\)</p> <li><p>&#40;C&#41; one matrix-vector product with \(\tilde{Q}^{(k)}\) i.e. \(\mathcal O(kn)\)</p> <li><p>&#40;D&#41; one upper-triangular solve of a \(k\times k\) system i.e. \(\mathcal O(k^2)\)</p> <li><p>&#40;E&#41; one application of \(Q^{(k)}\) and one of \(\tilde{Q}^{(k)}\) i.e. \(\mathcal O(nk)\)</p> </ul> <p>The step with dominant complexity is the matrix-vector multiplication &#40;computation of \(Aq_k\)&#41; &#40;and computation of \(A^tr_k\) in the <code>grad</code> case&#41;. Applying a \(n\times n\) matrix has complexity \(\mathcal O(n^2)\) making the overall procedure \(\mathcal O(Kn^2)\) with \(K\) the number of steps.</p> <div class="alert alert-secondary"><strong>Note</strong>: in many cases, there is a specific procedure available to compute &#40;exactly or approximately&#41; \(Ax\) for some \(x\) with complexity better than \(\mathcal O(n^2)\). This can for instance be the case when \(A\) is very sparse, or in some physics problem where the the Fast Multipole Method &#40;FMM&#41; can be used &#40;see e.g. <span class=bibref ><a href="#bg97">Beatson and Greengard (1997)</a></span>&#41;.</div> <h3 id=comparison ><a href="#comparison" class=header-anchor >Comparison</a></h3> <p>In the code below, we consider a random \(n\times n\) linear system that is poorly conditioned and check the norm of the residuals \(\|r_k\|\) as \(k\) increases for the different direction choices. We also use the <code>gmres&#33;</code> function from the excellent <a href="https://github.com/JuliaLinearAlgebra/IterativeSolvers.jl">IterativeSolvers.jl</a> package as a rough sanity check that our code is correct.</p> <div class="alert alert-secondary"><strong>Note</strong>: this is just meant to be a quick benchmark which highlights some interesting points for discussion, it&#39;s not meant to be an in-depth comparison.<br/> Note also that we use random matrices with no or very little structure, so iterative methods need a number of iteration comparable to the dimension to get satisfactory results, for more useful \(A\) the convergence is typically much faster.</div> <pre><code class=language-julia >using IterativeSolvers, PyPlot

rng &#61; StableRNG&#40;908&#41;

n &#61; 50
A &#61; randn&#40;rng, n, n&#41;^4 .&#43; rand&#40;rng, n, n&#41;
x &#61; randn&#40;rng, n&#41;
b &#61; A * x
x0 &#61; randn&#40;rng, n&#41;
r0 &#61; A * x0 - b

κ &#61; round&#40;cond&#40;A&#41;, sigdigits&#61;2&#41;
println&#40;&quot;cond&#40;A&#41;: &#36;κ&quot;&#41;

cases &#61; Dict&#40;&#41;
for dirs in &#40;:rand, :grad, :krylov&#41;
    cases&#91;dirs&#93; &#61; itersolve&#40;A, b, x0; niter&#61;n, dirs&#61;dirs&#41;
end

# Using the IterativeSolvers
x_gmres, log_gmres &#61; gmres&#33;&#40;copy&#40;x0&#41;, A, b; maxiter&#61;n, restart&#61;n&#43;1, log&#61;true&#41;

figure&#40;figsize&#61;&#40;8, 6&#41;&#41;
for dirs in &#40;:rand, :grad, :krylov&#41;
    plot&#40;cases&#91;dirs&#93;&#91;2&#93;, label&#61;dirs&#41;
end
plot&#40;&#91;norm&#40;r0&#41;, log_gmres.data&#91;:resnorm&#93;...&#93;, label&#61;&quot;gmres&quot;, ls&#61;&quot;none&quot;, marker&#61;&quot;x&quot;&#41;

xlabel&#40;&quot;Number of iterations&quot;&#41;
ylabel&#40;&quot;Norm of the residuals&quot;&#41;

legend&#40;&#41;</code></pre><pre><code class="plaintext code-output">cond(A): 1.0e6
</code></pre> <div class=reduce-vspace ><img src="/assets/posts/2021/07/01-gmres/code/output/comp_1.svg" alt=""></div> <p>On this first plot we can see a number of interesting things:</p> <ol> <li><p>the &quot;krylov&quot; directions &#40;where \(p_k = r_k\)&#41; leads to identical iterations as those from <code>gmres&#33;</code> which gives some confidence in the implementation of the <code>itersolve</code> function in terms of correctness,</p> <li><p>the &quot;random&quot; directions have a generally worse performance as could be expected since there&#39;s no information in the direction \(p_k\),</p> <li><p>the &quot;grad&quot; directions &#40;where \(p_k = A^tr_k\)&#41; lead to much faster convergence than the other two,</p> <li><p>all choices eventually lead to \(\|r_n\|\approx 0\) as expected.</p> </ol> <div class="alert alert-info"><strong>Note</strong>: when writing this post and running experiments, I was a bit surprised by how much better the &quot;grad&quot; version behaves compared to the others here, especially since I had not seen that choice discussed in the literature. It may well be an artefact of the benchmark though &#40;see e.g. <a href="/posts/2021/05-cg/">next post</a>&#41;, or a well known fact. Either way if you have thoughts on this, I&#39;ll be glad to <a href="https://github.com/tlienart/tlienart.github.io/issues/new/choose">hear from you</a>.</div> <p>We can repeat a similar experiment with a larger matrix and more iterations and look at the time taken since the start instead of the number of iterations. Before doing so note that:</p> <ol> <li><p>if you try to reproduce these experiments, you should run them a couple of times to make sure the pre-compilation of the function <code>itersolve</code> is factored away,</p> <li><p>if you compare the timings with <code>gmres&#33;</code> you&#39;ll see that <code>itersolve</code> is significantly slower, this is essentially because its implementation is geared for readability rather than speed,</p> <li><p>the main aim of the experiment below is to show that the &quot;grad&quot; choice is still much better here even though each step is twice as expensive as the other two.</p> </ol> <pre><code class=language-julia >rng &#61; StableRNG&#40;9080&#41;

n &#61; 250
# a random matrix with some sparsity
A &#61; 10 * randn&#40;rng, n, n&#41; .* rand&#40;rng, n, n&#41; .* &#40;rand&#40;rng, n, n&#41; .&gt; 0.3&#41;
nz &#61; sum&#40;A .&#61;&#61; 0&#41;
sp &#61; round&#40;nz / &#40;n * n&#41; * 100&#41;
println&#40;&quot;Sparsity ≈ &#36;sp&#37;&quot;&#41;

x &#61; randn&#40;rng, n&#41;
b &#61; A * x
x0 &#61; randn&#40;rng, n&#41;

cases &#61; Dict&#40;&#41;
for dirs in &#40;:rand, :grad, :krylov&#41;
    cases&#91;dirs&#93; &#61; itersolve&#40;A, b, x0; niter&#61;n, dirs&#61;dirs&#41;
end

figure&#40;figsize&#61;&#40;8, 6&#41;&#41;
for dirs in &#40;:rand, :grad, :krylov&#41;
    semilogy&#40;cases&#91;dirs&#93;&#91;3&#93;, cases&#91;dirs&#93;&#91;2&#93;, label&#61;dirs&#41;
end

xlabel&#40;&quot;Time elapsed &#91;s&#93;&quot;&#41;
ylabel&#40;&quot;Norm of the residuals&quot;&#41;
legend&#40;&#41;</code></pre><pre><code class="plaintext code-output">Sparsity ≈ 30.0%
</code></pre> <div class=reduce-vspace ><img src="/assets/posts/2021/07/01-gmres/code/output/comp_2.svg" alt=""></div> <p>Again, this should not be considered to be a thorough benchmark but one can see that even taking into account the extra time needed per step in the grad case, the convergence is significantly faster in the low iteration count for the &quot;grad&quot; choice.</p> <p>As a final note, we look at the time taken per method when the matrix size increases and with a fixed number of iteration:</p> <pre><code class=language-julia >rng &#61; StableRNG&#40;90801&#41;

K &#61; 50
times &#61; Dict&#40;:rand&#61;&gt;&#91;&#93;, :grad&#61;&gt;&#91;&#93;, :krylov&#61;&gt;&#91;&#93;, :gmres&#61;&gt;&#91;&#93;&#41;
N &#61; collect&#40;200:100:3_000&#41;
for n in N
    A &#61; randn&#40;rng, n, n&#41;
    x &#61; randn&#40;rng, n&#41;
    b &#61; A*x
    x0 &#61; randn&#40;rng, n&#41;

    for dirs in &#40;:rand, :grad, :krylov&#41;
        start &#61; time&#40;&#41;
        itersolve&#40;A, b, x0; niter&#61;K, dirs&#61;dirs&#41;
        push&#33;&#40;times&#91;dirs&#93;, time&#40;&#41;-start&#41;
    end
    start &#61; time&#40;&#41;
    gmres&#33;&#40;copy&#40;x0&#41;, A, b, maxiter&#61;K, restart&#61;K&#43;1&#41;
    push&#33;&#40;times&#91;:gmres&#93;, time&#40;&#41;-start&#41;
end

figure&#40;figsize&#61;&#40;8, 6&#41;&#41;
for dirs in &#40;:rand, :grad, :krylov&#41;
    plot&#40;N, times&#91;dirs&#93;, label&#61;dirs&#41;
end
plot&#40;N, times&#91;:gmres&#93;, label&#61;&quot;gmres&quot;&#41;
legend&#40;&#41;

xlabel&#40;&quot;Matrix size&quot;&#41;
ylabel&#40;&quot;Time taken &#91;s&#93;&quot;&#41;</code></pre> <div class=reduce-vspace ><img src="/assets/posts/2021/07/01-gmres/code/output/comp_3.svg" alt=""></div> <p>As could be expected, the <code>gmres&#33;</code> function is significantly faster &#40;our code is really not optimised&#41; but all methods exhibit the same behaviour, scaling like \(n^2\) as expected. Note that only the trend should be looked at, the peaks and trophs should be ignored and are mostly due to how un-optimised our implementation is and how well Julia manages to optimise the steps at various sizes.</p> <h2 id=short_references ><a href="#short_references" class=header-anchor >Short references</a></h2> <ol> <li><p><a id=vd05  class=anchor ></a><strong>Van Dooren</strong>, <a href="https://perso.uclouvain.be/paul.vandooren/Krylov.pdf">Krylov methods: an introduction</a>, 2005. – A set of slides on Krylov methods and GMRES.</p> <li><p><a id=t19  class=anchor ></a><strong>Townsend</strong>, <a href="http://pi.math.cornell.edu/~web6140/TopTenAlgorithms/KrylovSubspace.html">Krylov subspace methods</a>, 2019. – A tutorial on Krylov subspace methods using Julia.</p> <li><p><a id=bg97  class=anchor ></a><strong>Beatson</strong>, <strong>Greengard</strong>, <a href="https://math.nyu.edu/~greengar/shortcourse_fmm.pdf">A short course on fast multipole methods</a>, 1997.</p> <li><p><a id=saad03  class=anchor ></a><strong>Saad</strong>, <a href="https://www-users.cs.umn.edu/~saad/IterMethBook_2ndEd.pdf">Iterative Methods for Sparse Linear Systems</a>, 2003. – A reference book on iterative methods by one of the author of GMRES.</p> <li><p><a id=vv03  class=anchor ></a><strong>van der Vorst</strong>, <a href="http://www.lmn.pub.ro/~daniel/ElectromagneticModelingDoctoral/Books/Numerical&#37;20Methods/VanDerVorst2003&#37;20Iterative&#37;20Krylov&#37;20Methods&#37;20for&#37;20Large&#37;20Linear&#37;20Systems.pdf">Iterative Krylov Methods for Large Linear Systems</a>, 2003. – A reference book on iterative methods for linear systems by the author of the Bi-CGSTAB paper.</p> </ol> </div> </main> <div class='py-3'></div> <footer class="footer mt-auto py-3 bg-light"> <div class=container > <span class=text-muted > &copy; Thibaut Lienart, 2021. Website made with <a href="https://franklinjl.org">Franklin.jl</a>. </span> </div> </footer> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script> <script src="/libs/highlight/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script> <script> (function(){ // Get the elements. // - the 'pre' element. // - the 'div' with the 'paste-content' id. var pre = document.getElementsByTagName('pre'); // Add a copy button in the 'pre' element. // which only has the className of 'language-'. for (var i = 0; i < pre.length; i++) { var cName = pre[i].children[0].className if ( cName.includes('language-') || cName.includes('hljs') && !cName.includes('plaintext') ) { var button = document.createElement('button'); button.className = 'copy-button'; button.textContent = 'Copy'; pre[i].appendChild(button); } }; // Run Clipboard var copyCode = new Clipboard('.copy-button', { target: function(trigger) { return trigger.previousElementSibling; } }); // On success: // - Change the "Copy" text to "Copied". // - Swap it to "Copy" in 2s. // - Lead user to the "contenteditable" area with Velocity scroll. copyCode.on('success', function(event) { event.clearSelection(); event.trigger.textContent = 'Copied'; window.setTimeout(function() { event.trigger.textContent = 'Copy'; }, 2000); }); // On error (Safari): // - Change the "Press Ctrl+C to copy" // - Swap it to "Copy" in 2s. copyCode.on('error', function(event) { event.trigger.textContent = 'Press "Ctrl + C" to copy'; window.setTimeout(function() { event.trigger.textContent = 'Copy'; }, 5000); }); })(); </script> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script> <script src="/libs/highlight/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script>