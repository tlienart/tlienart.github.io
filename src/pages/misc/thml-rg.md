@def hasmath = false
@def hascode = false
@def title = "ThML learning reading group"

# Theoretical ML reading group

The aim of the reading group is to discuss key results from interesting recent theory papers from venues such as JMLR, NIPS, ICML or similar without spending too much time on proofs unless warranted by the discussion.
Applied papers and deep learning papers would generally be avoided.

## Spirit

The format is a ~30-40 min presentation _leaving ample room for discussion_; white-board presentations are preferred.
The purpose of these reading groups is heavily focused on the discussion part, in particular the strengths and weaknesses of the paper, links with existing literature and possible applications or further work.

In order for this reading group to be a success, all participants are expected to present at some point (especially PhDs and postdocs).
A bit of friendly chasing will hopefully ensure this happens üöÇ.

## Logistics

* üìÜ **schedule**: the reading groups take place on Thursdays from 4pm to 5pm,
* üé™ **room**: McDonnell, usually 7.02 unless otherwise mentioned,
* üìß **mailing list**: you can subscribe by [clicking this link](https://lists.unimelb.edu.au/subscribe/ml-theory).

We also have a small catering budget for biscuits and tea üç´.

## Presentations

### Upcoming

* (**Nov 15, '18**) [Explaining the success of adaboost and random forests as interpolating classifiers](http://jmlr.org/papers/volume18/15-240/15-240.pdf) by Wyner, Olson, Bleich and Mease, JMLR 2017. _Presenter: Neil Merchant_.
* (**Nov 22, '18**) Paper TBA. _Presenter: Dongge Liu_.
* (**Nov 29, '18**) [To tune or not to tune the number of trees in random forest](http://jmlr.org/papers/volume18/17-269/17-269.pdf) by Probst and Boulesteix, JMLR 2018. _Presenter: Bastian Oetomo_.

### Past

* (**Nov 1, '18**) [Can we trust the bootstrap in high dimension](http://jmlr.org/papers/volume19/17-006/17-006.pdf) by El Karoui and Purdom, JMLR 2017. _Presenter: Thibaut Lienart_.

### Suggested papers to review

To add papers to this list, please send me an email or let me know, it is just a draft at this point.
The papers are (a bit arbitrarily) sorted in two blocks with the first one judged to have priority as potentially more likely to be of interest to everyone.
At some point in the future, I'll organise the list with indicators of the topics.

#### priority block

* [Gradient descent learns linear dynamical systems](http://jmlr.org/papers/volume19/16-465/16-465.pdf) by Hardt, Ma and Recht, JMLR 2018.
* [Classification with imperfect training labels](https://arxiv.org/abs/1805.11505) by Cannings, Fan and Samworth, ArXiv 2018.
* [Second-order stochastic optimisation for machine learning in linear time](http://jmlr.org/papers/volume18/16-491/16-491.pdf) by Agarwal, Bullins and Hazan, JMLR 2017.
* [Minimising the maximal loss: how and why](http://proceedings.mlr.press/v48/shalev-shwartzb16-supp.pdf) by Shalev-Shwartz and Wexler, ICML 2016.

#### secondary block

* [On Markov chain Monte Carlo methods for tall data](http://jmlr.org/papers/volume18/15-205/15-205.pdf) by Bardenet, Doucet and Holmes, JMLR 2017.
* [An optimal algorithm for bandit and zero-order convex optimisation with two-point feedback](http://jmlr.org/papers/volume18/16-632/16-632.pdf) by Shamir, JMLR 2017.
* [An embarrassingly simple approach to zero-shot learning](http://proceedings.mlr.press/v37/romera-paredes15.pdf) by Romera-Paredes and Torr, ICML 2015.
* [On the global linear convergence of Frank-Wolfe optimization variants](http://papers.nips.cc/paper/5925-on-the-global-linear-convergence-of-frank-wolfe-optimization-variants.pdf) by Lacoste-Julien and Jaggi, NIPS 2015.
* [Fast and provably good seedings for k-means](https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf) by Bachem, Lucic, Hassani and Krause, NIPS 2016.
* [Revisiting the Nystrom method for improved large-scale machine learning](http://www.jmlr.org/papers/volume17/gittens16a/gittens16a.pdf), by Gittens and Mahoney, JMLR 2016.
* [Hamiltonian descent methods](http://jmlr.org/papers/volume18/15-205/15-205.pdf) by Maddison, Paulin, Teh, Donoghue and Doucet, ArXiv 2018.
* [Robust and scalable bayes via a median of subset posterior measures](http://jmlr.org/papers/volume18/16-491/16-491.pdf) by Minsker, Srivastava, Lin and Dunson, JMLR 2017.
* [Fast algorithms for robust PCA via gradient descent](http://papers.nips.cc/paper/6445-fast-algorithms-for-robust-pca-via-gradient-descent.pdf) by Yi, Park, Chen and Caramanis, NIPS 2016.
* [Adaptive randomized dimension reduction on massive data](http://jmlr.org/papers/volume18/15-143/15-143.pdf) by Darnell, Georgiev, Mukherjee and Engelhardt, JMLR 2017.
