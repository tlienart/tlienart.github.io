@def hasmath = false
@def hascode = false
@def title = "ThML learning reading group"

# Theoretical ML reading group

The aim of the reading group is to discuss key results from interesting recent theory papers from venues such as JMLR, NIPS, ICML or similar without spending too much time on proofs unless warranted by the discussion.
Applied papers and deep learning papers would generally be avoided.

## Spirit

The format is a ~30-40 min presentation _leaving ample room for discussion_; white-board presentations are preferred.
The purpose of these reading groups is heavily focused on the discussion part, in particular the strengths and weaknesses of the paper, links with existing literature and possible applications or further work.

In order for this reading group to be a success, all participants are expected to present at some point (especially PhDs and postdocs).
A bit of friendly chasing will hopefully ensure this happens ðŸš‚.

## Logistics

* ðŸ“† **schedule**: the reading groups take place on Thursdays from 4pm to 5pm,
* ðŸŽª **room**: McDonnell, usually 7.02 unless otherwise mentioned,
* ðŸ“§ **mailing list**: you can subscribe by [clicking this link](https://lists.unimelb.edu.au/subscribe/ml-theory).

## Presentations

### Upcoming

* TBD

### Past

* (**Apr 18, '19**) [Neural Ordinary Differential Equations](https://arxiv.org/pdf/1806.07366.pdf) by Chen, Rubanova, Bettencourt and Duvenaud. _Presenter: Thibaut Lienart_.
* (**Mar 7, '19**) [Adaptive Data Analysis](https://protect-au.mimecast.com/s/t9StCE8knvs5D9qVuNz-y6?domain=simons.berkeley.edu  ) by Roth. _Presenter: Ben Rubinstein_.
* (**Feb 21, '19**) [Conservative contextual linear bandits](https://papers.nips.cc/paper/6980-conservative-contextual-linear-bandits) by Kazerouni et al. _Presenter: Masoud Khorasani_.
* (**Feb 14, '19**) [Is Q-Learning provably efficient](https://papers.nips.cc/paper/7735-is-q-learning-provably-efficient.pdf) by Jin et al. _Presenter: Neil Merchant_.
* (**Feb 7, '19**) [Minimising the maximal loss: how and why](http://proceedings.mlr.press/v48/shalev-shwartzb16-supp.pdf) by Shalev-Shwartz and Wexler. _Presenter: Yi Han_.
* (**Jan 31, '19**) [Sampling can be faster than optimization](https://arxiv.org/abs/1811.08413) by Ma, Chen, Jin, Flammarion and Jordan. _Presenter: Miquel RamÃ­rez_.
* (**Jan 24, '19**) [Second-order stochastic optimisation for machine learning in linear time](http://jmlr.org/papers/volume18/16-491/16-491.pdf) by Agarwal, Bullins and Hazan, JMLR 2017. _Presenter: Bastian Oetomo_.
* (**Dec 20, '18**) [An Outsider's Tour of RL](http://www.argmin.net/2018/06/25/outsider-rl/) by Recht. _Presenter: Thibaut Lienart_.
* (**Dec 13, '18**) [Classification with imperfect training labels](https://arxiv.org/abs/1805.11505) by Cannings, Fan and Samworth, arXiv '18. _Presenter: Yi Han_.
* (**Nov 29, '18**) [To tune or not to tune the number of trees in random forest](http://jmlr.org/papers/volume18/17-269/17-269.pdf) by Probst and Boulesteix, JMLR 2018. _Presenter: Bastian Oetomo_.
* (**Nov 22, '18**) [An optimal algorithm for bandit and zero-order convex optimisation with two-point feedback](http://jmlr.org/papers/volume18/16-632/16-632.pdf) by Shamir, JMLR 2017. _Presenter: Dongge Liu_.
* (**Nov 15, '18**) [Explaining the success of adaboost and random forests as interpolating classifiers](http://jmlr.org/papers/volume18/15-240/15-240.pdf) by Wyner, Olson, Bleich and Mease, JMLR 2017. _Presenter: Neil Merchant_.
* (**Nov 1, '18**) [Can we trust the bootstrap in high dimension](http://jmlr.org/papers/volume19/17-006/17-006.pdf) by El Karoui and Purdom, JMLR 2017. _Presenter: Thibaut Lienart_.

### Suggested papers to review

To add papers to this list, please send me an email or let me know, it is just a draft at this point.
The papers are (a bit arbitrarily) sorted in two blocks with the first one judged to have priority as potentially more likely to be of interest to everyone.
At some point in the future, I'll organise the list with indicators of the topics.

#### priority block

* [Why are big data matrices approximately low rank](https://epubs.siam.org/doi/pdf/10.1137/18M1183480) by Udell and Townsend, SIAM 2019.

#### secondary block

* [On Markov chain Monte Carlo methods for tall data](http://jmlr.org/papers/volume18/15-205/15-205.pdf) by Bardenet, Doucet and Holmes, JMLR 2017.
* [An embarrassingly simple approach to zero-shot learning](http://proceedings.mlr.press/v37/romera-paredes15.pdf) by Romera-Paredes and Torr, ICML 2015.
* [On the global linear convergence of Frank-Wolfe optimization variants](http://papers.nips.cc/paper/5925-on-the-global-linear-convergence-of-frank-wolfe-optimization-variants.pdf) by Lacoste-Julien and Jaggi, NIPS 2015.
* [Fast and provably good seedings for k-means](https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf) by Bachem, Lucic, Hassani and Krause, NIPS 2016.
* [Revisiting the Nystrom method for improved large-scale machine learning](http://www.jmlr.org/papers/volume17/gittens16a/gittens16a.pdf), by Gittens and Mahoney, JMLR 2016.
* [Hamiltonian descent methods](http://jmlr.org/papers/volume18/15-205/15-205.pdf) by Maddison, Paulin, Teh, Donoghue and Doucet, ArXiv 2018.
* [Robust and scalable bayes via a median of subset posterior measures](http://jmlr.org/papers/volume18/16-491/16-491.pdf) by Minsker, Srivastava, Lin and Dunson, JMLR 2017.
* [Fast algorithms for robust PCA via gradient descent](http://papers.nips.cc/paper/6445-fast-algorithms-for-robust-pca-via-gradient-descent.pdf) by Yi, Park, Chen and Caramanis, NIPS 2016.
* [Adaptive randomized dimension reduction on massive data](http://jmlr.org/papers/volume18/15-143/15-143.pdf) by Darnell, Georgiev, Mukherjee and Engelhardt, JMLR 2017.
