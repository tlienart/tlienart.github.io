<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>RKHS Embeddings</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=name >Thibaut Lienart</div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h2>Introduction</h2> <p>These notes were prepared with help from <a href="http://www.stats.ox.ac.uk/~sejdinov/">Dino Sejdinovic</a> for a talk to a <em>kernel methods reading group</em> given in Oxford in July 2015. The notes are fairly dry, I intend to come back and add more comments on the intuition in the future &#40;in particular for the second part&#41;.</p> <h3>Basic definitions</h3> <div class=colbox-blue >A Hilbert space \(\mathcal H\) of functions \(f:\mathcal X\mapsto \mathbb R\) defined on a non-empty set \(\mathcal X\) is said to be a <em>reproducing kernel Hilbert space</em> &#40;RKHS&#41; if <em>evaluation functionals</em> \(\delta_x: f\mapsto f(x)\) are continuous for all \(x\in\mathcal X\)</div> <p>If we consider a RKHS then, by Riesz&#39;s representation theorem, since \(\delta_x\) is a continuous functional, it has a <em>representer</em> in \(\mathcal H\) that we can denote \(k_{x}\) such that</p> $$\begin{array}{c} \left\langle f, k_{x}\right\rangle_{\mathcal H} &=& \delta_x(f) \quad\!\! =\quad\!\! f(x). \end{array}$$ <p>We can then define a &#40;positive-definite&#41; bilinear form \(k:\mathcal X\times\mathcal X \to \mathbb R\) as \(k(x, x'):=\left\langle k_{x}, k_{x'}\right\rangle_{\mathcal H}\). This is known as the <strong>reproducing kernel</strong> of \(\mathcal H\); we will also write \(k_{x} = k(\cdot, x)\).</p> <p>There&#39;s then an important theorem that links the two &#40;and that we won&#39;t prove&#41; reproduced in a simplified form below:</p> <div class=colbox-blue >&#40;<strong>Moore-Aronszajn</strong>&#41; Every positive-definite bilinear form \(k\) is a reproducing kernel for some Hilbert space \(\mathcal H_k\).</div> <p>When the kernel is clear from the context, we will simply write \(\mathcal H\) for the RKHS and \(k\) for its reproducing kernel.</p> <h2>Kernel embedding of a distribution</h2> <p>A classical way to try to represent points in a given space \(\mathcal X\) is to <em>embed</em> them in \(\mathbb R^s\) using a \(s\)-dimensional <em>feature map</em> \(\Phi:\mathcal X\to \mathbb R^s\) with $$\begin{array}{c} x\quad\!\!\mapsto\quad\!\!\Phi(x)\,\,=\,\, (\varphi_1(x), \varphi_2(x), \dots, \varphi_s(x)). \end{array}$$ Instead, we can now consider embedding points in a RKHS with the infinite dimensional feature map \(x\mapsto k_{x}\). In that case, we have an easily computable inner product between points with</p> $$\begin{array}{c} \left\langle k_{x}, k_{y}\right\rangle_{\mathcal H} &=& \left\langle k(\cdot, x), k(\cdot, y)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! k(x, y). \end{array}$$ <p>Recall that an inner product is a <em>measure of alignment</em> so that this automatically gives us a measure of similarity between points through this kernel.</p> <p>When the embedding is <em>injective</em> &#40;i.e.: different objects are mapped to different points in the RKHS&#41;, the corresponding kernel is said to be <em>characteristic</em> &#40;this is often the case for standard kernels&#41;.</p> <h3>Mean embedding</h3> <p>An example of objects we can embed in an RKHS are distributions. Each distribution is then considered as a point which we can embed through the <em>mean-embedding</em>.</p> <div class=colbox-blue >Let \(P\) denote a distribution among a set \(\mathcal P(\Omega)\) of distributions over some \(\Omega\), the <strong>mean embedding</strong> is defined as follows:</p> $$\begin{array}{c} P \quad\!\! \mapsto\quad\!\! \mu(P; k) \quad\!\! :=\quad\!\! \mathbb E_{X\sim P}[k(\cdot, X)] \quad\!\! =\quad\!\! \mathbb E_{X\sim P}[k_{_X}],\end{array}$$ and, naturally, \(\mu(P, k)\in\mathcal H\). When the kernel and distribution are clear from the context, we will simply write \(\mu(P; k)=:\mu_{_X}\) where, implicitly \(X\sim P\).</div> <p>Observe that we now have this continuous embedding instead of a finite-dimensional embedding that we could have considered such as \(P\mapsto (\mathbb E[\varphi_1(X)], \dots, \mathbb E[\varphi_s(X)])\). Also, as before, we inherit a notion of <em>similarity</em> between points &#40;here probability distributions&#41; by considering the inner product on the RKHS: $$\begin{array}{c} \left\langle \mu(P; k), \mu(Q; k)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \mathbb E_{X Y}[k(X, Y)], \end{array}$$ and this can easily be estimated if we have samples from both \(P\) and \(Q\).</p> <p>Note finally that \(\mu_{_X}\) represents *expectations with respect to \(P\)* i.e.: for any \(f\in\mathcal H\), $$\begin{array}{c} \left\langle f, \mu_{_X}\right\rangle_{\mathcal H} &=& \mathbb E_X[{\scal{f, k_{_X}}_{\mathcal H}}] \quad\!\! =\quad\!\! \mathbb E_X[f(X)] . \end{array}$$ <h3>Joint embedding</h3> <p>The generalisation to joint distributions is straightforward using tensor product feature spaces.</p> <div class=colbox-blue >Let \(X, Y\) be jointly distributed according to some distribution \(P\), the <strong>joint embedding</strong> of \(P\) is defined as $$\begin{array}{c} P \quad\!\! \mapsto\quad\!\! \mathcal C_{XY}(P) \quad\!\! :=\quad\!\! \mathbb E_{XY}[k_{_X}\otimes k_{_Y}], \end{array}$$ assuming that the two variables share the same kernel.</div> <p>The tensor product satisfies \(\left\langle k_{_x} \otimes k_{_y}, k_{x'} \otimes k_{y'}\right\rangle_{\mathcal H\otimes \mathcal H} = k(x, x')k(y, y')\).</p> <p>In the same way that \(\mu_{_X}\) represents the expectation operator, the joint-embedding \(\mathcal C_{XY}\) can be viewed as the <em>uncentered cross-covariance operator</em>: for any two functions \(f, g \in \mathcal H\), their uncentered covariance is given by</p> <a name=2626712616588176884 ></a>$$\begin{array}{c} \mathbb E_{XY}[f(X)g(Y)] &=& \left\langle f\otimes g, \mathcal C_{XY}\right\rangle_{\mathcal H\otimes \mathcal H} \quad\!\! =\quad\!\! \left\langle f, \mathcal C_{XY} g\right\rangle_{\mathcal H} \end{array}$$ <p>&#40;still assuming both random variables share the same kernel&#41;. Following the same reasoning, we can define the <em>auto-covariance operators</em> \(\mathcal C_{XX}\) and \(\mathcal C_{YY}\). Note also that, where \(\mu_{_X}\) represents expectations with respect to \(P\) &#40;the distribution of \(X\)&#41;, these operators represent cross-covariance/auto-covariance with respect to \(P\).</p> <p><strong>Remark</strong>: we have assumed that both random variables share the same kernel but this need not be the case: we can consider a second kernel \(k'\) with a RKHS \(\mathcal H'\); the cross-covariance operator then belongs to the product space \(\mathcal H\otimes \mathcal H'\) &#40;which is also a RKHS&#41;.</p> <h3>MMD and HSIC</h3> <p>We have now seen two ways of embedding a distribution into a RKHS, now let&#39;s see why that can be useful. When considering a characteristic kernel &#40;such as, for example, the Gaussian RBF with \(k(x, x')=\exp(-\sigma\|x-x'\|^2_2)\)&#41;, the RKHS embedding is <em>injective</em>. In that case, we can use the distance in the RKHS &#40;induced by the inner-product&#41; as a proxy for similarity in the distribution space. This can be used in the two-sample test &#40;to see whether two random variables are distributed according to the same distribution&#41; or when testing for independence between random variables.</p> <p>In the <em>kernel two sample test</em> <span class=bibref >(<a href="#1505112226573082996">Gretton et al. (2012a)</a>)</span>, the test statistic is the squared distance between the embeddings of the two distributions:</p> <div class=colbox-blue >The kernel <strong>Maximum Mean Discrepancy</strong> &#40;<strong>MMD</strong>&#41; measure is defined for two distributions \(P\) and \(Q\) by $$\begin{array}{c} \mathrm{MMD}(P, Q) &:=& \|\mu_{_X} - \mu_{_Y}\|_{\mathcal H}^2, \end{array}$$ where \(X\sim P\) and \(Y\sim Q\). </div> where &#36;X\sim P&#36; and &#36;Y\sim Q&#36;. @@</p> <p>It is then possible to study the asymptotic properties of both the MMD and the MMD² and build hypothesis tests at a given level, independently of the distributions considered &#40;see <span class=bibref ><a href="#1505112226573082996">Gretton et al. (2012a)</a></span> corollary 9 and 11&#41;.</p> <p>When testing for <em>independence</em>, the test statistic can be the squared distance between the embeddings of the joint distribution and the product of its marginals which leads to the <em>kernel independence test</em> <span class=bibref >(<a href="#12975797260028795485">Gretton et al. (2007)</a>)</span>:</p> <div class=colbox-blue >The <strong>Hilbert-Schmidt Independence Criterion</strong> &#40;<strong>HSIC</strong>&#41; is defined for two distributions \(P\) and \(Q\) by $$\begin{array}{c} \mathrm{HSIC}(P, Q) &:=& \|\mathcal C_{XY} - \mu_{_X} \otimes \mu_{_Y}\|_{\mathcal H}^2, \end{array}$$ where \(X\sim P\) and \(Y\sim Q\).</div> where &#36;X\sim P&#36; and &#36;Y\sim Q&#36;. @@</p> <p>Again, it is then possible to study the asymptotic properties of the HSIC and build a hypothesis test at a given level, independently of the distributions considered &#40;cf. <span class=bibref >(<a href="#12975797260028795485">Gretton et al. (2007)</a>)</span>&#41;.</p> <h3>Finite sample embeddings</h3> <p>All of the embeddings defined above can readily be estimated using samples drawn from the distributions of interest. Let \(\{x_1, \dots, x_n\}\) be an iid. draw from \(P\), the <em>empirical kernel embedding</em> is defined as</p> $$ \widehat{\mu}_{_X} \quad\!\! :=\quad\!\! n^{-1}\sum_{i=1}^n k_{x_i}. $$ <p>As for standard Monte Carlo estimators, the rate of convergence is \(\mathcal O(1/\sqrt{n})\) &#40;an hence does not depend upon the dimensionality of the underlying space&#41;. Similarly, for an iid draw of pairs \(\{(x_1,y_1), \dots, (x_n, y_n)\}\), the <em>empirical covariance operator</em> is defined as</p> <a name=18058751326955117006 ></a>$$\begin{aligned} \widehat{\mathcal C}_{XY} \quad\!\!&=\quad\!\! n^{-1}\sum_{i=1}^n k_{x_i} \otimes k_{y_i} \\ &=\quad\!\! n^{-1}\Upsilon\Phi^t \end{aligned}$$ where \(\Upsilon := (k_{x_1}, \dots, k_{x_n})\) and \(\Phi:=(k_{y_1}, \dots, k_{y_n})\) are the <em>feature matrices</em>.</p> <p>To conclude, it is straightforward to obtain empirical estimators for the MMD and HSIC criterion considering kernel elements \(k(x_i, x_j)\), \(k(y_i, y_j)\) and \(k(x_i, y_j)\). In the case of the MMD for instance, one has:</p> $$ \widehat{\mathrm{MMD}}(P, Q) \quad\!\! =\quad\!\! {1\over n^{2}}\sum_{i,j=1}^n \left(k(x_{i},x_{j})+k(y_{i},y_{j})-2k(x_{i},y_{j})\right) $$ <h2>Kernel embeddings of conditional distributions</h2> <h3>Pointwise definition</h3> <p>In line with the definitions presented earlier, the kernel embedding of a conditional distribution \(P(Y|X)\) is defined as</p> $$\begin{array}{c} \mu_{_{Y|x}} &=& \mathbb E_{Y|x}[k_{_Y}], \end{array}$$ <p>and the conditional expectation of a function \(g\in \mathcal H\) can be expressed as</p> $$\begin{array}{c} \mathbb E_{Y|x}[g(Y)] &=& \left\langle g, \mu_{_{Y|x}}\right\rangle_{\mathcal H}. \end{array}$$ <p>Note that we now have a family of points in the RKHS indexed by \(x\), the value upon which we condition.</p> <h3>Conditional operator</h3> <p>We can also define an operator \(\mathcal C_{Y|X}\) such that</p> $$\begin{array}{c} \mu_{_{Y|x}} &=& \mathcal C_{Y|X} k_x.\end{array}$$ <p>To do so, we must first introduce a result for which we will provide a partial proof &#40;the full proof can be found in <span class=bibref ><a href="#10410378134158183994">Fukumizu et al. (2004)</a></span>&#41;.</p> <div class=colbox-blue >The following identity holds &#40;under mild technical conditions&#41;:</p> <a name=12095315107479576287 ></a>$$\begin{array}{c} \mathcal C_{XX} \mathbb E_{Y|X}[g(Y)] &=& \mathcal C_{XY}g. \end{array}$$</div> <p>To prove this, note that for \(f \in \mathcal H\), using the definition of the joint embedding, we have</p> $$\begin{aligned} \left\langle f, \mathcal C_{XX} \mathbb E_{Y|X}[g(Y)]\right\rangle_{\mathcal H} \quad\!\! &=\quad\!\! \mathbb E_X[f(X)\mathbb E_{Y|X}[g(Y)]] \\ &=\quad\!\! \mathbb E_X[\mathbb E_{Y|X}[f(X)g(Y)]] \quad\!\! =\quad\!\! \mathbb E_{XY}[f(X)g(Y)], \end{aligned}$$ where at the last equality, we used the tower property of expectations. Comparing with <span class="eqref)">(<a href="#2626712616588176884">8</a>)</span> we get <span class="eqref)">(<a href="#12095315107479576287">17</a>)</span>.</p> <p>With this, we can show another nice identity. Using the definition of \(\mu_{Y|x}\), we have $$\begin{array}{c} \left\langle g, \mu_{Y|x}\right\rangle_{\mathcal H} &=& \mathbb E_{Y|x}[g(Y)] \quad\!\! =\quad\!\! \left\langle \mathbb E_{Y|X}[g(Y)], k_x\right\rangle_{\mathcal H}. \end{array}$$ But, using <span class="eqref)">(<a href="#12095315107479576287">17</a>)</span>, we have $$\begin{array}{c} \left\langle \mathbb E_{Y|X}[g(Y)], k_x\right\rangle_{\mathcal H} &=& \left\langle \mathcal C_{XX}^{-1}\mathcal C_{XY} g, k_x\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \left\langle g, \mathcal C_{YX}C_{XX}^{-1} k_x\right\rangle_{\mathcal H} \end{array}$$ where at the last equality we took the adjoint of \(\mathcal C_{XX}^{-1}\mathcal C_{XY}\) which allows us to introduce the definition that follows.</p> <div class=colbox-blue >The <strong>conditional embedding operator</strong> is defined as $$ \mathcal C_{Y|X} \quad\!\! :=\quad\!\! \mathcal C_{YX}\mathcal C_{XX}^{-1}. $$</div> <p>In practice, \(\mathcal C_{XX}\) is a <em>compact operator</em> which means that its eigenvalues go to zero and hence its inverse is not a bounded operator. So the definition of \(\mathcal C_{Y|X}\) given above is a slight abuse of notation. The inversion of \(\mathcal C_{XX}\) can be replaced by the <em>regularised inverse</em> \((\mathcal C_{XX}+\lambda I)^{-1}\) where \(\lambda\) is a positive factor that can be determined by cross-validation.</p> <h3>Finite sample kernel estimator</h3> <p>If we consider an iid draw \(\{(x_i, y_i)\}_{i=1}^m\) from a joint distribution \(P\), we know that the empirical estimators for \(\mathcal C_{YX}\) and \(\mathcal C_{XX}\) can be written as</p> $$ \widehat\mathcal C_{YX} \quad\!\! =\quad\!\! {1\over n}\Phi\Upsilon^t\quad\!\!\text{and}\quad\!\! \widehat\mathcal C_{XX} \quad\!\! =\quad\!\! = {1\over n}\Upsilon\Upsilon^t, $$ <p>where \(\Phi\) and \(\Upsilon\) are defined as before &#40;see equation <span class="eqref)">(<a href="#18058751326955117006">12</a>)</span>&#41;. Using a trick from linear algebra for the regularised inverse &#40;coming from an application of Woodbury&#39;s formula&#41;, we have:</p> $$ \widehat\mathcal C_{Y|X} \quad\!\! =\quad\!\! n^{-1} \Phi \left[\Upsilon^t\left(\lambda I + n^{-1}\Upsilon\Upsilon^t\right)\right] \quad\!\! =\quad\!\! n^{-1}\Phi\left[n(\lambda I + \Upsilon^t\Upsilon)\Upsilon^t\right],$$ <p>which gives an estimator for the conditional embedding operator.</p> <div class=colbox-blue >A finite sample estimator for the conditional embedding operator is $$ \widehat\mathcal C_{Y|X} \quad\!\! =\quad\!\! \Phi[K+\lambda I]^{-1}\Upsilon^t $$ where \(K:=\Upsilon^t\Upsilon\) is the Gram matrix.</div> <p>The regularisation parameter \(\lambda\) helps to control for overfitting. The resulting kernel embedding is then</p> $$ \widehat\mu_{Y|x} \quad\!\! =\quad\!\! \Phi\beta(x) $$ <p>where \(\beta(x):= (K+\lambda I)^{-1}K_{:x}\) and \(K_{:x}:=[k(x, x_i)]_{i=1}^m\) . It is thus a weighted sum of samples of \(Y\) in the feature space with weights depending on the conditioning variable \(x\).</p> <h2>References</h2> <ul> <li><p><a name=10410378134158183994 ></a> <strong>Fukumizu</strong>, <strong>Bach</strong> and <strong>Jordan</strong>, <a href="http://www.jmlr.org/papers/volume5/fukumizu04a/fukumizu04a.ps">Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</a>, 2004. A key paper in the RKHS literature.</p> <li><p><a name=12975797260028795485 ></a> <strong>Gretton</strong>, <strong>Fukumizu</strong>, <strong>Hui Teo</strong>, <strong>Le Song</strong>, <strong>Schölkopf</strong>, <strong>Smola</strong>, <a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/NIPS2007-Gretton_&#37;5b0&#37;5d.pdf">A kernel statistical test of independence</a>, 2007. Paper describing how to perform an independence test using the HSIC.</p> <li><p><a name=1505112226573082996 ></a> <strong>Gretton</strong>, <strong>Borgwardt</strong>, <strong>Rasch</strong>, <strong>Schölkopf</strong>, <strong>Smola</strong>, <a href="http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf">A kernel two-sample test</a>, 2012. Paper describing how to perform a two-sample test using the MMD.</p> <li><p><a name=11688084164518444760 ></a> <strong>Gretton</strong>, <strong>Sriperumbudur</strong>, <strong>Sejdinovic</strong>, <strong>Strathmann</strong>, <strong>Balakrishnan</strong>, <strong>Pontil</strong> and <strong>Fukumizu</strong>, <a href="http://www.gatsby.ucl.ac.uk/~gretton/papers/GreSriSejStrBalPonFuk12.pdf">Optimal kernel choice for large-scale two-sample tests</a>, 2012. Another paper describing how to perform a two-sample test using the MMD focusing on computational perspectives.</p> <li><p><a name=3244370221621754717 ></a> <strong>Song</strong>, <strong>Fukumizu</strong> and <strong>Gretton</strong>, <a href="https://www.cc.gatech.edu/~lsong/papers/SonFukGre13.pdf">Kernel embeddings of conditional distributions</a>, 2013. The paper these notes are mainly based on.</p> </ul> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 19, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>