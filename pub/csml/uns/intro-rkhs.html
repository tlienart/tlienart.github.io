<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=stylesheet  href="/css/header.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>RKHS Embeddings</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=name >Thibaut Lienart</div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <p></p> <h2>Introduction</h2> <p>These notes were prepared with help from <a href="http://www.stats.ox.ac.uk/~sejdinov/">Dino Sejdinovic</a> for a talk to a <em>kernel methods reading group</em> given in Oxford in July 2015, they&#39;re mostly based on a 2013 paper by Song, Fukumizu and Gretton: <em>Kernel embeddings of conditional distributions</em></p> <h3>Basic definitions</h3> <p><div class=colbox-blue > A Hilbert space \(\mathcal H\) of functions \(f:\mathcal X\mapsto \mathbb R\) defined on a non-empty set \(\mathcal X\) is said to be a <em>reproducing kernel Hilbert space</em> &#40;RKHS&#41; if <em>evaluation functionals</em> \(\delta_x: f\mapsto f(x)\) are continuous for all \(x\in\mathcal X\) </div> </p> <p>If we consider a RKHS then, by Riesz&#39;s representation theorem, since \(\delta_x\) is a continuous functional, it has a <em>representer</em> in \(\mathcal H\) that we can denote \(k_x\) such that</p> <p>$$\begin{array}{c} \left\langle f, k_x\right\rangle_{\mathcal H} &=& \delta_x(f) \quad\!\! =\quad\!\! f(x). \end{array}$$</p> <p>We can then define a &#40;positive-definite&#41; bilinear form \(k:\mathcal X\times\mathcal X \to \mathbb R\) as \(k(x, x'):=\left\langle k_x, k_{x'}\right\rangle_{\mathcal H}\). This is known as the <strong>reproducing kernel</strong> of \(\mathcal H\); we will also write \(k_x = k(\cdot, x)\).</p> <p>There&#39;s then an important theorem that links the two &#40;and that we won&#39;t prove&#41; reproduced in a simplified form below:</p> <p><div class=colbox-blue > &#40;<strong>Moore-Aronszajn</strong>&#41; Every positive-definite bilinear form \(k\) is a reproducing kernel for some Hilbert space \(\mathcal H_k\). </div> </p> <p>When the kernel is clear from the context, we will simply write \(\mathcal H\) for the RKHS and \(k\) for its reproducing kernel.</p> <h2>Kernel embedding of a distribution</h2> <h3>Mean embedding</h3> <p>A classical way to try to represent points in a given space \(\mathcal X\) is to <em>embed</em> them in \(\mathbb R^s\) using a \(s\)-dimensional <em>feature map</em> \(\Phi:\mathcal X\to \mathbb R^s\) with $$\begin{array}{c} x\quad\!\!\mapsto\quad\!\!\Phi(x)\,\,=\,\, (\varphi_1(x), \varphi_2(x), \dots, \varphi_s(x)). \end{array}$$ Instead, we can now consider embedding points in a RKHS with the infinite dimensional feature map \(x\mapsto k_x\). In that case, we have an easily computable inner product between points with</p> <p>$$\begin{array}{c} \left\langle k_x, k_y\right\rangle_{\mathcal H} &=& \left\langle k(\cdot, x), k(\cdot, y)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! k(x, y). \end{array}$$</p> <p>Recall that an inner product is a <em>measure of alignment</em> so that this automatically gives us a measure of similarity between points through this kernel.</p> <p>When the embedding is <em>injective</em> &#40;i.e.: different objects are mapped to different points in the RKHS&#41;, the corresponding kernel is said to be <strong>characteristic</strong> &#40;this is often the case for standard kernels&#41;.</p> <p>An example of objects we can embed in an RKHS are distributions. Each distribution is then considered as a point which we can embed through the <em>mean-embedding</em>. <div class=colbox-blue > Let \(P\) denote a distribution among a set of distributions, the <strong>mean embedding</strong> is defined as follows:</p> <p>$$\begin{array}{c} P \mapsto \mu_X(P, k) \quad\!\!:=\quad\!\! \mathbb E_{X\sim P}[k(\cdot, X)]\quad\!\! =\quad\!\! \mathbb E_X[k_X],\end{array}$$ and, naturally, \(\mu_X(P, k)\in\mathcal H\). </div> </p> <p>When the kernel and the distribution are clear from the context, we will simply write \(\mu_X\). As before, note that we inherit a notion of <em>similarity</em> between probability measures by looking at the inner product on the RKHS: $$\begin{array}{c} \left\langle \mu_X(P, k), \mu_Y(Q, k)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \mathbb E_{X, Y}[k(X, Y)], \end{array}$$ and this can easily be estimated if we have samples from both \(P\) and \(Q\).</p> <p>Note finally that \(\mu_X\) represents <em>expectations with respect to \(P\)</em> i.e.: for any \(f\in\mathcal H\), $$\begin{array}{c} \left\langle f, \mu_X\right\rangle_{\mathcal H} &=& \mathbb E_X[{\left\langle f, k_X\right\rangle_{\mathcal H}}] \quad\!\! =\quad\!\! \mathbb E_X[f(X)] . \end{array}$$</p> <h3>Joint embedding</h3> <p>The generalisation to joint distributions is straightforward using tensor product feature spaces.</p> <p><div class=colbox-blue > Let \(X, Y\) be jointly distributed according to some distribution \(P\), the <strong>joint embedding</strong> of \(P\) is defined as $$\begin{array}{c} P \mapsto \mathcal C_{XY}(P) \quad\!\! :=\quad\!\! \mathbb E_{XY}[k_X\otimes k_Y], \end{array}$$ assuming that the two variables share the same kernel. </div> </p> <h3>MMD and HSIC</h3> <h3>Finite sample embeddings</h3> <h2>Kernel embeddings of conditional distributions</h2> <h3>Pointwise definition</h3> <h3>Conditional operator</h3> <h3>Finite sample kernel estimator</h3> <h2>Probabilistic reasoning with kernel embeddings</h2> <h3>Kernel sum rule</h3> <h3>Kernel chain rule</h3> <h3>Kernel Bayes rule</h3> <h3>Kernel Bayesian average and posterior decoding</h3> <h2>References</h2> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 12, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>