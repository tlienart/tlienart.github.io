<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>CV Ridge (pt.2)</title> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=jd-content > <h1 id=cv_ridge_part_2 ><a href="/pub/csml/glr/ridgecv-2.html#cv_ridge_part_2">CV Ridge &#40;part 2&#41;</a></h1> <p>Where in <a href="/pub/csml/glr/ridgecv.html">the first part</a>, we showed that the LOOCV trick could be obtained by using the Sherman-Morrison formula for the inversion of a rank-1 perturbation of an invertible matrix, in the general case where we want to sequentially drop more than one point. It is therefore not a rank-1 perturbation anymore but a sum of rank-1 perturbations.</p> <p>In these notes we investigate using Sherman-Morrison recursively and discuss in which circumstances this makes sense.</p> <h2 id=prelims ><a href="/pub/csml/glr/ridgecv-2.html#prelims">Prelims</a></h2> <p>When considering a leave-some-out scheme &#40;e.g. K-folds&#41;, we consider Ridge problems with data \(X_{S}\) and \(y_{S}\) where \(S\subset \{1,\dots,n\}\) is a set of rows to drop. As in the first part, we can write \(X_{S} = Ω_{S}X\) and \(y_S = Ω_{S}y\) where \(Ω_{S}\) is the identity matrix with the rows in \(S\) removed. Much like in the first part, \(\Omega_{S}^t\Omega_{S} = (I-D_{S})\) where \(D_{S}\) is a zero matrix with 1 on the diagonal for indices in \(S\). Observe also that:</p> \[ X^t D_{S} X = \sum_{i ∈ S} x_ix_i^t \] <p>and that \(X^tD_S y = z - \sum_{i∈S} y_ix_i\) where \(z=X^ty\).</p> <p>The solution of the Ridge regression problem corresponding to the subset \(S\) is therefore</p> <a id=eq-1 ></a>\[ β_S \quad=\quad \left(H - \sum_{i\in S} x_ix_i^t\right)^{-1} \left(z - \sum_{i\in S} y_i x_i\right), \] <p>where \(H = (X^tX + λI) = V(Σ^2+λI)V^t\) if \(X=UΣV^t\).</p> <p>As before, we care about the prediction error on the points that were removed: the vectors \(e_S\) with entries</p> \[ (e_S)_i = x_j^t β_S - y_j, \quad\text{where}\quad j = S_i. \] </p> <p>We now turn our attention to the efficient computation of the matrix inversion in <span class=eqref >(<a href="/pub/csml/glr/ridgecv-2.html#eq-1">2</a>)</span>.</p> <h3 id=recursive_sherman-morrison_rsm ><a href="/pub/csml/glr/ridgecv-2.html#recursive_sherman-morrison_rsm">Recursive Sherman-Morrison &#40;RSM&#41;</a></h3> <p>Let us compute, step-by-step, the inversion of \((H - x_1 x_1^t - x_2x_2^t)\) and show how this can be generalised. Let \(H_1 = H - x_1 x_1^t\) and \(H_2 = H_1 - x_2x_2^t\). Then, Sherman-Morrison gives</p> \[\begin{array}{c} H_1^{-1} &=& \displaystyle H^{-1} + {H^{-1}x_1 x_1^t H^{-1} \over 1 - x_1^t H^{-1} x_1}, \\[.5cm] H_2^{-1} &=&\displaystyle H_1^{-1} + {H_1^{-1}x_2 x_2^t H_1^{-1} \over 1 - x_2^t H_1^{-1} x_2}.\end{array}\] <p>Let us now write \(b_1 = H^{-1}x_1\), \(γ_1 = x_1^t b_1\), \(b_2=H_1^{-1}x_2\) and \(γ_2=x_2^tb_2\) then:</p> \[\begin{array}{c} H_1^{-1} &=& \displaystyle H^{-1} + {b_1 b_1^t \over 1 - γ_1}, \\[.5cm] H_2^{-1} &=&\displaystyle H^{-1} + {b_1 b_1^t \over 1 - γ_1} + {b_2 b_2^t \over 1 - γ_2}.\end{array}\] <p>It&#39;s straightforward to generalise this:</p> <a id=recursive_sm ></a>\[ \left( H - \sum_{i\in S} x_ix_i^t\right)^{-1} \quad=\quad H^{-1} + \sum_{i\in S} {b_ib_i^t\over 1-γ_i} \] <p>which can be computed recursively interlacing the computations for \(b_i\) and \(γ_i\) and the computation of the next term in the sum.</p> <h3 id=complexity_of_the_recursion ><a href="/pub/csml/glr/ridgecv-2.html#complexity_of_the_recursion">Complexity of the recursion</a></h3> <p>Let \(M=|S|\) the number of rows dropped per selection/fold. Then there are \(M\) terms in the sum <span class=eqref >(<a href="/pub/csml/glr/ridgecv-2.html#recursive_sm">6</a>)</span>. Each \(b_i\) is given by the application of a \(p\times p\) matrix on a vector. So, inherently, the complexity of this recursive inverse is \(O(Mp^2)\) and it shouldn&#39;t be used if \(M\) is of order comparable to \(p\).</p> <p>In K-folds cross validation, \(M = O(n/K)\) where \(K\) might typically be around \(5\) or \(10\) so that \(M\) may very well be of order \(p\) or larger. We will come back to this in the next section.</p> <h2 id=applying_rsm_in_the_tall_case ><a href="/pub/csml/glr/ridgecv-2.html#applying_rsm_in_the_tall_case">Applying RSM in the tall case</a></h2> <p>One unfortunate element of RSM is that it is not possible &#40;to the best of my knowledge&#41; to easily get the inverse of the matrix for several \(λ\) without recomputing the recursion each time. This is because the computation of each \(b_i\) after the initial one will involve non-diagonal matrices that are shaped by \(\lambda\). Note that in the LOO case, this was not the case because we only had one diagonal matrix to update which is \(O(p)\) &#40;i.e. negligible&#41;.</p> <p>The observation above means that for a single selection/fold, we have to pay \(O(κMp^2)\) if \(κ\) is the number of \(λ\) we want to test. Assuming we do \(K\) folds/selections, the overall cost is therefore \(O(κKMp^2)\) to form all of the \(e_S\) vectors.</p> <h3 id=k-folds_cv ><a href="/pub/csml/glr/ridgecv-2.html#k-folds_cv">K-folds CV</a></h3> <p>In K-folds CV, \(M\) is \(O(n/K)\) and the total cost with RSM is thus \(O(κnp^2)\).</p> <p>If, instead, we were to re-compute an SVD of \(X_S^tX_S\) per fold and recycle the computations for the \(λ\), the cost per fold is \(O((n - M + κ)p^2)\) or \(O(Knp^2)\) overall assuming that \(κ\) is dominated by \(n\).</p> <p>Since, in general, we are much more likely to want to test many \(λ\) &#40;large \(κ\)&#41; with a few folds &#40;with \(K\) of order \(5\)-\(10\)&#41;, doing re-computations per fold makes more sense than applying RSM.</p> <p><strong>Note</strong>: it is still important to recycle computations otherwise we pay \(O(κKnp^2)\) by doing a naive grid-computations for every fold and every \(\lambda\); surprisingly this seems to be what is currently done in SkLearn as per <a href="https://github.com/scikit-learn/scikit-learn/blob/e94b67a4d36bfa68f5a864a6401253846bac7138/sklearn/linear_model/_ridge.py#L1576-L1579">these lines of code</a>. <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: December 08, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script> <script src="/libs/highlight/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script>