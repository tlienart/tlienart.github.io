<!doctype html>
<html lang="en">
	<head>
		<meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
		 <!-- Un-minified script so that can play a bit -->
<link rel="stylesheet" href="/libs/katex/katex.min.css"></link>
 		
        
		
			<link rel="stylesheet" href="/css/main.css">
	        <link rel="icon" href="/assets/infra/favicon.png">
		
		 <title>Approximate Bayesian Inference - Introduction</title>	
	</head>
    <body>
		
        <header>
            <div class="blog-name"><a href="/">Thibaut Lienart</a></div>
            <nav>
                <ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/pub/csml.html">CS/ML notes</a></li>
                    <li><a href="/pub/julia.html">Julia notes</a></li>
                    <!-- <li><a href="/pub/misc.html">Misc.</a></li> -->
                </ul>
                <img src="/assets/infra/hamburger.svg" id="menu-icon">
            </nav>
        </header>
		

<!-- Content appended here -->

<div class="jd-content">
<h1><a id="introduction-to-approximate-bayesian-inference" href="#introduction-to-approximate-bayesian-inference">Introduction to Approximate Bayesian Inference</a></h1>
<p>In <em>Bayesian Machine Learning</em> &#40;BML&#41;, one is broadly interested in recovering a posterior distribution over the parameters \(x\) &#40;with typically \(x\in\mathcal X\subseteq \mathbb R^d\)&#41; of some parametric model of interest for which we have a likelihood \(p(\mathcal D|x)\) &#40;where \(\mathcal D\) denotes the data&#41; using Bayes&#39; trick:</p>
<a id="bayes-ml-1"></a>\[ p(x|\mathcal D) \quad\!\! \propto\quad\!\! p(\mathcal D|x)p(\theta). \]
<p>In equation <span class="eqref">(<a href="#bayes-ml-1">1</a>)</span>, \(p(x)\) denotes our <em>prior</em> belief on what the parameters should be and \(p(x|\mathcal D)\) is the <em>posterior distribution</em> over \(x\). The posterior is then used to make more robust predictions with the model and, in particular, to account for <strong>uncertainty</strong> in the model parameters.</p>
<p>This approach is optimal under two key assumptions:</p>
<ol>
<li><p>the prior is chosen appropriately,</p>
</li>
<li><p>the posterior is tractable &#40;or at least we can sample from it&#41;.</p>
</li>
</ol>
<p>The first assumption is usually overlooked by researchers in computational Bayesian statistics who usually assume the prior is given &#40;and, in practice, it typically encodes a regularisation&#41;. This is not without its weaknesses but it would require an entire set of notes to be discussed appropriately. </br> The second assumption is generally far from true especially with complex models and many approaches have been suggested such as MCMC methods; while these can do very well when \(\theta\) is low-dimensional, they typically struggle when the dimensionality is high.</p>
<p>In these notes we define <em>Approximate Bayesian Inference</em> &#40;ABI&#41; in broad terms as the class of methods attempting to recover a distribution \(q\) such that \(q\) is a good <em>proxy</em> for the posterior distribution of interest \(p\). A number of such methods are commonly used when the dimensionality of the problem makes MCMC-like methods computationally too expensive.</p>
<h2><a id="variational-problem" href="#variational-problem">Variational problem</a></h2>
<p>In what follows we simplify the context a little bit by considering methods which find good proxy \(q\) to some target distribution \(p\). We write \(\mathcal F\mathcal \subset \mathcal P(\mathcal X)\) the space of candidate proxy distributions where \(\mathcal P(\mathcal X)\) designates all probability distributions on \(\mathcal X\).</p>
<p>Let \(D:\mathcal P(\mathcal X)\times \mathcal P(\mathcal X) \mapsto \mathbb R_+\) denote a discrepancy measure between two probability distribution functions. Then, the generic variational problem considered in ABI is the minimisation of this \(D\) between the candidate \(q\in \mathcal F\) and the target \(p\):</p>
<a id="eqbasic-abi"></a>\[ q^\star \quad\!\! \in\quad\!\! \arg\min_{q\in\mathcal F} D(q, p). \]
<p>Techniques attempting to solve such problems rely upon exploiting at least one of the following three main characteristics:</p>
<ol>
<li><p>the definition of the discrepancy measure \(D\) &#40;aka. divergence&#41;,</p>
</li>
<li><p>the definition of the restricted set of probability distributions \(\mathcal F\),</p>
</li>
<li><p>the structure of the target distribution \(p\).</p>
</li>
</ol>
<p>Many discrepancy measures can be considered such as the total variation distance, Wasserstein distances or \(f\)-divergences. However, most of these choices can lead to the corresponding problem <span class="eqref">(<a href="#eqbasic-abi">2</a>)</span> being computationally &#40;very&#41; expensive to solve in general, especially if the space \(\mathcal X\) is continuous.</p>
<p>In these notes we will focus on the Kullback-Leibler &#40;KL&#41; divergence which, as we&#39;ll see, can lead to optimisation problems for which efficient computational methods exist. As a reminder, the KL divergence between two distributions \(p\) and \(q\) is defined as</p>
\[ \mathrm{KL}(p, q) \quad\!\! =\quad\!\! \mathbb E_p[\log p(X) - \log q(X)]. \]
<p>We will look at both \(\mathrm{KL}(p, q)\) and \(\mathrm{KL}(q, p)\) and, while in both case <span class="eqref">(<a href="#eqbasic-abi">2</a>)</span> is intractable, in both cases proxy problems can be obtained and lead to two well known algorithms &#40;along with related variants&#41;:</p>
<ul>
<li><p>the &#40;Mean-Field&#41; Variational Bayes algorithm &#40;&#40;MF&#41;VB&#41;, which we will discuss <a href="/pub/csml/abi/vb.html">here</a>,</p>
</li>
<li><p>the Expectation Propagation algorithm &#40;EP&#41;, which we will discuss <a href="/pub/csml/abi/ep.html">here</a>.</p>
</li>
</ul>
<h2><a id="a-warning-note" href="#a-warning-note">A warning note</a></h2>
<p>At the risk of stating the obvious, it must stressed that ABI algorithms do in general <em>not lead to the correct posterior</em> but rather to a proxy which may be a very poor approximation to the target posterior. Therefore, no matter what ABI algorithm you use, you should in general be very careful about how you interpret the recovered proxy \(q\) and should ideally refrain from making strong statements about the uncertainty in the model parameters based upon \(q\) unless you&#39;ve tested it somehow.</p>
<p>While there are some cases where ABI algorithms can give good proxy &#40;in particular if \(p\) is known to be log-concave and thin-tailed&#41;, in many circumstances, and especially in the context of complex/high-dimensional models such as neural networks, ABI algorithms lead to proxy distributions which may</p>
<ul>
<li><p>typically recover reasonable location &#40;e.g. the most likely \(\theta\) according to the proxy \(q\) is a good parameter for the model in terms of generalisation error&#41;</p>
</li>
<li><p>typically recover appalling uncertainty &#40;often catastrophically underestimated&#41;.</p>
</li>
</ul>
<p>ABI is a nice theoretical field with interesting mathematical tools but that should not prevent you from checking that the results obtained are sensible. If you are not in a position to check the validity of the obtained proxy, you should probably revert to simply considering the maximum a posteriori &#40;MAP&#41; and avoid fooling yourself into thinking that you have a workable model of the uncertainty.
<div class="page-foot">
		<div class="copyright">
				&copy; T. Lienart. Last modified: March 26, 2019. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
		</div>
</div>

</div>
<!-- CONTENT ENDS HERE -->
        
                <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

        
        
    </body>
</html>
