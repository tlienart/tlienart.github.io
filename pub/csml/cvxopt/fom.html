<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>First order methods</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h1>Thoughts on first order descent methods</h1> <p>First order methods &#40;FOM&#41; broadly designate iterative methods for continuous and differentiable optimisation that mainly use information from the &#40;sub&#41;gradient of the function.</p> <p>In these notes we consider again the constrained minimisation problem \(\min_{x\in C} f(x)\) and, to simplify the presentation, we&#39;ll assume that \(C\) is closed and convex and that \(f\) is strictly convex and smooth on \(C\).</p> <p>What we&#39;re interested in here is discussing, at a high level, how to generate a <em>minimising sequence</em> for \(f\) i.e., a sequence \(\{x_k\}\) with \(x_k\in C\) such that \(f(x_{k+1}) < f(x_k)\) and \(f(x_k) \to f(x^\dagger)\) as \(k\) grows.</p> <p><em>Remark</em>: all norms \(\|\cdot\|\) are 2-norms.</p> <h2>Local linearisation</h2> <p>Local linearisation is basically what first order methods are about: form a linear approximation of the function around a current point. Still, let&#39;s try to look at this from scratch: consider the local linearisation of \(f\) around a point \(a\in C^\circ\):</p> $$ f(x) \quad\!\! =\quad\!\! f(a) + \left\langle x-a, \nabla f(a)\right\rangle + r(x, a) $$ <p>where \(r(x, a)\) is the <em>remainder function</em>.</p> <p><strong>Note</strong>: here you may be thinking: <em>Taylor expansion</em>, but actually let&#39;s put that on the side for now and just assume that you&#39;re building this \(f(a)+\left\langle x-a,\nabla f(a)\right\rangle\) and that you want to investigate the properties of the remainder function.</p> <p>That function enjoys the following properties:</p> <ol> <li><p>\(r(a, a)=0\), and \(r(x, a)>0\) for all \(x\neq a\) by strict convexity of \(f\),</p> <li><p>\(r(\cdot, a)\) is also strictly convex and smooth for all \(a\in C^\circ\).</p> </ol> <p>Let us introduce a more compact notation: \(r_a(\delta) := r(a+\delta, a)\) which will be quite useful. It is easy to note that \(r_a\) is smooth and strictly convex with \(r_a(0)=0\) and, in fact, is globally minimised at \(0\). By definition of strict convexity, we have</p> $$ r_a(\delta') \quad\!\! >\quad\!\! r_a(\delta) + \left\langle \delta'-\delta, \nabla r_a(\delta)\right\rangle, \quad \forall\delta' \neq \delta. $$ <p>Taking \(\delta'=0\) and rearranging terms yields</p> $$ r_a(\delta) \quad\!\! <\quad\!\! \left\langle \delta, \nabla r_a(\delta)\right\rangle \quad\!\! \le\quad\!\! \|\delta\|\|\nabla r_a(\delta)\|, $$ <p>using Cauchy-Schwartz for the second inequality. Rearranging again gives \(r_a(\delta)/\|\delta\| < \|\nabla r_a(\delta)\|\) for all \(\delta\neq 0\). Since \(r_a\) is smooth away from \(0\) and since \(\nabla r_a(0)=0\) &#40;minimiser&#41;, we can take the limit on both sides of the inequality as \(\|\delta\|\to 0\) and get the following result.</p> <div class=colbox-blue >The function \(r_a\) is \(o(\|\delta\|)\) meaning <a name=R3Ni ></a>$$ \lim_{\|\delta\|\to 0} {r_a(\delta)\over \|\delta\|} \quad\!\! =\quad\!\! 0. $$</div> <p>This could have been obtained directly from consider the Taylor expansion of \(f\) but I think it&#39;s nice to obtain it using only the notion of convexity. Another note is that the reasoning essentially still holds if \(f\) is only convex and sub-differentiable.</p> <h2>Admissible descent steps</h2> <p>Let&#39;s consider a point \(x\) in \(C\) and a step from \(x\) to \(x+\delta\) for some \(\delta\neq 0\). We&#39;re interested in determining what are &quot;good&quot; steps \(\delta\) to take. Using the notations from the previous point, we have</p> $$ f(x+\delta) \quad\!\! =\quad\!\! f(x)+\left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta). $$ <p>Such a step will be called an <em>admissible descent step</em> if \(x+\delta\in C\) and if it decreases the function, i.e. if \(f(x+\delta) < f(x)\) or:</p> <a name=A0Qz ></a>$$ \left\langle \delta, \nabla f(x)\right\rangle + r_x(\delta) \quad\!\! <\quad\!\! 0. $$ <p>Let \(\mathcal D_\) be the set of admissible descent steps from \(x\). Observe that it is always non-empty provided that \(0<\|\nabla f(x)\|\infty\).</p> <p>To show this, let \(\delta_\epsilon := -\epsilon(g+v)\) with \(\epsilon>0\), \(g=\nabla f(x)/\|\nabla f(x)\|\) &#40;the unit vector in direction of the gradient&#41; and \(v\) such that \(\left\langle v, \nabla f(x)\right\rangle=0\) and \(0 < \|v\|\le 1\). Then just by plugging things in we have $$\left\langle \delta_\epsilon, \nabla f(x)\right\rangle + r_x(\delta_\epsilon) \quad\!\! =\quad\!\! -\epsilon + r_x(\epsilon(g+v)) $$ but recall that \(r_x(\epsilon(g+v)) = o(\epsilon\|g+v\|)\) by <span class=eqref >(<a href="#R3Ni">4</a>)</span>; by definition of \(v\) and the triangular inequality we also have that \(\|g+v\|\le 2\epsilon\) so that \(r_x(\delta_\epsilon)=o(\epsilon)\) so that, for sufficiently small \(\epsilon\), the condition <span class=eqref >(<a href="#A0Qz">6</a>)</span> holds for \(\delta_\epsilon\).</p> <div class=colbox-blue >For sufficiently small \(\epsilon\), \(\delta_\epsilon=-\epsilon(g+v)\) is an admissible descent step.</div> <p>Note that, by construction, these \(\delta_\epsilon\) span a half-ball of radius \(\epsilon\) so that \(\mathcal D_x\) is non-empty and also non-degenerate.</p> <div class=colbox-blue >Let \(w\) be such that \(0<\|w\|=:\eta\) and \(\left\langle w, \nabla f(x)\right\rangle<0\), then, provided \(\eta\) is small enough, \(w\in\mathcal D_x\).</div> <p>Obviously, what we would like is to get the <em>best possible step</em>:</p> <a name=57xf ></a>$$ \delta^\dagger \quad\!\! \in\quad\!\! \arg\min_{\delta \mid x+\delta \in C} \,\, \left[\left\langle \delta, \nabla f(x)\right\rangle+r_x(\delta)\right] $$ <p>which leads directly to the minimiser \(x^\dagger = x+\delta^\dagger\). Of course that&#39;s a bit silly since solving <span class=eqref >(<a href="#57xf">8</a>)</span> is as hard as solving the original problem. However the expression <span class=eqref >(<a href="#57xf">8</a>)</span> will help us generate descent algorithms.</p> <h2>Local update schemes</h2> <p>What we would like is thus to solve a problem that is simpler than <span class=eqref >(<a href="#57xf">8</a>)</span> and yet still generates admissible descent direction. A natural way to do that is to replace \(r(z,x)\) and \(r_x(\delta)\) by <em>a proxy function</em> \(d(z,x)\) and corresponding \(d_x(\delta)\) enjoying the same properties of positive definiteness and strict convexity. We then have a general iteration scheme:</p> $$ \tilde\delta_k \quad\!\! \in\quad\!\! \arg\min_{\delta \mid x_k+\delta \in C} \left[\left\langle \delta, \nabla f(x_k)\right\rangle + \beta_k d_x(\delta_k)\right]. $$ <div class=colbox-red >ongoing</div> <h2>References</h2> <ol> <li><p><a name=eNFy ></a> <strong>El Ghaoui</strong>, <a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture19.pdf">Interior-Point Methods</a>, 2012. Lecture notes at Berkeley, covering another topic &#40;IPMs&#41; but also summarising descent methods in general.</p> </ol> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 28, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>