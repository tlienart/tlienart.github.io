<!doctype html> <html lang=en  class=h-100 > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta name=description  content="Thibaut Lienart's website"> <meta name=author  content="Thibaut Lienart"> <meta name=generator  content=Franklin > <link rel=icon href="/assets/favicon.png"> <link href="/css/bootstrap.min.css" rel=stylesheet > <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/libs/highlight/github.min.css"> <script src="/libs/clipboard.min.js"></script> <title>CV Ridge &ndash; pt. I</title> <link rel=stylesheet  href="/css/main.css"> <style> :root { --oxb: #002147; --camb: #A3C1AD; --banana: #FFE135; --lgray: #D3D3D3; } .bd-placeholder-img { font-size: 1.125rem; text-anchor: middle; -webkit-user-select: none; -moz-user-select: none; user-select: none; } @media (min-width: 768px) { .bd-placeholder-img-lg {font-size: 3.5rem;} } .border-oxb {border-color: var(--oxb);} .bg-oxb {background-color: var(--oxb);} .text-oxb {color: var(--oxb);} .border-camb {border-color: var(--camb);} .bg-camb {background-color: var(--camb);} .text-camb {color: var(--camb);} </style> <body class="d-flex flex-column h-100"> <header> <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark"> <div class=container-fluid > <a class=navbar-brand  id=main-name  href="/">T. Lienart</a> <ul class=navbar-nav  id=main-nav > <li class=nav-item > <a class="nav-link " aria-current=page  href="/posts/">Posts</a> <li class=nav-item > <a class="nav-link " href="/tags/">Tags</a> <li class=nav-item > <a class="nav-link " href="/about/">About</a> </ul> <ul class="navbar-nav me-auto" id=title-nav > <li class=nav-item  id=post-title > <a class="nav-link active" href="#">CV Ridge &ndash; pt. I</a> </ul> </div> </nav> </header> <main class=flex-shrink-0 > <div class=container > <div class=pt-5 > </div> <h1 id=cv_ridge_loo ><a href="#cv_ridge_loo" class=header-anchor >CV Ridge &#40;LOO&#41;</a></h1> <div class=tags ><a href="/tags/" id=tag-icon ><svg width=20  height=20  viewBox="0 0 512 512"><defs><style>.cls-1{fill:#141f38}</style></defs><path class=cls-1  d="M215.8 512a76.1 76.1 0 0 1-54.17-22.44L22.44 350.37a76.59 76.59 0 0 1 0-108.32L242 22.44A76.11 76.11 0 0 1 296.2 0h139.2A76.69 76.69 0 0 1 512 76.6v139.19A76.08 76.08 0 0 1 489.56 270L270 489.56A76.09 76.09 0 0 1 215.8 512zm80.4-486.4a50.69 50.69 0 0 0-36.06 14.94l-219.6 219.6a51 51 0 0 0 0 72.13l139.19 139.19a51 51 0 0 0 72.13 0l219.6-219.61a50.67 50.67 0 0 0 14.94-36.06V76.6a51.06 51.06 0 0 0-51-51zm126.44 102.08A38.32 38.32 0 1 1 461 89.36a38.37 38.37 0 0 1-38.36 38.32zm0-51a12.72 12.72 0 1 0 12.72 12.72 12.73 12.73 0 0 0-12.72-12.76z"/><path class=cls-1  d="M217.56 422.4a44.61 44.61 0 0 1-31.76-13.16l-83-83a45 45 0 0 1 0-63.52L211.49 154a44.91 44.91 0 0 1 63.51 0l83 83a45 45 0 0 1 0 63.52L249.31 409.24a44.59 44.59 0 0 1-31.75 13.16zm-96.7-141.61a19.34 19.34 0 0 0 0 27.32l83 83a19.77 19.77 0 0 0 27.31 0l108.77-108.7a19.34 19.34 0 0 0 0-27.32l-83-83a19.77 19.77 0 0 0-27.31 0l-108.77 108.7z"/><path class=cls-1  d="M294.4 281.6a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85zM256 320a12.75 12.75 0 0 1-9.05-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2A12.8 12.8 0 0 1 256 320zM217.6 358.4a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 1 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85z"/></svg></a><a href="/tag/linear_algebra/">linear algebra</a>, <a href="/tag/machine_learning/">machine learning</a></div> <div class=franklin-toc ><ol><li><a href="#brief_recap">Brief recap</a><li><a href="#complexity_of_solving_ridge">Complexity of solving Ridge</a><ol><li><a href="#tall_case_np">Tall case &#40;\(n>p\)&#41;</a><li><a href="#fat_case_pn">Fat case &#40;\(p>n\)&#41;</a><li><a href="#conclusion">Conclusion</a></ol><li><a href="#recycling_computations_when_changing_λ">Recycling computations when changing \(λ\)</a><ol><li><a href="#tall_case">Tall case</a><li><a href="#fat_case">Fat case</a><li><a href="#simple_code">Simple code</a></ol><li><a href="#loocv_trick">LOOCV trick</a><ol><li><a href="#tall_case__2">Tall case</a><li><a href="#fat_case__2">Fat case</a><li><a href="#simple_code__2">Simple code</a></ol><li><a href="#conclusion__2">Conclusion</a><li><a href="#references">References</a></ol></div> <h2 id=brief_recap ><a href="#brief_recap" class=header-anchor >Brief recap</a></h2> <p>The Ridge regression is a simple penalised regression model corresponding to a L2 loss with L2 penalty on the coefficients:</p> \[ β_{\text{ridge}} \quad\!\! =\quad\!\! \arg\min \|X\beta - y\|_2^2 + \lambda \|\beta\|_2^2, \] <p>where \(X\) is \(n\times p\) and \(\lambda > 0\) is the penalty &#40;hyper&#41;parameter. We&#39;ll drop the subscript &quot;ridge&quot; as it&#39;s now obvious from the context.</p> <p>The minimum verifies</p> <a id=ridge-tall  class=anchor ></a>\[ (X^tX + λ I) β \quad = \quad X^ty. \] <h2 id=complexity_of_solving_ridge ><a href="#complexity_of_solving_ridge" class=header-anchor >Complexity of solving Ridge</a></h2> <h3 id=tall_case_np ><a href="#tall_case_np" class=header-anchor >Tall case &#40;\(n>p\)&#41;</a></h3> <p>The computational cost of solving <span class=eqref >(<a href="#ridge-tall">2</a>)</span> directly is \(O(np^2 + p^3)\):</p> <ul> <li><p>building \(X^tX\) is \(O(np^2)\),</p> <li><p>solving the resulting \(p\times p\) system is \(O(p^3)\).</p> </ul> <p>In the case where \(n > p\) &#40;<em>tall</em> case&#41;, this is asymptotically dominated by the first term: \(O(np^2)\).</p> <h3 id=fat_case_pn ><a href="#fat_case_pn" class=header-anchor >Fat case &#40;\(p>n\)&#41;</a></h3> <p>In the case where \(p > n\) &#40;<em>fat</em> case&#41;, it is more efficient to solve a derived system obtained by pre-multiplying <span class=eqref >(<a href="#ridge-tall">2</a>)</span> by \(X\):</p> <a id=ridge-fat  class=anchor ></a>\[ (XX^t + λ I)Xβ \quad = \quad XX^t y \] <p>indeed, in that case \(XX^t\) is \(n\times n\) which is then smaller than \(p \times p\). It is useful to show a way to solve this explicitly: consider the SVD \(X = UΣ V^t\), then \(XX^t = UΣ^2 U^t\) and the equation <span class=eqref >(<a href="#ridge-fat">3</a>)</span> becomes</p> \[ U(Σ^2 + λ I) Σ V^t β \quad=\quad U Σ^2U^t y\] <p>taking advantage from the fact that \(U\) is orthogonal so that \(UU^t = I\) &#40;same with \(V\)&#41;. This can be further massaged into</p> <a id=ridge-fat-sol  class=anchor ></a>\[\begin{array}{rcl} β &=& V Σ (Σ^2 + \lambda I)^{-1}U^t y \\ &=& X^t U(Σ^2 + \lambda I)^{-1}U^t y,\end{array}\] <p>using that \(X=UΣ V^t\) so that \(V = X^t U Σ^{-1}\). Overall, the complexity is asymptotically dominated by the construction of \(XX^t\) which is \(O(pn^2)\) followed by the complexity of computing its SVD which is \(O(n^3)\).</p> <h3 id=conclusion ><a href="#conclusion" class=header-anchor >Conclusion</a></h3> <ul> <li><p><strong>tall case</strong>: \(n > p\), \(O(np^2)\), dominated by the construction of \(X^tX\),</p> <li><p><strong>fat case</strong>: \(n < p\), \(O(pn^2)\), dominated by the construction of \(XX^t\).</p> </ul> <h2 id="recycling_computations_when_changing_λ"><a href="#recycling_computations_when_changing_λ" class=header-anchor >Recycling computations when changing \(λ\)</a></h2> <p>When tuning the hyperparameter \(λ\), we will potentially want to compute \(β_λ\) for a number of different \(λ\); in Ridge regression we can ensure that this is done efficiently so that trying out different \(λ\) is cheap after having computed the first solution.</p> <p>We will again consider separately the tall and fat case, and will assume that it is reasonable to form either \(X^tX\) or \(XX^t\), for each case we will consider its SVD.</p> <h3 id=tall_case ><a href="#tall_case" class=header-anchor >Tall case</a></h3> <p>Let \(X = UΣV^t\) and consequently \(X^tX = V\Sigma^2V^t\) then <span class=eqref >(<a href="#ridge-tall">2</a>)</span> can be written</p> \[ V(Σ^2 + λ I)V^t β \quad =\quad X^ty \] <p>and consequently \(\beta = V(Σ^2 + λ I)^{-1}X^ty\).</p> <p>Once the SVD has been formed, we are left, for any \(λ\), with computing \(VD_λz\) where \(z = X^ty\) can be computed once in \(O(np)\), \(D_λ = (Σ^2 + λ I)^{-1}\) is a diagonal matrix which is computed and applied in \(O(p)\) and the application of \(V\) is \(O(p^2)\).</p> <p>Overall:</p> <div class=ctable ><table><tr><th align=left >Operation<th align=left >Cost<tr><td align=left >initial computation of \(X^tX\)<td align=left >\(O(np^2)\)<tr><td align=left >initial SVD of \(X^tX\)<td align=left >\(O(p^3)\)<tr><td align=left >computation of \(VD_λz\)<td align=left >\(O(p^2)\) per \(λ\)</table></div> <p>in other words, computing the solution for \(O(n)\) different \(\lambda\) is only twice as expensive asymptotically as computing the solution for a single one.</p> <h3 id=fat_case ><a href="#fat_case" class=header-anchor >Fat case</a></h3> <p>In the fat case, we already have the equation <span class=eqref >(<a href="#ridge-fat-sol">5</a>)</span>: \(β = X^t U(Σ^2+λI)^{-1}U^ty\), we get an analogous argument:</p> <div class=ctable ><table><tr><th align=left >Operation<th align=left >Dominating Cost<tr><td align=left >initial computation of \(XX^t\)<td align=left >\(O(pn^2)\)<tr><td align=left >initial SVD of \(XX^t\)<td align=left >\(O(n^3)\)<tr><td align=left >initial computation of \(w = U^ty\)<td align=left >\(O(pn)\)<tr><td align=left >computation of \(X^t U D_λ w\)<td align=left >\(O(np)\) per \(λ\)</table></div> <p>in other words, computing the solution for \(O(n)\) different \(\lambda\) is only twice as expensive asymptotically as computing the solution for a single one.</p> <p><strong>Notes</strong>:</p> <ul> <li><p>Observe that for <em>both</em> the tall and fat case, paying around <em>twice</em> the cost of the initial computation, allows to compute the solutions for \(O(n)\) different \(λ\).</p> <li><p>thus far we have not considered the computation of the intercept but it&#39;s not hard to do and does not change the complexity analysis.</p> </ul> <h3 id=simple_code ><a href="#simple_code" class=header-anchor >Simple code</a></h3> <p>We can easily check all this with code in Julia; for instance, let&#39;s consider the fat case:</p> <pre><code class=language-julia >using LinearAlgebra

basic_ridge_solve&#40;X, y, λ&#61;1&#41; &#61; &#40;X&#39;X &#43; λ*I&#41; \ X&#39;y

X_fat &#61; randn&#40;50, 500&#41;
y_fat &#61; randn&#40;50&#41;

λ &#61; 1

# slow solve requiring the solution of a 500x500 system
β_fat &#61; basic_ridge_solve&#40;X_fat, y_fat, λ&#41;;</code></pre> <p>Now let&#39;s check that we can recover it the cheaper way &#40;via the SVD of \(XX^t\)&#41;</p> <pre><code class=language-julia ># SVD of a 50x50 system
F &#61; svd&#40;Symmetric&#40;X_fat * X_fat&#39;&#41;&#41;
U, S &#61; F.U, F.S
w_fat &#61; U&#39;y_fat ./ &#40;S .&#43; λ&#41;
β_fat ≈ X_fat&#39; * &#40;U * w_fat&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>Now let&#39;s change \(λ\):</p>
<pre><code class=language-julia >λ′ &#61; 3

# naive route
β_fat′ &#61; basic_ridge_solve&#40;X_fat, y_fat, λ′&#41;

# efficient route
w_fat′ &#61; U&#39;y_fat ./ &#40;S .&#43; λ′&#41;
β_fat′ ≈ X_fat&#39; * &#40;U * w_fat′&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<h2 id=loocv_trick ><a href="#loocv_trick" class=header-anchor >LOOCV trick</a></h2>
<p>In Leave-One-Out CV, for a given \(λ\) we want to train the model for each of \(n-1\) cases where we consider only \(n-1\) data points and want to report the error on the last point.</p>
<p>Let \(X_{(i)}\) &#40;resp \(y_{(i)}\)&#41; be the matrix &#40;resp. vector&#41; with the \(i\)th row removed, and let \(β_{(i)}\) be the Ridge coefficients in that case. We are interested in the predicted error on the dropped point:</p>
\[ e_i \quad=\quad x_i^tβ_{(i)} - y_i\]
<p>We know that for a fixed \(i\), we can compute that error efficiently for many \(\lambda\) but it does require one initial SVD. That&#39;s a bit annoying because we have to do that for every \(i\), meaning a lot of SVDs to compute; this seems silly, surely we can re-use some computations&#33;</p>
<p>It&#39;s well known that we can indeed speed things up. See for instance <span class=bibref ><a href="#rifkin07">Rifkin and Lippert (2007)</a></span> whose formula is used in Sklearn&#39;s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html">RidgeCV</a> model, or <span class=bibref ><a href="#golub78">Golub, Heath and Wahba (1978)</a></span> for a statistical perspective. The presentation below is different but the gist in terms of the complexity gain is the same.</p>
<h3 id=tall_case__2 ><a href="#tall_case__2" class=header-anchor >Tall case</a></h3>
<p>Let us write \(X_{(i)} = Ω_{(i)} X\) where \(Ω_i\) is the identity matrix with the \(i\)th row removed. We then also have \(y_{(i)}=\Omega_{(i)}y\). It is not hard to show that \(Ω_{(i)}^tΩ_{(i)} = (I - D_{(i)})\) where \(D_{(i)}\) is an all-zero matrix with a 1 on the <nobr> \(i\)th</nobr> diagonal element.</p>
<p>The \(i\)th Ridge solution verifies \((X_{(i)}^tX_{(i)} + λI)β_{(i)} = X_{(i)}^t y_{(i)}\). Using the notations introduced in the previous paragraph, we can rewrite this as</p>
<a id=loo1  class=anchor ></a>\[ ((X^tX + λI) - X^tD_{(i)}X)β_{(i)} \quad=\quad X^t(I-D_{(i)})y \]
<p>If we write \(H = (X^tX + λI)\), then we know that it can be efficiently inverted assuming  we have initially computed the SVD of \(X^tX\). Note also that \(X^tD_{(i)}X = x_i x_i^t\).</p>
<p>On the left-hand side of <span class=eqref >(<a href="#loo1">8</a>)</span> we have therefore a rank-1 perturbation of an invertible matrix. The inverse of such a matrix can be readily expressed using the Sherman-Morrison formula &#40;see the page on <a href="/pub/csml/mtheory/matinvlem.html">matrix inversion lemmas</a> for details&#41;:</p>
\[ (H - x_ix_i^t)^{-1} \quad=\quad H^{-1} + {H^{-1}x_i x_i^t H^{-1} \over 1 - x_i^t H^{-1} x_i}. \]
<p>Plugging this in the lhs of <span class=eqref >(<a href="#loo1">8</a>)</span>, we can get an explicit expression for \(β_{(i)}\). But remember that what we really want is \(e_i\) so that we&#39;re more interested in \(x_i^t β_{(i)}\):</p>
\[ x_i^tβ_{(i)} \quad=\quad \left(x_i^t H^{-1} + {x_i^tH^{-1}x_i x_i^t H^{-1}\over 1-x_i^tH^{-1}x_i}  \right)X^t(I-D_{(i)})y. \]
<p>As it turns out, this can be simplified a fair bit. Note that the second factor can be re-written \(X^ty - X^tD_{(i)}y\) but that last term is simply \(x_iy_i\); let also \(z=X^ty\) which can be pre-computed and let \(γ=x_i^tH^{-1}x_i\). Then, massaging a bit, we get</p>
\[ x_i^tβ_{(i)} \quad=\quad {x_i^t H^{-1}z - γy_i\over 1-γ}, \]
<p>and, correspondingly, \(e_i = (x_i^tH^{-1}z - y_i) / (1-γ)\).</p>
<p>To conclude, with \(H^{-1} = V(Σ^2+λI)^{-1}V^t\), \(w = V^tz\), \(g_i=V^tx_i\) and \(g̃_i = (Σ^2+λI)^{-1}g_i\), we can rewrite the computation of \(e_i\) as</p>
<a id=loo-trick  class=anchor ></a>\[ e_i \quad = \quad {g̃_i^t w - y_i \over 1 - g̃_i^tg_i}  \]
<p>and  the cost of computing <em>all</em> the LOO errors is:</p>
<div class=ctable ><table><tr><th align=left >Operation<th align=center >Dominating complexity<tr><td align=left >form \(X^tX\) and compute its SVD<td align=center >\(O(np^2)\)<tr><td align=left >pre-compute \(z=X^ty\) and \(w=V^tz\)<td align=center >\(O(pn)\)<tr><td align=left >&#40;\(∀i\)&#41; compute \(g_i = V^tx_i\)<td align=center >\(O(p^2)\)<tr><td align=left >&#40;\(∀i,λ\)&#41; compute \(g̃_i = (Σ^2+λI)^{-1} g_i\)<td align=center >\(O(p)\)<tr><td align=left >&#40;\(∀i,λ\)&#41; compute \(e_i = (g̃_i^tw - y_i) / (1-g̃_i^tg_i)\)<td align=center >\(O(p)\)</table></div>
<p>So overall, the initial setup cost is dominated by \(O(np^2)\) and the subsequent cost is that of computing each \(g_i\): \(O(np^2)\); finally there&#39;s a cost \(O(npκ)\) where \(κ\) is the number of \(λ\) tested.</p>
<p>In other words, modulo constant factors, it costs the same to get <em>all</em> the LOO errors for \(O(p)\) different \(\lambda\) than to get a single Ridge solution&#33;</p>
<h3 id=fat_case__2 ><a href="#fat_case__2" class=header-anchor >Fat case</a></h3>
<p>When \(p > n\), we already know that it&#39;s beneficial to consider the SVD of \(XX^t = UΣ^2U^t\) instead of that of \(X^tX\). Here it turns out that just taking <span class=eqref >(<a href="#loo-trick">12</a>)</span> and computing \(g_i, g̃_i\) and \(w\) in terms of \(U\) and \(X\) instead of \(V\) is all we need to do; for this recall that \(X = UΣV^t\) so that \(V^t = Σ^{-1}U^tX\):</p>
<div class=ctable ><table><tr><th align=left >Operation<th align=center >Dominating complexity<tr><td align=left >form \(K = XX^t\) and compute its SVD<td align=center >\(O(pn^2)\)<tr><td align=left >pre-compute \(w=Σ^{-1}U^t K y\)<td align=center >\(O(n^2)\)<tr><td align=left >&#40;\(∀i\)&#41; compute \(g_i = Σ^{-1} U^t X x_i\)<td align=center >\(O(np)\)<tr><td align=left >&#40;\(∀i,λ\)&#41; compute \(g̃_i = (Σ^2+λI)^{-1} g_i\)<td align=center >\(O(n)\)<tr><td align=left >&#40;\(∀i,λ\)&#41; compute \(e_i = (g̃_i^tw - y_i) / (1-g̃_i^tg_i)\)<td align=center >\(O(n)\)</table></div>
<p>So overall, the initial setup cost is dominated by \(O(pn^2)\) and the subsequent cost is that of computing each \(g_i\): \(O(pn^2)\); finally there&#39;s a cost \(O(n^2κ)\) where \(κ\) is the number of \(λ\) tested.</p>
<p>Again, modulo constant factors, it costs the same to get all the LOO errors for \(O(p)\) different \(λ\) than to get a single Ridge solution.</p>
<h3 id=simple_code__2 ><a href="#simple_code__2" class=header-anchor >Simple code</a></h3>
<p>We can easily check all this with code again, let&#39;s extend the previous case:</p>
<pre><code class=language-julia >using InvertedIndices
i &#61; 5   # random index between 1 and n
λ &#61; 2.5

X̃ᵢ &#61; X_fat&#91;Not&#40;i&#41;,:&#93;
ỹᵢ &#61; y_fat&#91;Not&#40;i&#41;,:&#93;
β̃ᵢ &#61; basic_ridge_solve&#40;X̃ᵢ, ỹᵢ, λ&#41;

xᵢ &#61; vec&#40;X_fat&#91;i,:&#93;&#41;
yᵢ &#61; y_fat&#91;i&#93;
rᵢ &#61; dot&#40;xᵢ, β̃ᵢ&#41; - yᵢ; # this the i-th LOO residual</code></pre>
<p>Now let&#39;s show we can recover this using the previous point:</p>
<pre><code class=language-julia >K &#61; Symmetric&#40;X_fat * X_fat&#39;&#41;
F &#61; svd&#40;K&#41;
U, Ut, S &#61; F.V, F.Vt, F.S
Σ² &#61; S
Σ  &#61; sqrt.&#40;S&#41;

w  &#61; &#40;Ut * &#40;K * y_fat&#41;&#41; ./ Σ
gᵢ &#61; &#40;Ut * &#40;X_fat * xᵢ&#41;&#41; ./ Σ
g̃ᵢ &#61; gᵢ ./ &#40;Σ² .&#43; λ&#41;

rᵢ ≈ &#40;dot&#40;g̃ᵢ, w&#41; - yᵢ&#41; / &#40;1 - dot&#40;g̃ᵢ, gᵢ&#41;&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>Now if we change \(i\) and \(λ\),</p>
<pre><code class=language-julia >i &#61; 7
λ &#61; 5.5

X̃ᵢ &#61; X_fat&#91;Not&#40;i&#41;,:&#93;
ỹᵢ &#61; y_fat&#91;Not&#40;i&#41;,:&#93;
β̃ᵢ &#61; basic_ridge_solve&#40;X̃ᵢ, ỹᵢ, λ&#41;

xᵢ &#61; vec&#40;X_fat&#91;i,:&#93;&#41;
yᵢ &#61; y_fat&#91;i&#93;
rᵢ &#61; dot&#40;xᵢ, β̃ᵢ&#41; - yᵢ;</code></pre>
<p>we only have to recompute <code>gᵢ</code> and <code>g̃ᵢ</code>:</p>
<pre><code class=language-julia >gᵢ &#61; &#40;Ut * &#40;X_fat * xᵢ&#41;&#41; ./ Σ
g̃ᵢ &#61; gᵢ ./ &#40;Σ² .&#43; λ&#41;

rᵢ ≈ &#40;dot&#40;g̃ᵢ, w&#41; - yᵢ&#41; / &#40;1 - dot&#40;g̃ᵢ, gᵢ&#41;&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<h2 id=conclusion__2 ><a href="#conclusion__2" class=header-anchor >Conclusion</a></h2>
<p>In short, provided that computing the SVD of either a \(p \times p\) or \(n\times n\) matrix is manageable, it&#39;s not much more expensive to compute a single Ridge solution than to compute a bunch of solutions or to compute all the LOO errors for a number of different \(λ\).</p>
<p>This LOO trick can be generalised to an extent to all leave-some-out schemes; this is covered in the <a href="/pub/csml/glr/ridgecv-2.html">second part</a>.</p>
<h2 id=references ><a href="#references" class=header-anchor >References</a></h2>
<ol>
<li><p><a id=rifkin07  class=anchor ></a><strong>Rifkin</strong> and <strong>Lippert</strong>, <a href=" http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf">Notes on  Regularized Least Squares</a>, 2007. – Detailed development of a way to get the LOO error in &#40;Kernel&#41; Ridge efficiently, note that their development is not the same than the one presented here but the end result is comparable. Their formula is the one implemented in Sklearn&#39;s RidgeCV.</p>

<li><p><a id=golub78  class=anchor ></a><strong>Golub</strong>, <strong>Heath</strong> and <strong>Wahba</strong>, <a href="http://w3.atomki.hu/~efo/hornyak/Tikhonov_references/Technometrics_Golub_Heath_Wahba">Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter</a>, 1978. – Introduction of the GCV parameter, appropriate for selecting \(λ\) in the tall case.</p>

</ol>

  </div> 
</main> 

<div class='py-3'></div>

<footer class="footer mt-auto py-3 bg-light">
	<div class=container >
		<span class=text-muted >
			&copy; Thibaut Lienart, 2021.
			Website made with <a href="https://franklinjl.org">Franklin.jl</a>.
		</span>
	</div>
</footer>


	<script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>



	<script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

	<script>
		(function(){

			// Get the elements.
			// - the 'pre' element.
			// - the 'div' with the 'paste-content' id.

			var pre = document.getElementsByTagName('pre');

			// Add a copy button in the 'pre' element.
			// which only has the className of 'language-'.

			for (var i = 0; i < pre.length; i++) {
				var cName = pre[i].children[0].className
				if ( cName.includes('language-') || cName.includes('hljs') && !cName.includes('plaintext') ) {
					var button           = document.createElement('button');
					button.className = 'copy-button';
					button.textContent = 'Copy';

					pre[i].appendChild(button);
				}
			};

			// Run Clipboard

			var copyCode = new Clipboard('.copy-button', {
				target: function(trigger) {
					return trigger.previousElementSibling;
				}
			});

			// On success:
			// - Change the "Copy" text to "Copied".
			// - Swap it to "Copy" in 2s.
			// - Lead user to the "contenteditable" area with Velocity scroll.

			copyCode.on('success', function(event) {
				event.clearSelection();
				event.trigger.textContent = 'Copied';
				window.setTimeout(function() {
					event.trigger.textContent = 'Copy';
				}, 2000);

			});

			// On error (Safari):
			// - Change the  "Press Ctrl+C to copy"
			// - Swap it to "Copy" in 2s.

			copyCode.on('error', function(event) {
				event.trigger.textContent = 'Press "Ctrl + C" to copy';
				window.setTimeout(function() {
					event.trigger.textContent = 'Copy';
				}, 5000);
			});

		})();
	</script>






  
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>




    <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>