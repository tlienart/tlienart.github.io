<!doctype html>
<html lang="en-UK">
	<head>
		<meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="/css/main.css">
        <link rel="stylesheet" href="/css/header.css">
        <link rel="icon" href="/assets/infra/favicon.png">
		 <title>Convex Optimisation I</title>	
		 <!-- Un-minified script so that can play a bit -->
<link rel="stylesheet" href="/libs/katex/katex.css"></link>
<script src="/libs/katex/katex.js"></script>
<script src="/libs/katex/auto-render.js"></script>
 		
        
	</head>
    <body>
        <header>
            <div class="name">Thibaut Lienart</div>
            <nav>
                <ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/pub/csml.html">CS/ML notes</a></li>
                    <li><a href="/pub/julia.html">Julia notes</a></li>
                    <li><a href="/pub/misc.html">Misc.</a></li>
                </ul>
                <img src="/assets/infra/hamburger.svg" id="menu-icon">
            </nav>
        </header>

<!-- CONTENT IS APPENDED HERE -->

<!--
[ ~~~ ][ ~~~ ]
-->
<div class=content>
<p> </p>
<h1>Introduction</h1>
<h2>Prerequisites </h2>
<p>In these notes I will assume that you&#39;re comfortable with the following notions at the level of an undergrad in a quantitative topic:</p>
<ul>
<li><p>interior and boundary of a set,</p>
</li>
<li><p><em>convex sets</em> and <em>convex functions</em>,</p>
</li>
<li><p>basic linear algebra and in particular things like the inner product &#40;which we&#39;ll write \(\left\langlex, y\right\rangle\)&#41; and norms,</p>
</li>
<li><p>basic calculus and in particular things like differentiability and the gradient.</p>
</li>
</ul>
<p>For each of these there is a plethora of excellent material online.</p>
<h2>Context </h2>
<p>We&#39;ll consider the standard <em>constrained minimisation problem</em> in convex optimisation:</p>
<p><a name="11126617841431966942"></a>$$
	\min_{x\in C}\quad f(x) 
$$</p>
<p>where \(C\) is a non-empty convex subset of \(\mathbb R^n\) and \(f\) a <em>nice</em> convex function. The following notions of <em>nice</em> are used &#40;unless explicitly specified&#41;:</p>
<ul>
<li><p>\(\mathrm{dom}\, f \supseteq C\), i.e.: the domain of \(f\) covers \(C\),</p>
</li>
<li><p>\(f\in\Gamma_0(C)\), the set of convex functions that are <em>proper</em> and <em>lower semi-continuous</em> on \(C\) &#40;see <a href="/pub/csml/cvxopt/ca1.html">further</a>&#41;,</p>
</li>
<li><p>\(f\) achieves its minimum on the interior of \(C\) denoted \(C^\circ\), i.e.: there is a \(x^\sharp\in C\) such that \(f(z)\ge f(x^\sharp)\) for all \(z\in C\).</p>
</li>
</ul>
<p></p>
<p>Roughly speaking, these conditions guarantee that there is a solution to the problem and that we can find one applying some simple iterative algorithm. We will come back to these assumptions as we get to use them throughout the notes.</p>
<p>We will also consider the unconstrained form of the problem, i.e.: when \(C=\mathbb R^n\) &#40;and will then just write \(\min_x f(x)\)&#41;. Constrained problems can always be interpreted as unconstrained problems: indeed, if we define the <em>indicator</em> of a convex set \(C\) as</p>
<p>$$\begin{array}{c}
	i_C(x) &:=& \begin{cases} 0 & (x\in C) \\\\ +\infty & (x\notin C) \end{cases}
\end{array}$$</p>
<p>then the constrained problem <span class="eqref"><a href="#11126617841431966942">(1)</a></span> is equivalent to</p>
<p>$$
	\min_{x\in \mathbb R^n} \quad f(x)+i_C(x).
$$</p>
<p>This is not entirely pointless as will become apparent when deriving the projected gradient descent.</p>
<h2>Iterative methods </h2>
<p>Before delving into the details it&#39;s useful to understand how algorithms for optimisation can often be constructed.</p>
<p>A big part of convex optimisation aims at defining clever <em>iterative algorithms</em> which, ideally, enjoy the following properties when started from a sensible initial point \(x_0\):</p>
<ul>
<li><p>the iterations converge &quot;quickly&quot; to a minimiser,</p>
</li>
<li><p>the iterations are &quot;cheap&quot; to compute.</p>
</li>
</ul>
<p>Often these iterative algorithms can be derived from some kind of <em>fixed point equation</em> that is satisfied by a minimiser \(x^\sharp\), i.e.: an equation of the form</p>
<p>$$\begin{array}{c}
	x^\sharp &=& P(x^\sharp)
\end{array}$$</p>
<p>where \(P\) is an appropriate operator. Provided we have such a fixed point equation, we can consider a <em>fixed point algorithm</em> with the simplest form being:</p>
<p>$$\begin{array}{c}
	x_{k+1} &=& P(x_k).
\end{array}$$</p>
<p>Under some conditions on the operator \(P\) and possibly on \(x_0\), such an  algorithm will provably converge to \(x^\sharp\). This may seem reasonably straightforward but there are quite a few difficult questions that need be addressed and will be investigated in the notes:</p>
<ul>
<li><p>how can we get a decent starting point? &#40;quite a hard problem in general&#41;</p>
</li>
<li><p>how can we pick an operator \(P\) that is numerically stable and converges quickly?</p>
</li>
<li><p>how can we offer guarantees of how close we are to the true minimiser at a step \(k\)e rest of these notes, we will show how to obtain the fixed point equations and useful fixed point algorithms for a variety of scenarios and, by doing so, will recover well known algorithms such as the classical gradient descent and the mirror descent.</p>
</li>
</ul>
<h2>General references</h2>
<p>More precise pointers will be given on subsequent pages but most of the content in these notes relates in some way or another to the following references &#40;in particular the first one&#41;:</p>
<ol>
<li><p><strong>Rockafellar</strong>: <a href="http://press.princeton.edu/titles/1815.html">Convex analysis</a>, 1970. A &quot;must-read&quot; in convex analysis, a reference of choice for technical details.</p>
</li>
<li><p><strong>Nesterov</strong>: <a href="https://www.springer.com/us/book/9781402075537">Introductory Lectures on Convex Optimization</a>, 1998. Another must-read with all the technical details and more focused on algorithms and convergence rates than Rockafellar&#39;s book.</p>
</li>
<li><p><strong>Ben-Tal</strong>, <strong>Nemirovski</strong>, <a href="https://www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf">Lectures on Modern Optimization</a>, 2013. A recent perspective on optimisation methods. </p>
</li>
<li><p><strong>Boyd</strong> and <strong>Vandenberghe</strong>: <a href="https://stanford.edu/~boyd/cvxbook/">Convex Optimization</a>, 2004. More accessible and more oriented towards giving &quot;tools&quot; to the reader than the previous references.</p>
</li>
</ol>
<div class="page-foot">
		<div class="copyright">
				&copy; T. Lienart. Last modified: September 24, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
		</div>
</div>
</div>
<!-- CONTENT ENDS HERE -->
        
                <script>
		renderMathInElement(document.body)
</script>

        
    </body>
</html>
