<!doctype html>
<html lang="en-UK">
	<head>
		<meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
		
			<link rel="stylesheet" href="/css/main.css">
	        <link rel="icon" href="/assets/infra/favicon.png">
		
		 <title>RKHS Embeddings</title>	
		 <!-- Un-minified script so that can play a bit -->
<link rel="stylesheet" href="/libs/katex/katex.min.css"></link>
<script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
 		
        
	</head>
    <body>
		
        <header>
            <div class="name">Thibaut Lienart</div>
            <nav>
                <ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/pub/csml.html">CS/ML notes</a></li>
                    <li><a href="/pub/julia.html">Julia notes</a></li>
                    <li><a href="/pub/misc.html">Misc.</a></li>
                </ul>
                <img src="/assets/infra/hamburger.svg" id="menu-icon">
            </nav>
        </header>
		

<!-- Content appended here -->
<div class=content>
<p></p>
<h2>Introduction</h2>
<p>These notes were prepared with help from <a href="http://www.stats.ox.ac.uk/~sejdinov/">Dino Sejdinovic</a> for a talk to a <em>kernel methods reading group</em> given in Oxford in July 2015, they&#39;re mostly based on <span class="bibref"><a href="#3244370221621754717">Song et al. (2013)</a></span>.</p>
<h3>Basic definitions</h3>
<p><div class="colbox-blue">A Hilbert space \(\mathcal H\) of functions \(f:\mathcal X\mapsto \mathbb R\) defined on a non-empty set \(\mathcal X\) is said to be a <em>reproducing kernel Hilbert space</em> &#40;RKHS&#41; if <em>evaluation functionals</em> \(\delta_x: f\mapsto f(x)\) are continuous for all \(x\in\mathcal X\)</div>
</p>
<p>If we consider a RKHS then, by Riesz&#39;s representation theorem, since \(\delta_x\) is a continuous functional, it has a <em>representer</em> in \(\mathcal H\) that we can denote \(k_{_X}\) such that</p>
<p>$$\begin{array}{c}   \left\langle f, k_{_X}\right\rangle_{\mathcal H} &=& \delta_x(f) \quad\!\! =\quad\!\! f(x). \end{array}$$</p>
<p>We can then define a &#40;positive-definite&#41; bilinear form \(k:\mathcal X\times\mathcal X \to \mathbb R\) as \(k(x, x'):=\left\langle k_{_X}, k_{x'}\right\rangle_{\mathcal H}\). This is known as the <strong>reproducing kernel</strong> of \(\mathcal H\); we will also write \(k_{_X} = k(\cdot, x)\).</p>
<p>There&#39;s then an important theorem that links the two &#40;and that we won&#39;t prove&#41; reproduced in a simplified form below:</p>
<p><div class="colbox-blue">&#40;<strong>Moore-Aronszajn</strong>&#41; Every positive-definite bilinear form \(k\) is a reproducing kernel for some Hilbert space \(\mathcal H_k\).</div>
</p>
<p>When the kernel is clear from the context, we will simply write \(\mathcal H\) for the RKHS and \(k\) for its reproducing kernel.</p>
<h2>Kernel embedding of a distribution</h2>
<h3>Mean embedding</h3>
<p>A classical way to try to represent points in a given space \(\mathcal X\) is to <em>embed</em> them in \(\mathbb R^s\) using a \(s\)-dimensional <em>feature map</em> \(\Phi:\mathcal X\to \mathbb R^s\) with $$\begin{array}{c}    x\quad\!\!\mapsto\quad\!\!\Phi(x)\,\,=\,\, (\varphi_1(x), \varphi_2(x), \dots, \varphi_s(x)). \end{array}$$ Instead, we can now consider embedding points in a RKHS with the infinite dimensional feature map \(x\mapsto k_{_X}\). In that case, we have an easily computable inner product between points with</p>
<p>$$\begin{array}{c}    \left\langle k_{_X}, k_{_Y}\right\rangle_{\mathcal H} &=& \left\langle k(\cdot, x), k(\cdot, y)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! k(x, y).       \end{array}$$</p>
<p>Recall that an inner product is a <em>measure of alignment</em> so that this automatically gives us a measure of similarity between points through this kernel.</p>
<p>When the embedding is <em>injective</em> &#40;i.e.: different objects are mapped to different points in the RKHS&#41;, the corresponding kernel is said to be <strong>characteristic</strong> &#40;this is often the case for standard kernels&#41;.</p>
<p>An example of objects we can embed in an RKHS are distributions. Each distribution is then considered as a point which we can embed through the <em>mean-embedding</em>.</p>
<p><div class="colbox-blue">Let \(P\) denote a distribution among a set of distributions, the <strong>mean embedding</strong> is defined as follows:</p>
<p>$$\begin{array}{c}   P \mapsto \mu_X(P, k) \quad\!\!:=\quad\!\! \mathbb E_{X\sim P}[k(\cdot, X)]\quad\!\! =\quad\!\! \mathbb E_X[k_{_X}],\end{array}$$ and, naturally, \(\mu_X(P, k)\in\mathcal H\).</div>
</p>
<p>When the kernel and the distribution are clear from the context, we will simply write \(\mu_X\). As before, note that we inherit a notion of <em>similarity</em> between probability measures by looking at the inner product on the RKHS: $$\begin{array}{c} 
    \left\langle \mu_X(P, k), \mu_Y(Q, k)\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \mathbb E_{X, Y}[k(X, Y)],
\end{array}$$ and this can easily be estimated if we have samples from both \(P\) and \(Q\).</p>
<p>Note finally that \(\mu_X\) represents <em>expectations with respect to \(P\)</em> i.e.: for any \(f\in\mathcal H\), $$\begin{array}{c} 
    \left\langle f, \mu_X\right\rangle_{\mathcal H} &=& \mathbb E_X[{\left\langle f, k_{_X}\right\rangle_{\mathcal H}}] \quad\!\! =\quad\!\! \mathbb E_X[f(X)] .
\end{array}$$</p>
<h3>Joint embedding</h3>
<p>The generalisation to joint distributions is straightforward using tensor product feature spaces.</p>
<p><div class="colbox-blue">Let \(X, Y\) be jointly distributed according to some distribution \(P\), the <strong>joint embedding</strong> of \(P\) is defined as $$\begin{array}{c} 
    P \mapsto \mathcal C_{XY}(P) \quad\!\! :=\quad\!\! \mathbb E_{XY}[k_{_X}\otimes k_{_Y}],
\end{array}$$ assuming that the two variables share the same kernel.</div>
</p>
<p>The tensor product satisfies \(\left\langle k_{_X} \otimes k_{_Y}, k_{x'} \otimes k_{y'}\right\rangle_{\mathcal H\otimes \mathcal H} = k(x, x')k(y, y')\).</p>
<p>In the same way that \(\mu_X\) represents the expectation operator, the joint-embedding \(\mathcal C_{XY}\) can be viewed as the <em>uncentered cross-covariance operator</em>: for any two functions \(f, g \in \mathcal H\), their covariance is given by</p>
<p>$$\begin{array}{c} \mathbb E_{XY}[f(X)f(Y)] &=& \left\langle f\otimes g, \mathcal C_{XY}\right\rangle_{\mathcal H\otimes \mathcal H} \quad\!\! =\quad\!\! \left\langle f, \mathcal C_{XY} g\right\rangle_{\mathcal H}\end{array}$$</p>
<p>&#40;still assuming both random variables share the same kernel&#41;. Following the same reasoning, we can define the <em>auto-covariance operators</em> \(\mathcal C_{XX}\) and \(\mathcal C_{YY}\). Note that, in the same way that \(\mu_X\) represents expectations with respect to \(P\) &#40;the distribution of \(X\)&#41;, these operatros represent cross-covariance/auto-covariance with respect to \(P\).</p>
<p><strong>Remark</strong>: we have assumed that both random variables share the same kernel but this need not be the case: we can consider a second kernel \(k'\) with a RKHS \(\mathcal H'\); the cross-covariance operator then belongs to the product space \(\mathcal H\otimes \mathcal H'\) &#40;which is also a RKHS&#41;.</p>
<h3>MMD and HSIC</h3>
<p>When considering a characteristic kernel &#40;such as, for example, the Gaussian RBF with \(k(x, x')=\exp(-\sigma\|x-x'\|^2_2)\)&#41;, the RKHS embedding is injective. In that case, we can use the distance in the RKHS as a proxy for similarity in the distribution space. This can be used in the two-sample test or when testing for independence.</p>
<p>In the <em>two sample test</em>, the test statistic is the squared distance between the embeddings of the two distributions:</p>
<p><div class="colbox-blue">The kernel <strong>Maximum Mean Discrepancy</strong> &#40;<strong>MMD</strong>&#41; measure is defined for two distributions \(P\) and \(Q\) by $$\begin{array}{c} \mathrm{MMD}(P, Q) &:=& \|\mu_X - \mu_Y\|_{\mathcal H}^2, \end{array}$$  where \(X\sim P\) and \(Y\sim Q\).</div>
</p>
<p>When testing for <em>independence</em>, the test statistic is the squared distance between the embeddings of the joint distribution and the product of its marginals:</p>
<p><div class="colbox-blue">The <strong>Hilbert-Schmidt Independence Criterion</strong> &#40;<strong>HSIC</strong>&#41; is defined for two distributions \(P\) and \(Q\) by $$\begin{array}{c} \mathrm{HSIC}(P, Q) &:=& \|\mathcal C_{XY} - \mu_X \otimes \mu_Y\|_{\mathcal H}^2, \end{array}$$ where \(X\sim P\) and \(Y\sim Q\).</div>
</p>
<h3>Finite sample embeddings</h3>
<h2>Kernel embeddings of conditional distributions</h2>
<h3>Pointwise definition</h3>
<h3>Conditional operator</h3>
<h3>Finite sample kernel estimator</h3>
<h2>Probabilistic reasoning with kernel embeddings</h2>
<p>Following notations in <span class="bibref"><a href="#3244370221621754717">Song et al. (2013)</a></span>, we still consider two random variables \(X\) and \(Y\) with joint distribution \(P(X, Y)\) and, additionally, we consider a prior distribution \(\pi\) on \(Y\).</p>
<h3>Kernel sum rule</h3>
<p>The marginal distribution of \(X\) can be computed by integrating out \(Y\) from the joint density, i.e.:</p>
<p>$$ Q(X) \quad\!\! =\quad\!\! \int P(X|Y) \mathrm d{\pi}(Y) \quad\!\! =\quad\!\! \mathbb E_{Y\sim\pi}[P(X|Y)]. $$</p>
<p>Embedding it, we have</p>
<p>$$ \mu_{_X}^\pi \quad\!\! :=\quad\!\! \mathbb E_{X\sim Q}[k_{_X}] \quad\!\! =\quad\!\! \mathbb E_{Y\sim \pi}[\mathbb E_{X|Y}[k_{_X}]], $$</p>
<p>which leads to the kernel sum rule quoted below.</p>
<p><div class="colbox-blue">Let \(X\) and \(Y\) denote two random variables and \(\pi\) a prior on \(Y\), then the <strong>Kernel sum rule</strong> reads $$ \mu_{_X}^\pi \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mu^\pi_{_Y} $$</div>
</p>
<p>This is straightforward to prove using the definition of the conditional embedding.</p>
<p>$$\begin{aligned}  \mu_{_X}^\pi \quad\!\! =\quad\!\! \mathbb E_{X|Y}[k_{_Y}] \quad\!\!&=\quad\!\! \mathbb E_{Y\sim \pi}[\mathcal C_{X|Y}k_{_Y}]\\
    &=\quad\!\! \mathcal C_{X|Y}\mathbb E_{Y\sim\pi}[k_{_Y}] \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mu_{_Y}
\end{aligned}$$</p>
<p>The kernel sum rule shows that the conditional embedding operator \(\mathcal C_{X|Y}\) maps the embedding of \(\pi(Y)\) to that of \(Q(X)\).</p>
<p>In practice, an estimator \(\hat\mu_{_Y}^\pi\) is given in the form \(\sum_{i=1}^n \alpha_i k_{\tilde y_i} = \widetilde\Phi \alpha\) based on samples \(\{\tilde y_i\}_{i=1}^n\). Let&#39;s also assume that the conditional embedding operator has been estimated from a sample \(\{(x_i,y_i)\}_{i=1}^m\) drawn from the joint distribution with \(\widehat\mathcal C_{X|Y}=\Upsilon(G+\lambda I)^{-1}\Phi\) where \(\Upsilon = (k_{x_i})_{i=1}^m\), \(\Phi=(k_{y_i})_{i=1}^m\), \(G_{ij} = k(y_i,y_j)\) and \(\widetilde G_{ij} = k(y_i, \tilde y_j)\).</p>
<p><div class="colbox-blue">The kernel sum rule in the finite sample case has the following form: $$ \hat\mu_{_X}^\pi \quad\!\! =\quad\!\! \widehat\mathcal C_{X|Y}\hat\mu_{_Y}^\pi \quad\!\! =\quad\!\! \Upsilon(G+\lambda I)^{-1}\Phi \widetilde G\alpha. $$</div>
</p>
<h3>Kernel chain rule</h3>
<p>A joint distribution \(Q\) can be factorised into a product between conditional and marginal with \(Q(X, Y)=P(X|Y)\pi(Y)\).</p>
<p><div class="colbox-blue">The <strong>kernel chain rule</strong> reads $$ \mathcal C^\pi_{XY} \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mathcal C^\pi_{YY}. $$</div>
</p>
<p>This is straightforward to prove:</p>
<p>$$\begin{aligned} 
    \mathcal C^\pi_{XY} \quad\!\!&=\quad\!\! \mathbb E_{(X,Y)\sim Q}[k_{_X}\otimes k_{_Y}] \quad\!\! =\quad\!\! \mathbb E_{Y\sim\pi}[\mathbb E_{X|Y}[k_{_X}] \otimes k_{_Y}]\\
    &= \quad\!\! \mathcal C_{X|Y}\mathbb E_{Y\sim \pi}[k_Y \otimes k_Y] \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mathcal C^\pi_{YY}.
\end{aligned}$$</p>
<p><div class="colbox-blue">The kernel chain rule in the finite sample case has the following form: $$ \widehat{\mathcal C}_{XY}^\pi \quad\!\! =\quad\!\! \widehat{\mathcal C}_{X|Y}\widehat{\mathcal C}^\pi_{YY} \quad\!\! =\quad\!\! \Upsilon(G+\lambda I)^{-1}\widetilde G\mathrm{diag}(\alpha)\widetilde\Phi^t,$$ using \(\widehat{\mathcal C}^\pi_{YY} = \widetilde\Phi \mathrm{diag}(\alpha)\widetilde\Phi^t\) and \(\widehat{\mathcal C}_{X|Y} = \Upsilon(G+\lambda I)^{-1}\Phi\).</div>
</p>
<h3>Kernel Bayes rule</h3>
<p>A posterior distribution can be expressed in terms of a prior and a likelihood as</p>
<p>$$ Q(Y|x) \quad\!\! =\quad\!\! {P(x|Y)\pi(Y)\over Q(x)}, $$</p>
<p>where \(Q(x)\) is the relevant normalisation factor. We seek to construct the conditional embedding operator \(\mathcal C^\pi_{Y|X}\).</p>
<p><div class="colbox-blue">The <strong>kernel Bayes rule</strong> reads</p>
<p>$$ \mu^\pi_{_Y|x} \quad\!\! =\quad\!\! \mathcal C^\pi_{Y|X}k_x \quad\!\! =\quad\!\! \mathcal C^\pi_{YX} (\mathcal C^\pi_{XX})^{-1}k_x, $$ with then \(\mathcal C^\pi_{Y|X} = \mathcal C^\pi_{YX}(\mathcal C^\pi_{XX})^{-1}\).</div>
</p>
<p>Using the sum rule, \(\mathcal C^\pi_{XX}=\mathcal C_{(XX)|Y}\mu_{_Y}^\pi\) and, using the chain rule, \(\mathcal C^\pi_{YX}=(\mathcal C_{X|Y}\mathcal C^\pi_{YY})^t\). The finite sample case can also be obtained &#40;and is a bit messy&#41;.</p>
<h3>Kernel Bayesian average and posterior decoding</h3>
<p>Say we&#39;re interested in evaluating the expected value of a function \(g\in \mathcal H\) with respect to the posterior \(Q(Y|x)\) or to decode \(y^{\star}\) most typical of the posterior. Assume that the embedding \(\widehat\mu^{\pi}_{_{Y|x}}\) is given as \(\sum_{i=1}^n \beta_{i}(x)k_{\tilde y_{i}}\) and \(g=\sum_{i=1}^m\alpha_{i}k_{y_{i}}\) then</p>
<p><div class="colbox-blue">the <strong>kernel Bayes average</strong> reads</p>
<p>$$
    \left\langle g,\widehat\mu_{Y|x}^{\pi}\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \beta^{t} \widetilde G  \alpha \quad\!\! =\quad\!\! \sum_{ij} \alpha_{i}\beta_{j}(x)k(y_{i},\tilde y_{j}),
$$</p>
<p>and the <strong>kernel Bayes posterior decoding</strong> reads</p>
<p>$$
y^{\star} \quad\!\! =\quad\!\! \arg\min_{y} \,\, -2\beta^{t}\widetilde G_{:y}+k(y,y).
$$</p>
<p>The second expression coming from the minimisation \(\min_{y}\|\widehat \mu^{\pi}_{_{Y|x}}-k_{y}\|_{\mathcal H}^{2}\).</div>
  In general, the optimisation problem is difficult to solve. It corresponds to the so-called &quot;pre-image&quot; problem in kernel methods.</p>
<h2>References</h2>
<ul>
<li><p><a name="10410378134158183994"></a> <strong>Fukumizu</strong>, <strong>Bach</strong> and <strong>Jordan</strong>, <a href="http://www.jmlr.org/papers/volume5/fukumizu04a/fukumizu04a.ps">Dimensionality Reduction for Supervised Learning with Reproducing Kernel Hilbert Spaces</a>, 2004. A key paper in the RKHS literature. </p>
</li>
<li><p><a name="3244370221621754717"></a> <strong>Song</strong>, <strong>Fukumizu</strong> and <strong>Gretton</strong>,  <a href="https://www.cc.gatech.edu/~lsong/papers/SonFukGre13.pdf">Kernel embeddings of conditional distributions</a>, 2013. The paper these notes are mainly based on.</p>
</li>
</ul>
<div class="page-foot">
		<div class="copyright">
				&copy; T. Lienart. Last modified: October 16, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
		</div>
</div>
</div>
<!-- CONTENT ENDS HERE -->
        
                <script>
		renderMathInElement(document.body)
</script>

        
    </body>
</html>
