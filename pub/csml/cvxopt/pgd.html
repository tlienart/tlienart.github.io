<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>Projected gradient descent</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=name >Thibaut Lienart</div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h1>Projected gradient descent</h1> <p>Here we will show a general method to approach a constrained minimisation problem of a convex, differentiable function \(f\) over a closed convex set \(C\subset \mathbb R^n\). Such problems can be written in an unconstrained form as we discussed in the <a href="/pub/csml/cvxopt/intro.html">introduction</a></p> $$\begin{array}{c} \min_x\quad\!\! f(x) + i_C(x),\end{array}$$ <p>where \(i_C\) is the indicator function associated with the set.</p> <p>To get to the method, we will need three main parts:</p> <ol> <li><p>discuss the first order condition &#40;FOC&#41; of the problem which will bring us to the notion of <em>normal cone</em>,</p> <li><p>discuss how the <em>normal cone</em> relates to the <em>Euclidean projection</em>,</p> <li><p>introduce the <em>projected gradient descent</em> algorithm.</p> </ol> <h2>First order condition</h2> <p>The FOC indicates that \(x^\sharp \in C\) is a minimiser if and only if \(0\in \partial(f + i_C)(x^\sharp)\). However, remember that the subdifferential of a sum contains the sum of subdifferentials &#40;see <a href="/pub/csml/cvxopt/ca2.html">convex analysis pt. 2</a>&#41; so that if \(0 \in \nabla f(x^\sharp) + \partial i_C(x^\sharp)\) then \(x^\sharp\) is a minimiser.</p> <p>If \(y\) is a subgradient of \(i_C\) at a point \(x\in C\) then, by definition,</p> $$\begin{array}{c} i_C(z) &\ge& i_C(x) + \left\langle z-x, y\right\rangle, \quad\forall z. \end{array}$$ <p>For any \(z\notin C\), this inequality is trivially verified since the left hand side is infinite. Letting \(z\in C\) and since \(x\in C\) so that \(i_C(x)=i_C(z)=0\), we are left with</p> $$\begin{array}{c} 0 &\ge& \left\langle z-x, y\right\rangle, \quad\forall z\in C. \end{array}$$ <p>This inequality defines the subdifferential \(\partial i_C\) but also happens to be the definition of the <strong>normal cone</strong> to the convex set \(C\) at \(x\) denoted \(N_C(x)\). In short: $$\begin{array}{c} \partial i_C(x) &=& \{y\in \mathbb R^n \,\mid\, \left\langle z-x, y\right\rangle\le 0, \forall z\in C\}\quad\!\! =\quad\!\! N_C(x).\end{array}$$ Bringing the pieces together, we have <a name=7706369853534962573 ></a>$$ 0 \,\in\, \nabla f(x^\sharp) + N_C(x^\sharp) \quad\!\! \Longrightarrow\quad\!\! x^\sharp \,\in\,\arg\min_{x\in C}\, f(x). $$ <p>Of course this doesn&#39;t really help much because at this point we don&#39;t know how to find a \(x^\sharp\) such that the left-hand-side holds. This is what the next section will explore.</p> <h2>Normal cone and Euclidean projection</h2> <p>A useful &#40;and obvious&#41; property of the normal cone which we shall use in the sequel is that it is invariant to a non-negative scalar multiplication, i.e.:</p> <a name=8290924001926339138 ></a>$$\begin{array}{c} \mathrm{if}\,\, u\in N_C(x)\,\,\mathrm{then}\,\, \alpha u\in N_C(x), \,\forall \alpha \ge 0. \end{array}$$ <p>We will now show the connection between the normal cone and the Euclidean projection on \(C\). Remember that the Euclidean projection onto \(C\) &#40;denoted \(\pi_C\)&#41; is defined as follows for any \(z\in \mathbb R^n\):</p> <a name=8960741556087457471 ></a>$$ \pi_C(z) \quad\!\! =\quad\!\! \arg\min_{x\in C}\,\, \frac12\|x-z\|_2^2 $$ <p>Note that in the <em>unconstrained problem</em>, \(C=\mathbb R^n\) and hence \(\pi_C=\mathbb I\), the identity operator. Note also that the factor \(1/2\) is here for aesthetics when computing the gradient which is just \((x-z)\).</p> <p>Considering the condition <span class="eqref)">(<a href="#7706369853534962573">5</a>)</span> for the optimisation problem <span class="eqref)">(<a href="#8960741556087457471">7</a>)</span>, we have that if</p> $$\begin{array}{c} 0\,\in\, (x^\sharp-z) + N_C(x^\sharp) \end{array}$$ <p>then \(x^\sharp\) is a minimiser with thus \(x^\sharp = \pi_C(z)\). This is equivalent to requiring \(z \in x^\sharp + N_C(x^\sharp)\) or, equivalently, \(z\in (\mathbb I + N_C)(x^\sharp)\). Replacing \(x^\sharp\) by \(\pi_C(z)\) and re-arranging terms gives</p> $$\begin{array}{c} \pi_C(z) &=& (\mathbb I + N_C)^{-1}(z). \end{array}$$ <p>This is a classical and important relationship.</p> <div class=colbox-blue >Let \(C\) denote a convex subset of \(\mathbb R^n\) then <a name=16186488855891449143 ></a>$$\begin{array}{c} \pi_C &\equiv& (\mathbb I+ N_C)^{-1}. \end{array}$$</div> <h2>Projected gradient descent</h2> <p>We indicated at <span class="eqref)">(<a href="#8290924001926339138">6</a>)</span> that for any \(\alpha \ge 0\) and \(u\in N_C(x)\), then \(\alpha u\in N_C(x)\). We can rewrite this as:</p> <a name=9033558848688047001 ></a>$$\begin{array}{c} (x+\alpha u) - x &\in& N_C(x), \end{array}$$ <p>which may seem pointless but will lead us to a fixed-point algorithm &#40;remember from <a href="/pub/csml/cvxopt/intro.html">the introduction</a> that many algorithms for convex optimisation can be expressed in terms of fixed point algorithms&#41;.</p> <p>Let \(z:=(x+\alpha u)\) so that <span class="eqref)">(<a href="#9033558848688047001">11</a>)</span> can be rearranged to \(z\in (\mathbb I+N_C)(x)\). By <span class="eqref)">(<a href="#16186488855891449143">10</a>)</span>, this is equivalent to \(x=\pi_C(z)\) or</p> <a name=3271864073650078948 ></a>$$\begin{array}{c} x &=& \pi_C(x+\alpha u), \quad (\alpha \ge 0, \,u\in N_C(x)). \end{array}$$ <p>To finish up, let&#39;s go back once more to the FOC <span class="eqref)">(<a href="#7706369853534962573">5</a>)</span> which indicates that if \(x^\sharp\) is such that \(-\nabla f(x^\sharp) \in N_C(x^\sharp)\) then it is a minimiser. Combined with <span class="eqref)">(<a href="#3271864073650078948">12</a>)</span>, we get the following useful fixed-point form for the minimiser of the constrained problem:</p> <div class=colbox-blue ><a name=17226506964853765554 ></a>$$ x^\sharp \quad\!\! =\quad\!\! \pi_C(x^\sharp - \alpha \nabla f(x^\sharp)). $$</div> <p><strong>Note</strong>: had we not assumed that \(f\) was differentiable on \(C\), we would still have that there exists a subgradient \(f'(x^\sharp) \in \partial f(x^\sharp)\) with \(-f'(x^\sharp)\in N_C(x^\sharp)\) and consequently the fixed-point \(x^\sharp = \pi_C(x^\sharp - \alpha f'(x^\sharp))\) leading the <em>projected subgradient method</em>. In practice however, one tries to cast a convex objective function as a sum of a smooth function with a non-smooth one, to follow a gradient descent on the smooth part and use a projection for the non-smooth part &#40;this is the case here with \(i_C\) being the non-smooth part&#41;.</p> <h3>The algorithm</h3> <p>Well there&#39;s not much left to do. Applying a fixed-point iteration to <span class="eqref)">(<a href="#17226506964853765554">13</a>)</span> from a starting point \(x_0\) generates the sequence \(\{x_0, x_1, \dots\}\) via</p> $$\begin{array}{c} x_{k+1} &=& \pi_C(x_k - \alpha_k \nabla f(x_k)) \end{array}$$ <p>where \(\alpha_k > 0\) is the <em>step size</em> &#40;ignoring the trivial case \(\alpha_k=0\)&#41;. This is the <em>projected gradient descent</em> method.</p> <p>Assuming that the \(\alpha_k\) are picked sensibly and basic regularity conditions on the problem are met, the method enjoys a convergence rate \((f(x_k)-f(x^\sharp)) = \mathcal O(k^{-1})\) &#40;see references for more&#41;.</p> <p><strong>Note</strong>: this method is pretty easy to apply <em>provided you can compute the projection</em>. Of course, there may be situations for which computing the projection is as hard as the initial problem&#33; But there are many special cases where efficient projection can be applied &#40;e.g. if \(C\) is a polyhedron, i.e. corresponds to a set of \(x\) such that \(Ax\le b\) for some matrix \(A\) and vector \(b\).&#41;</p> <h2>Additional references</h2> <ol> <li><p><strong>Burke</strong>, <a href="https://sites.math.washington.edu/~burke/crs/408/notes/nlp/gpa.pdf">The Gradient Projection Algorithm</a>, 2014. Lecture notes at the University of Washington covering the topic in a bit more depth.</p> <li><p><strong>Saunders</strong>, <a href="https://web.stanford.edu/class/msande318/notes/notes-first-order-smooth.pdf">Notes on First-Order Methods for Minimizing Smooth Functions</a>, 2017. Lectures notes at Stanford covering the topic &#40;among others&#41; and proving the linear convergence rate under regularity conditions.</p> <li><p><strong>Liu</strong>, <strong>Ye</strong>, <a href="https://icml.cc/Conferences/2009/papers/123.pdf">Efficient Euclidean Projections in Linear Time</a>, 2009. A paper describing the projection problem and how it can be done efficiently in some cases.</p> </ol> <p><em>See also the general references mentioned in the <a href="/pub/csml/cvxopt/intro.html">introduction</a>.</em></p> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 13, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>