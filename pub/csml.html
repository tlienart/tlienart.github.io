<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport content="width=device-width, initial-scale=1"> <link rel=stylesheet href="/css/main.css"> <link rel=icon href="/assets/infra/favicon.png"> <title>CS/ML</title> <link rel=stylesheet href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=name >Thibaut Lienart</div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <p></p> <h1>Computer science &amp; machine learning notes</h1> <p>The aim of the notes you&#39;ll find here is to suggest a direct, constructive path to results I find useful, beautiful, or simply interesting. This stems mainly from my experience reading papers where the presentation sometimes trades clarity for generality.</p> <p>The emphasis here is on building intuition rather than mathematical rigour though I try to indicate clearly where simplifications are made and what they may imply. Often, these simplifications allow for straightforward demonstrations. The references to more technical work can then be helpful to learn the results in full.</p> <p>The target audience is advanced undergrads or grads in quantitative fields such as applied-maths, comp-sci, etc, assuming a decent background in basic maths &#40;in particular linear algebra. real analysis and basic probability theory&#41;. When the level of the notes is judged to be close to research, it is marked by a &quot;⭒&quot; symbol.</p> <p>If you find anything dubious in the notes, please send me an email, feedback is always much appreciated.</p> <h2>Stats, ML and related topics</h2> <p></p> <ul> <li><p><strong>Kernel methods</strong></p> <ul> <li><p>&#40;⭒&#41; <a href="/pub/csml/rkhs/intro-rkhs1.html">RKHS embeddings part 1</a>: notes from a reading group kernel methods introducing <em>reproducing kernel Hilbert space</em> embeddings of distributions and their use.</p> <li><p>&#40;⭒&#41; <a href="/pub/csml/rkhs/intro-rkhs2.html">RKHS embeddings part 2</a>: where we discuss probabilistic reasoning with kernel embeddings.</p> </ul> </ul> <p></p> <p> woodbury formula, link in rkhs part 1 –&gt;</p> <h2>Applied maths</h2> <ul> <li><p><strong>notes on convex optimisation</strong> &#40;<em>assumes basic knowledge of convexity</em>&#41;: </p> <ul> <li><p><a href="/pub/csml/cvxopt/intro.html">introduction</a>: introduction of the general minimisation problem and hint at generic iterative methods. </p> <li><p><a href="/pub/csml/cvxopt/ca1.html">convex analysis part 1</a>: the <em>subdifferential</em> and the <em>first-order optimality condition</em>. </p> <li><p><a href="/pub/csml/cvxopt/ca2.html">convex analysis part 2</a>: the <em>convex conjugate</em> along with some useful properties. </p> <li><p><a href="/pub/csml/cvxopt/ca3.html">convex analysis part 3</a>: <em>strict</em> and <em>strong</em> convexity, the <em>Bregman divergence</em> and link between <em>lipschitz continuity</em> and <em>strong convexity</em>. </p> <li><p><a href="/pub/csml/cvxopt/pgd.html">projected gradient descent</a>: <em>normal cone</em>, <em>Euclidean projection</em> and <em>projected gradient descent</em>.</p> </ul> <p></p> </ul> <p> </p> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 18, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>