<!doctype html> <html lang=en-UK > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/main.css"> <link rel=icon  href="/assets/infra/favicon.png"> <title>RKHS Embeddings pt. 2</title> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <script src="/libs/katex/katex.min.js"></script> <script src="/libs/katex/auto-render.min.js"></script> <header> <div class=blog-name ><a href="/">Thibaut Lienart</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/pub/csml.html">CS/ML notes</a> <li><a href="/pub/julia.html">Julia notes</a> <li><a href="/pub/misc.html">Misc.</a> </ul> <img src="/assets/infra/hamburger.svg" id=menu-icon > </nav> </header> <div class=content> <h2>Probabilistic reasoning with kernel embeddings</h2> <p>Following notations in <span class=bibref ><a href="#9Eos">Song et al. (2013)</a></span>, we still consider two random variables \(X\) and \(Y\) with joint distribution \(P(X, Y)\) and, additionally, we consider a prior distribution \(\pi\) on \(Y\).</p> <h3>Kernel sum rule</h3> <p>The marginal distribution of \(X\) can be computed by integrating out \(Y\) from the joint density, i.e.:</p> $$ Q(X) \quad\!\! =\quad\!\! \int P(X|Y) \mathrm d{\pi}(Y) \quad\!\! =\quad\!\! \mathbb E_{Y\sim\pi}[P(X|Y)]. $$ <p>Embedding it, we have</p> $$ \mu_{_X}^\pi \quad\!\! :=\quad\!\! \mathbb E_{X\sim Q}[k_{_X}] \quad\!\! =\quad\!\! \mathbb E_{Y\sim \pi}[\mathbb E_{X|Y}[k_{_X}]], $$ <p>which leads to the kernel sum rule.</p> <div class=colbox-blue >Let \(X\) and \(Y\) denote two random variables and \(\pi\) a prior on \(Y\), then the <strong>Kernel sum rule</strong> reads $$ \mu_{_X}^\pi \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mu^\pi_{_Y} $$</div> <p>This is straightforward to prove using the definition of the conditional embedding &#40;see <a href="/pub/csml/rkhs/intro-rkhs1.html">part 1</a>&#41;.</p> $$\begin{aligned} \mu_{_X}^\pi \quad\!\! =\quad\!\! \mathbb E_{X|Y}[k_{_Y}] \quad\!\!&=\quad\!\! \mathbb E_{Y\sim \pi}[\mathcal C_{X|Y}k_{_Y}]\\ &=\quad\!\! \mathcal C_{X|Y}\mathbb E_{Y\sim\pi}[k_{_Y}] \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mu_{_Y} \end{aligned}$$ <p>The kernel sum rule shows that the conditional embedding operator \(\mathcal C_{X|Y}\) maps the embedding of \(\pi(Y)\) to that of \(Q(X)\).</p> <p>In practice, an estimator \(\hat\mu_{_Y}^\pi\) is given in the form \(\sum_{i=1}^n \alpha_i k_{\tilde y_i} = \widetilde\Phi \alpha\) based on samples \(\{\tilde y_i\}_{i=1}^n\). Let&#39;s also assume that the conditional embedding operator has been estimated from a sample \(\{(x_i,y_i)\}_{i=1}^m\) drawn from the joint distribution with \(\widehat\mathcal C_{X|Y}=\Upsilon(G+\lambda I)^{-1}\Phi\) where \(\Upsilon = (k_{x_i})_{i=1}^m\), \(\Phi=(k_{y_i})_{i=1}^m\), \(G_{ij} = k(y_i,y_j)\) and \(\widetilde G_{ij} = k(y_i, \tilde y_j)\).</p> <div class=colbox-blue >The kernel sum rule in the finite sample case has the following form: $$ \hat\mu_{_X}^\pi \quad\!\! =\quad\!\! \widehat\mathcal C_{X|Y}\hat\mu_{_Y}^\pi \quad\!\! =\quad\!\! \Upsilon(G+\lambda I)^{-1}\Phi \widetilde G\alpha. $$</div> <h3>Kernel chain rule</h3> <p>A joint distribution \(Q\) can be factorised into a product between conditional and marginal with \(Q(X, Y)=P(X|Y)\pi(Y)\).</p> <div class=colbox-blue >The <strong>kernel chain rule</strong> reads $$ \mathcal C^\pi_{XY} \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mathcal C^\pi_{YY}. $$</div> <p>This is straightforward to prove:</p> $$\begin{aligned} \mathcal C^\pi_{XY} \quad\!\!&=\quad\!\! \mathbb E_{(X,Y)\sim Q}[k_{_X}\otimes k_{_Y}] \quad\!\! =\quad\!\! \mathbb E_{Y\sim\pi}[\mathbb E_{X|Y}[k_{_X}] \otimes k_{_Y}]\\ &= \quad\!\! \mathcal C_{X|Y}\mathbb E_{Y\sim \pi}[k_Y \otimes k_Y] \quad\!\! =\quad\!\! \mathcal C_{X|Y}\mathcal C^\pi_{YY}. \end{aligned}$$ <div class=colbox-blue >The kernel chain rule in the finite sample case has the following form: $$ \widehat{\mathcal C}_{XY}^\pi \quad\!\! =\quad\!\! \widehat{\mathcal C}_{X|Y}\widehat{\mathcal C}^\pi_{YY} \quad\!\! =\quad\!\! \Upsilon(G+\lambda I)^{-1}\widetilde G\mathrm{diag}(\alpha)\widetilde\Phi^t,$$ using \(\widehat{\mathcal C}^\pi_{YY} = \widetilde\Phi \mathrm{diag}(\alpha)\widetilde\Phi^t\) and \(\widehat{\mathcal C}_{X|Y} = \Upsilon(G+\lambda I)^{-1}\Phi\).</div> <h3>Kernel Bayes rule</h3> <p>A posterior distribution can be expressed in terms of a prior and a likelihood as</p> $$ Q(Y|x) \quad\!\! =\quad\!\! {P(x|Y)\pi(Y)\over Q(x)}, $$ <p>where \(Q(x)\) is the relevant normalisation factor. We seek to construct the conditional embedding operator \(\mathcal C^\pi_{Y|X}\).</p> <div class=colbox-blue >The <strong>kernel Bayes rule</strong> reads</p> $$ \mu^\pi_{_Y|x} \quad\!\! =\quad\!\! \mathcal C^\pi_{Y|X}k_x \quad\!\! =\quad\!\! \mathcal C^\pi_{YX} (\mathcal C^\pi_{XX})^{-1}k_x, $$ with then \(\mathcal C^\pi_{Y|X} = \mathcal C^\pi_{YX}(\mathcal C^\pi_{XX})^{-1}\).</div> <p>Using the sum rule, \(\mathcal C^\pi_{XX}=\mathcal C_{(XX)|Y}\mu_{_Y}^\pi\) and, using the chain rule, \(\mathcal C^\pi_{YX}=(\mathcal C_{X|Y}\mathcal C^\pi_{YY})^t\). The finite sample case can also be obtained &#40;and is a bit messy&#41;.</p> <h3>Kernel Bayesian average and posterior decoding</h3> <p>Say we&#39;re interested in evaluating the expected value of a function \(g\in \mathcal H\) with respect to the posterior \(Q(Y|x)\) or to decode \(y^{\star}\) most typical of the posterior. Assume that the embedding \(\widehat\mu^{\pi}_{_{Y|x}}\) is given as \(\sum_{i=1}^n \beta_{i}(x)k_{\tilde y_{i}}\) and \(g=\sum_{i=1}^m\alpha_{i}k_{y_{i}}\) then</p> <div class=colbox-blue >the <strong>kernel Bayes average</strong> reads</p> $$ \left\langle g,\widehat\mu_{Y|x}^{\pi}\right\rangle_{\mathcal H} \quad\!\! =\quad\!\! \beta^{t} \widetilde G \alpha \quad\!\! =\quad\!\! \sum_{ij} \alpha_{i}\beta_{j}(x)k(y_{i},\tilde y_{j}), $$ <p>and the <strong>kernel Bayes posterior decoding</strong> reads</p> $$ y^{\star} \quad\!\! =\quad\!\! \arg\min_{y} \,\, -2\beta^{t}\widetilde G_{:y}+k(y,y). $$ <p>The second expression coming from the minimisation \(\min_{y}\|\widehat \mu^{\pi}_{_{Y|x}}-k_{y}\|_{\mathcal H}^{2}\).</div> <p>In general, the optimisation problem is difficult to solve. It corresponds to the so-called &quot;pre-image&quot; problem in kernel methods.</p> <h2>References</h2> <ol> <li><p><a name=9Eos ></a> <strong>Song</strong>, <strong>Fukumizu</strong> and <strong>Gretton</strong>, <a href="https://www.cc.gatech.edu/~lsong/papers/SonFukGre13.pdf">Kernel embeddings of conditional distributions</a>, 2013. The paper these notes are mainly based on.</p> </ol> <p><em>See also the references given in the <a href="/pub/csml/rkhs/intro-rkhs1.html">first part</a>.</em></p> <div class=page-foot > <div class=copyright > &copy; T. Lienart. Last modified: October 28, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>. </div> </div> </div> <script> renderMathInElement(document.body) </script>