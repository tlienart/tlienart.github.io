<!doctype html>
<html lang="en-UK">
	<head>
		<meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" href="/css/main.css">
        <link rel="stylesheet" href="/css/header.css">
        <link rel="icon" href="/assets/infra/favicon.png">
		 <title>Convex analysis I</title>	
		 <!-- Un-minified script so that can play a bit -->
<link rel="stylesheet" href="/libs/katex/katex.css"></link>
<script src="/libs/katex/katex.js"></script>
<script src="/libs/katex/auto-render.js"></script>
 		
        
	</head>
    <body>
        <header>
            <nav>
                <ul>
                    <li><a href="/">Home</a></li>
                    <li><a href="/pub/csml.html">CS/ML Notes</a></li>
                    <li><a href="/pub/julia.html">Julia Notes</a></li>
                    <li><a href="/pub/misc.html">Misc.</a></li>
                </ul>
                <img src="/assets/infra/hamburger.svg" id="menu-icon">
            </nav>
        </header>

<!-- CONTENT IS APPENDED HERE -->
<div class=content>
<p></p>
<h1>Convex analysis &#40;part 1&#41;</h1>
<p>In the notes \(\Gamma_0(C)\) denotes the set of <em>proper</em> and <em>lsc</em> convex functions on a convex set \(C\subseteq \mathbb R^n\). Often we will just write \(\Gamma_0\) when the domain of the minimisation is unambiguous.</p>
<ul>
<li><p><strong>proper</strong> indicates that the function takes a finite value for at least one \(x\in C\) &#40;i.e.: \(\exists x\in C, f(x) < \infty\)&#41; and is always lower bounded &#40;i.e.: \(f(x)>-\infty, \forall x\in C\)&#41;. For instance: the indicator of a non-empty set is a proper convex function.</p>
</li>
<li><p><strong>lsc</strong> &#40;<em>lower semi continuous</em>&#41; indicates that around a point \(x_d\in C\) of discontinuity, we will either have \(f(x)>f(x_d)\) or \(f(x)\le f(x_d)\). For instance: the function \(f(x)=1\) if \(x>0\) and \(f(x)=0\) otherwise, is lsc. on \(\mathbb R\). However, the function \(g(x)=1\) if \(x\ge0\) and \(g(x)=0\) is <em>upper</em> semi continuous on \(\mathbb R\).</p>
</li>
</ul>
<p><div class="img-small"> <img src="/assets/csml/cvxopt/lsc-usc.svg" alt="" /> </div></p>
<p>We denote by \(x^\sharp \in C^\circ\) a minimiser of the function, i.e. a point such that \(f(x)\ge f(x^\sharp)\) for all \(x\in C\) &#40;remember that we assume that the minimum is achieved on \(C^\circ\)&#41;. It is easy to show that the set of minimisers \(\arg\min_{x\in C}\) is a convex subset of \(C\).</p>
<h2>Subgradient, subdifferential and FOC </h2>
<p><div class="colbox-yellow"></p>
<p>We say that \(y\in\mathbb R^n\) is a <em>subgradient</em> of the function \(f\) at \(x\in C\) if it verifies the following inequality:</p>
<p><a name="10778028128907436957"></a>$$\begin{array}{c} 
    f(z) &\ge & f(x) + \langle z-x, y \rangle, \qquad \forall z\in C. 
\end{array}$$ </div></p>
<p>The inequality <span class="eqref"><a href="#10778028128907436957">(1)</a></span> simply indicates that the graph of the function \(f\) at \(x\) is supported by the hyperplane defined by the right-hand side. A subgradient is thus the &quot;slope&quot; of one such <em>supporting hyperplane</em>.</p>
<p>If the function is differentiable at \(x\) then there is only one such subgradient at \(x\) &#40;the classical gradient \(\nabla f(x)\)&#41; and, correspondingly, only one supporting hyperplane:</p>
<p>$$\begin{array}{c} 
    f(z) &\ge& f(x) + \left\langle z-x, \nabla f(x)\right\rangle, \qquad \forall z \in C.
\end{array}$$</p>
<p>However, if the function is not differentiable at \(x\) &#40;e.g., if there is a kink at \(x\)&#41; then there may be infinitely many supporting hyperplanes and infinitely many subgradients.</p>
<p><div class="colbox-yellow"> The set of subgradients of a function \(f\) at a point \(x\in \mathrm{dom}\, f\) is called the <em>subdifferential</em> and denoted \(\partial f(x)\). For a convex function \(f\), it can be shown that the subdifferential of \(f\) is a non-empty bounded set at any point \(x\in (\mathrm{dom}\,f)^\circ\). </div></p>
<p>An example is the absolute value function \(f(x)=|x|\) which is not differentiable at \(0\). It is however supported at that point by all lines of the form \(\ell_\alpha(x)=\alpha x\) with \(\alpha\in [-1,1]\) &#40;see figure below&#41;. The set \([-1, 1]\) is therefore the subdifferential of the function at \(0\), denoted \(\partial f(0)\).</p>
<p><img src="/assets/csml/cvxopt/abs-subgrad.svg" alt="" /></p>
<h3>First order optimality condition &#40;FOC&#41; </h3>
<p>A point \(x^{\sharp}\in C\) is a minimiser of the function \(f\) if and only if \(f(z)\ge f(x^{\sharp})\) for all \(z\in C\). This can be written equivalently as:</p>
<p>$$\begin{array}{c} 
    f(z) &\ge& f(x^{\sharp}) + \left\langle z-x, 0\right\rangle, \qquad \forall z \in C,
\end{array}$$</p>
<p>and hence \(0\) must be a subgradient of \(f\) at \(x^\sharp\).</p>
<p><div class="colbox-yellow"> <em>First-order optimality condition</em> &#40;FOC&#41;: for a convex function \(f\),</p>
<p>$$
x^\sharp \,\in\, \arg\min_{x\in C} \, f(x) \quad\!\! \Longleftrightarrow\quad\!\! 0\,\in\, \partial f(x^\sharp).$$ </div></p>
<p>If we take the subdifferential as an <em>operator</em> then, intuitively, looking for a minimiser amounts to &quot;inverting&quot; the subdifferential and evaluating it at \(0\), i.e.: \(x^\sharp = (\partial f)^{-1}(0)\). Of course at this point we don&#39;t know how to compute \((\partial f)^{-1}\) in general. We shall come back to this in more details but the idea of inverting an operator involving the subdifferential to find the minimiser is key in convex optimisation.</p>
<p>Note that in some simple situations the FOC is sufficient to immediately find a minimiser, for instance, if \(f(x)=|x|\), then clearly the subdifferential:</p>
<p>$$\begin{array}{c} 
    \partial f(x) &=& \begin{cases} \mathrm{sign}(x) & (x\neq 0) \\\\ [-1, 1] & (x=0) \end{cases}
\end{array}$$</p>
<p>which shows that the only point \(x^\sharp\) where \(0\in \partial f(x^\sharp)\) is \(x^\sharp=0\). In other words, \((\partial f)^{-1}(0) = 0\).</p>
<h3>Subdifferential of a sum </h3>
<p><div class="colbox-yellow"> Let \(f_i:C\to \mathbb R\) be convex functions then <a name="2307685298850424370"></a>$$\begin{array}{c} 
    \partial \sum_i f_i &\supseteq& \sum_i \partial f_i. 
\end{array}$$ </div></p>
<p>Indeed, let \(g\equiv\sum_i f_i\) and let \(y_i\in\partial f_i(x)\) then, by definition,</p>
<p>$$\begin{array}{c} 
    f_i(z) &\ge& f_i(x) + \left\langle z-x, y_i\right\rangle, \quad\forall z\in C,
\end{array}$$ and we can sum across these inequalities to get $$\begin{array}{c} 
    g(z) &\ge& g(x) + \langle z-x, \sum_i y_i\rangle, \quad\forall z\in C,
\end{array}$$ so that \(\sum_i y_i \in \partial g(x)\). Note that if we have \(0\in \sum_i \partial f_i(x^\dagger)\) then the inclusion implies that \(0\in\partial \sum_i f_i(x^\dagger)\) which is sufficient to show that \(x^\dagger\) is a minimiser.</p>
<h2>Short references</h2>
<ol>
<li><p><strong>Boyd</strong> and <strong>Vandenberghe</strong>, <a href="https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf">Subgradients</a>: accessible lecture notes introducing the subgradients and proving that the subdifferential of a convex function is non-empty and closed at any point in the interior of the domain of the function.</p>
</li>
</ol>
<p><em>See also the books mentioned in the introduction.</em></p>
<div class="page-foot">
		<div class="copyright">
				&copy; T. Lienart. All rights reserved. Last modified: September 12, 2018. Website built with <a href="https://github.com/tlienart/JuDoc.jl">JuDoc.jl</a>.
		</div>
</div>
</div><!-- CONTENT ENDS HERE -->
        
                <script>
		renderMathInElement(document.body)
</script>

        
    </body>
</html>
